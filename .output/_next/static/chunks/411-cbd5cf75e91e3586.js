"use strict";(self.webpackChunk_N_E=self.webpackChunk_N_E||[]).push([[411],{2381:function(e,n,t){t.d(n,{Button:function(){return c}});var a=t(7437),o=t(2265),i=t(535),s=t(3448);let r=(0,i.j)("inline-flex items-center justify-center whitespace-nowrap rounded-md text-sm font-medium transition-colors focus-visible:outline-none focus-visible:ring-1 focus-visible:ring-ring disabled:pointer-events-none disabled:opacity-50",{variants:{variant:{default:"bg-primary text-primary-foreground shadow hover:bg-primary/90",destructive:"bg-destructive text-destructive-foreground shadow-sm hover:bg-destructive/90",outline:"border border-input bg-transparent shadow-sm hover:bg-accent hover:text-accent-foreground",secondary:"bg-secondary text-secondary-foreground shadow-sm hover:bg-secondary/80",ghost:"hover:bg-accent hover:text-accent-foreground",link:"text-primary underline-offset-4 hover:underline"},size:{default:"h-9 px-4 py-2",sm:"h-8 rounded-md px-3 text-xs",lg:"h-10 rounded-md px-8",icon:"h-9 w-9"}},defaultVariants:{variant:"default",size:"default"}}),c=o.forwardRef((e,n)=>{let{className:t,variant:i,size:c,asChild:l=!1,...h}=e,u=l?o.Fragment:"button";return(0,a.jsx)(u,{className:(0,s.cn)(r({variant:i,size:c,className:t})),ref:n,...h})});c.displayName="Button"},9820:function(e,n,t){t.d(n,{Ol:function(){return r},SZ:function(){return l},Zb:function(){return s},aY:function(){return h},eW:function(){return u},ll:function(){return c}});var a=t(7437),o=t(2265),i=t(3448);let s=o.forwardRef((e,n)=>{let{className:t,...o}=e;return(0,a.jsx)("div",{ref:n,className:(0,i.cn)("rounded-lg border bg-card text-card-foreground shadow-sm",t),...o})});s.displayName="Card";let r=o.forwardRef((e,n)=>{let{className:t,...o}=e;return(0,a.jsx)("div",{ref:n,className:(0,i.cn)("flex flex-col space-y-1.5 p-6",t),...o})});r.displayName="CardHeader";let c=o.forwardRef((e,n)=>{let{className:t,...o}=e;return(0,a.jsx)("h3",{ref:n,className:(0,i.cn)("text-2xl font-semibold leading-none tracking-tight",t),...o})});c.displayName="CardTitle";let l=o.forwardRef((e,n)=>{let{className:t,...o}=e;return(0,a.jsx)("p",{ref:n,className:(0,i.cn)("text-sm text-muted-foreground",t),...o})});l.displayName="CardDescription";let h=o.forwardRef((e,n)=>{let{className:t,...o}=e;return(0,a.jsx)("div",{ref:n,className:(0,i.cn)("p-6 pt-0",t),...o})});h.displayName="CardContent";let u=o.forwardRef((e,n)=>{let{className:t,...o}=e;return(0,a.jsx)("div",{ref:n,className:(0,i.cn)("flex items-center p-6 pt-0",t),...o})});u.displayName="CardFooter"},3448:function(e,n,t){t.d(n,{cn:function(){return i}});var a=t(1994),o=t(3335);function i(){for(var e=arguments.length,n=Array(e),t=0;t<e;t++)n[t]=arguments[t];return(0,o.m6)((0,a.W)(n))}},3791:function(e){e.exports=JSON.parse('{"X":[{"id":"1","question":"There is an application, which consists of the EC2 instance in an\\nAuto-scaling group. During a particular timeframe, users are\\ncomplaining about the poor response time of application due to the\\nincrease in traffic. You have to deploy a new EC2 instance for an\\nAuto-scaling group when utilization of CPU is greater than 60% for\\ntwo consecutive periods of 5 minutes. How will you do this?","options":{"A":"By decreasing the consecutive number of collection periods","B":"By increasing the minimum number of instances in the Autoscaling group","C":"By decreasing the collection period to ten minutes","D":"By decreasing the threshold CPU utilization percentage at\\nwhich to deploy a new instance"},"answer":"B","explanation":"If you increase the minimum number of instances, the\\napplication will be running even when the load on the website is not high."},{"id":"2","question":"You have a setup in AWS that contains an elastic load balancer, an\\nAuto-scaling group, which launches EC2 instances, and AMIs with\\nyour code pre-installed. You want a cost-effective solution and\\ndeploy the updates up to a certain number of users from which you\\nare able to revert quickly. What is the best approach?","options":{"A":"Create a second ELB and a new Auto-scaling group assigned a\\nnew launch configuration. Create a new AMI with an updated\\napp. Use Route53 weighted round robin records to adjust the\\nproportion of traffic hitting the two ELBs","B":"Create a new AMI’s with a new app. Then use the new EC2\\ninstances in half proportion to the older instances","C":"Redeploy with AWS Elastic Beanstalk and Elastic Beanstalk\\nversions. Use Route53 weighted round robin records to adjust\\nthe proportion of traffic hitting the two ELBs","D":"Create a full second stack of instances, cut the DNS over to the\\nnew stack of instances, and change the DNS back if a rollback\\n\\n\\fis needed"},"answer":"A","explanation":"The weighted routing policy of Route53 is used to direct the\\nproportion of traffic to your application. The best policy is to create new\\nELB, attach to the Auto-scaling group and then divert the traffic by using\\nRoute"},{"id":"53","question":"3. A specific process running an application, which is critical to\\napplication functionality and health check is added to the Autoscaling group. The instances are showing healthy, but the application\\nis not working properly. What is the problem with the health checks?","options":{"A":"You do not have the time range in the health check properly\\nconfigured","B":"It is not possible for a health check to monitor a process that\\ninvolves an application","C":"The health check is not configured properly","D":"The health check is not checking the application process"},"answer":"D","explanation":"In the case of the custom health check, you can send\\ninformation from your health check to Auto-scaling. So that Auto-scaling can\\nutilize this information. For example, if you determine that an instance is not\\nfunctioning as expected, you can set the health status of the instance to\\n“Unhealthy”. The next time that Auto-scaling performs a health check on the\\ninstance, it will determine that the instance is unhealthy and then launch a\\nreplacement instance."},{"id":"4","question":"In the AWS ap-south-1 region, Alan developed an AWS console git\\nrepository called \\"MyRepo\\". The required credentials for cloning of\\nthe repository were configured in CodeCommit. Alan is now\\ndeveloping the software with a Ubuntu Linux machine. Which\\ncommands can the repository be cloned in a local directory from\\nCodeCommit? (Choose 2)","options":{"A":"git\\nclone\\nssh://git-codecommit.ap-south1.amazonaws.com/v1/repos/MyDemoRepo my-demo-repo","B":"git\\nclone\\nssh://gitcodecommit.aws.com/v1/repos/MyDemoRepo my-demo-repo","C":"git\\nclone\\nhttp://git-codecommit.ap-south1.amazonaws.com/v1/repos/MyDemoRepo my-demo-repo","D":"git\\nclone\\nhttps://git-codecommit.ap-south1.amazonaws.com/v1/repos/MyDemoRepo my-demo-repo","E":"git\\nclone\\nhttps://gitcodecommit.amazonaws.com/v1/repos/MyDemoRepo\\nmydemo-repo"},"answer":"A and D","explanation":"You must have a clone URL for the relevant CodeCommit\\nrepository to clone a CodeCommit repository. One thing to note is that the\\nrepository name and AWS Region must be included in this URL. The best\\nway to get the URL is to click on \'clone URL\'.\\nBy using SSH and HTTPS, you can connect to the repository."},{"id":"5","question":"A new employee accidentally removed a CodeCommit repository\\nbranch last week. For certain key repositories, when someone deletes\\nany branch in CodeCommit, Julie is told to create a notification. In\\nCodeCommit, you plan to set a trigger. Which service can be used for\\nthis purpose?","options":{"A":"For the repository event of deletion of branches, create a trigger\\nin CodeCommit to an AWS CloudWatch Event to provide\\nwarnings","B":"For the repository event of deletion of branches, create a trigger\\nin CodeCommit to an Amazon SNS topic to provide\\nnotifications to users","C":"In CodeCommit, create a trigger for branch deletion event to an\\nAmazon SQS queue to provide warnings to users","D":"In CodeCommit, create a trigger to a Lambda function, which\\nworks with AWS Simple Email Service to send emails\\nnotifying users when a branch is deleted"},"answer":"B","explanation":"You can set triggers for a repository of CodeCommit to move\\n\\n\\fcode or to trigger actions for other events. SNS and Lambda are the\\nsupported trigger services. For each CodeCommit repository, you can create\\nup to 10 triggers, as SNS is the easiest method. Users simply have to\\nsubscribe to SNS topic and be notified."},{"id":"6","question":"Multiple applications are running on AWS. The company wants you\\nto develop a tool that immediately calls the team when the alarm is\\ntriggered. You also have to make sure that alarm for the on-call team\\ngenerated must handle to notify the correct team at the correct time.\\nWhat steps should be taken to implement this?","options":{"A":"Create an Amazon SNS topic and an Amazon SQS queue.\\nConfigure the Amazon SQS queue as a subscriber to the\\nAmazon SNS topic. Configure CloudWatch alarms to notify\\nthis topic when an alarm is triggered. Create an Amazon EC2\\nAuto-scaling group with both minimum and desired instances\\nconfigured to 0. The worker node in this group spawns when\\nmessages are added to the queue. Workers then use Amazon\\nSimple Email Service to send messages to your on-call teams","B":"Create an Amazon SNS topic and configure your on-call team\\nemail addresses as a subscriber. Use the AWS SDK tools to\\nintegrate your application with Amazon SNS and send\\nmessages to this new topic. Notifications will be sent to on-call\\nusers when a CloudWatch alarm is triggered","C":"Create an Amazon SNS topic and configure your on-call email\\naddresses as subscribers. Create a secondary Amazon SNS\\ntopic for alarms and configure your CloudWatch alarms to\\nnotify this topic when triggered. Create an HTTP subscriber to\\nthis topic that notifies your application via HTTP post when an\\nalarm is triggered. Use the AWS SDK tools to integrate your\\napplication with Amazon SNS and messages to the first topic\\nso that on-call engineers receive alerts","D":"Create an Amazon SNS topic for each on-call group, and\\nconfigure each of these with the team member emails as a\\nsubscriber. Create another Amazon SNS topic and configure\\nyour CloudWatch alarms to notify this topic when triggered.\\nCreate an HTTP subscriber to this topic that notifies your\\n\\n\\fapplication via HTTP post when an alarm is triggered. Use the\\nAWS SDK tools to integrate your application with Amazon\\nSNS and send messages to the correct team topic when on shift"},"answer":"D","explanation":"This option fulfills all the requirements. First step is to create\\nan SNS group so that specific member gets the email address and ensure that\\nthe application uses HTTP endpoint and SDK for publishing messages.\\nYou cannot use SQS service for this purpose, and as the message notification\\nneeds to be given to only specific members and not to all members so other\\noptions are also not valid."},{"id":"7","question":"While analyzing the metrics, you got to know that the company\\nwebsite is experiencing response times higher than anticipated during\\npeak hours. You rely on Auto-scaling to make sure you are scaling\\nyour time during peak windows. What can you do to enhance your\\nAuto-scaling policy to decrease the response time, which is high?","options":{"A":"Push custom metrics to CloudWatch to monitor your CPU and\\nnetwork bandwidth from your servers, which will allow your\\nAuto-scaling policy you have better fine-grain insight","B":"Increase your Auto-scaling group’s number of max servers","C":"Create a script that runs and monitors your servers; when it\\ndetects an anomaly in load, it posts to an Amazon SNS topic\\nthat triggers elastic load balancing to add more servers to the\\nload balancer","D":"Push custom metrics to CloudWatch for your application that\\ninclude more detailed information about your web application,\\nsuch as how many requests it is handling and how many are\\nwaiting to be processed"},"answer":"B and D","explanation":"Option B is valid because the max server is low. Therefore, the\\napplication cannot handle the peak load.\\nOption D ensures that Auto-scaling can scale the group to the right metrics."},{"id":"8","question":"An organization has an application that uses stateless web tier on\\n\\n\\fEC2 instances that are behind the ELB and use RDS read replicas.\\nFrom the following options, which option is best to implement selfhealing and cost effective architecture?","options":{"A":"Use a larger Amazon EC2 instance type for the web server tier\\nand a larger DB instance type for the data storage layer to\\nensure that they do not become unhealthy","B":"Use an Amazon RDS Multi-AZ deployment","C":"Set up a third-party monitoring solution on a cluster of Amazon\\nEC2 instances in order to emit custom CloudWatch metrics to\\ntrigger the termination of unhealthy Amazon EC2 instances","D":"Set up an Auto-scaling group for the web server tier along with\\nan Auto-scaling policy that uses the Amazon EC2 CPU\\nutilization CloudWatch metric to scale the instances"},"answer":"B and D","explanation":"Amazon RDS Multi-AZ deployments provide enhanced DB\\n(database) availability and durability to make them natural for production\\ndatabase workloads. Amazon RDS creates automatically a primary DB\\nInstance when providing a multi-AZ DB Instance and synchronizes the data\\nto a standby instance in a different Availability Zone (AZ). For scaling of\\nEC2 instance in Auto-scaling, you must use the CPU utilization metric of the\\ninstance."},{"id":"9","question":"There is a code repository that is stored in Amazon S3. In a recent\\naudit of security control, some questions arose about maintaining the\\nintegrity of the data stored in Amazon S3 and securely deploying\\ncodes from S3 to running applications on EC2 in a virtual private\\ncloud. What can you do to reduce these concerns? (Choose 2)","options":{"A":"Add an Amazon S3 bucket policy with a condition statement to\\nallow access only from Amazon EC2 instances with RFC 1918\\nIP addresses and enable bucket versioning","B":"Use a configuration management service to deploy AWS\\nidentity and access management user credentials to the\\nAmazon EC2 instances. Use these credentials to securely\\n\\n\\faccess the Amazon S3 bucket when deploying code","C":"Create an Amazon identity access and management role with\\nauthorization to access the Amazon S3 bucket, and launch all\\nof your application’s Amazon EC2 instances with this role","D":"Use AWS data pipeline to lifecycle the data in your Amazon\\nS3 bucket to Amazon Glacier on a weekly basis","E":"Use AWS pipe line with multi-factor authentication to securely\\ndeploy code from the Amazon S3 bucket to your Amazon EC2\\ninstances","F":"Add an Amazon S3 bucket policy with a condition statement\\nthat requires multi-factor authentication in order to delete\\nobjects and enable bucket versioning"},"answer":"C and F","explanation":"MFA delete is enabled on versioning bucket by adding another\\nlayer of protection. In order to perform the permanent deletion, you need to\\nprovide the AWS account’s access key and code from the MFA device.\\nIAM functions are built to safely request APIs from your applications,\\nwithout you having to manage the security credentials of the applications.\\nYou can delegate authorization to make API requests using IAM roles\\ninstead of creating and distributing your AWS credentials."},{"id":"10","question":"The operation and development department wants a place\\nwhere it can show both the operation system and application logs.\\nWhat should you do to activate this service using AWS? (Choose 2)","options":{"A":"Using AWS CloudFormation, create a CloudWatch Log. Log\\ngroups and send the operating system and application logs of\\ninterest using the CloudWatch logs agent","B":"Using AWS CloudWatch and configuration management, set\\nup remote logging to send events through UDP packets to\\nCloudTrail","C":"Using configuration management, set up remote logging to\\nsend events to Amazon Kinesis and insert these into Amazon\\ncloud search or Amazon RedShift, depending on the available\\nanalytic tool","D":"Using AWS CloudFormation, merge the application logs with\\n\\n\\fthe operating system logs, and use IAM roles to allow both\\nteams to have access to view console output from Amazon EC2"},"answer":"A and C","explanation":"Amazon CloudWatch logs can be used to monitor, store, and\\naccess log files from Amazon Elastic Compute Cloud (Amazon EC2), AWS\\nCloud Trail instances, and other sources. The related log data can then be\\ncollected from CloudWatch Logs."},{"id":"11","question":"An enterprise is using CloudFormation for its application\\ndeployment and now they want a cost-effective solution for creating a\\nrolling deployment with minimum downtime. How can this be\\nexecuted? (Choose 2)","options":{"A":"By re-deploying the application using a CloudFormation\\ntemplate to deploy Elastic Beanstalk","B":"By re-deploying with a CloudFormation template and defining\\nupdate policies on Auto-scaling groups in the template","C":"By using the AutoScalingRollingUpdate attribute to specify\\nhow CloudFormation handles updates to Auto-scaling Group\\nResource","D":"By tearing down the old stack after each stack is deployed"},"answer":"B and C","explanation":"The Auto-scaling group resource supports an UpdatePolicy\\nattribute. This defines how an Auto-scaling group is updated when an update\\nin CloudFormation occurs. The Auto-scaling group is updated by the rolling\\nupdate, which is performed by specifying the AutoscalingRollingUpdate\\nPolicy. This retains the same Auto-scaling group by replacing the old\\ninstances with a new one."},{"id":"12","question":"John created a mobile application whose main purpose is\\nphoto sharing. In the very beginning, there will be approx. ten\\nthousand users expected. He uses AWS S3 to store images and has to\\ndetermine how users can be authenticated in order to access the\\nimages. The storage of these images also needs to be managed.\\nWhich steps should John take next? (Choose 2)","options":{"A":"Use a key-based naming scheme comprised from the user ID’s\\nfor all user objects in a single Amazon S3 bucket","B":"Create an Amazon S3 bucket per user, and use the application\\nto generate the S3 URL for the appropriate content","C":"Use AWS IAM user accounts as the application level user\\ndatabase, and offload the burden of authentication from the\\napplication code","D":"Authenticate users at the application level, and use AWS\\nsecurity token service (STS) to grant token-based authorization\\nto S3-object","E":"Authenticate users at the application level, and send an SMS\\ntoken message to the user. Create an Amazon S3 bucket with\\nthe same name as the SMS message token, and move the user’s\\nobjects to that bucket"},"answer":"A and D","explanation":"The AWS STS is the web service that enables you to request\\ntemporary and limited access credentials to IAM users and for users that you\\nauthenticate. This token is then used for accessing an object in S"},{"id":"3","question":"13. Michael uses an Auto-scaling group for its application. The\\nAuto-scaling group has 2 AZs, one AZ contains 4 EC2 instances and\\nanother AZ contains 3 instances. None of the instances are protected\\nfrom the scale. What is going to happen, depending on the Autoscaling termination policy set by default?","options":{"A":"Auto-scaling will select an instance to terminate randomly","B":"Auto-scaling will select the AZ with 4 EC2 instances and\\nterminate an instance","C":"Auto-scaling will terminate un-protected instances in the\\navailability zone with the oldest launch configuration","D":"Auto-scaling will terminate all unprotected instances that are\\nthe closest to the next billing hour"},"answer":"B","explanation":"To ensure that the network architecture spans availability\\n\\n\\fzones evenly, the default termination policy is designed. When using the\\ndefault termination policy, Auto-scaling selects an instance to terminate as\\nfollow:\\nAuto-scaling determines if instances exist in multiple AZs. If so, you must\\nselect the AZ with the most instances and at least one instance that is not\\nprotected from the scale. In case of more than one availability zone having\\nthe same number of instances, Auto-scaling selects the availability zone with\\nthe instances with the oldest configuration."},{"id":"14","question":"David manages a continual integration application to monitor\\nversion control for changes and then run a complete suite of build\\ntests on new Amazon EC2 instances. What should he do to guarantee\\nthe lowest overall cost while running as many tests as possible in\\nparallel?","options":{"A":"Perform syntax checking on the continuous integration system\\nbefore launching a new Amazon EC2 instance for build test,\\nunit, and integration tests","B":"Perform all test on continuous integration system, using AWS\\nOpsWorks for units, integration, and build tests","C":"Perform syntax and build test on the continuous integration\\nsystem before launching the new Amazon EC2 instance units\\nand integration tests","D":"Perform syntax checking on the continuous integration system\\nbefore launching a new AWS data pipeline for coordinating the\\noutput of unit, integration, and build tests"},"answer":"C","explanation":"When developers want to integrate the code to the shared\\nrepository for as much time as they want, then Continuous Integration is the\\nbest practice tool for development. Each of the check-ins is verified by an\\nautomated build, which allows the team to detect problems early."},{"id":"15","question":"There is an Auto-scaling group associated with a load\\nbalancer. You want to suspend Auto-scaling AddToLoadBalancer for\\nsome time. What is the effect of this on launched instances during the\\nsuspension period?","options":{"A":"The instances will be registered with ELB once the process has\\nresumed","B":"The instances will not be registered with ELB. You must\\nmanually register when the process is resumed","C":"Auto-scaling will not launch the instances during this period\\nbecause of the suspension","D":"It is not possible to suspend the AddToLoadBalancer process"},"answer":"C","explanation":"Auto-scaling starts up the instances but does not add them to\\nload balancers or target groups until you suspend the AddToLoadBalancer.\\nAuto-scaling will resume adding instances to load balancer or target group if\\nyou resume AddToLoadBalancer. Auto-scaling, however, does not include\\nthe instances, which are launched during that process. You have to manually\\nadd them."},{"id":"16","question":"You decided to change the instance type of your production\\ninstances that run in the Auto-scaling group. To launch our\\narchitecture, we used the CloudFormation template and currently\\nused 4 instances in production. The service cannot be interrupted;\\ntherefore, two instances should always run during the update. Which\\nof the following options can be applicable?","options":{"A":"Auto-scalingReplacingUpdate","B":"Auto-scalingScheduledAction","C":"Auto-scalingRollingUpdate","D":"Auto-scalingIntegrationUpdate"},"answer":"C","explanation":"The UpdatePolicy attribute is provided by the AWS::Autoscaling::Auto-scalingGroup asset. This is to describe the updating of the\\nAuto-scaling resource group when an update to the CloudFormation stack\\ntakes place. A common approach is to perform a rolling update for updating\\nan Auto-scaling group by defining the Auto-scalingRollingUpdate policy.\\nThis keeps the same Auto-scaling Group and, according to the specified\\nparameters, replaces old instances with new ones."},{"id":"17","question":"A company wants to use Blue/Green Deployment for its\\napplication, which runs behind a load balancer on Amazon EC2\\ninstances. How this be done for every deployment?","options":{"A":"By creating a new load balancer with new Amazon EC2\\ninstances, carrying out the deployment, and then switching\\nDNS over to the new load balancer using Amazon Route53\\nafter testing","B":"By setting up Amazon Route53 health checks to fail over from\\nany Amazon EC2 instance that is currently being deployed to","C":"By creating a test stack for validating the code using AWS\\nCloudFormation, and then deploying the code to each\\nproduction Amazon EC2 instance","D":"By launching more Amazon EC2 instance to ensure high\\navailability, de- registering each Amazon EC2 instance from\\nthe load balancer, then upgrading, testing, and registering it\\nagain with the load balancer"},"answer":"A","explanation":"For blue/green deployment you need to create new ELB, which\\nneeds to be used for pointing to new production changes. To assign traffic to\\ntwo ELBs on the basis of 80% to 20% traffic, use the Weighted Routing\\nPolicy for Route"},{"id":"53","question":"Change the percentage value as per requirement. Once\\nthe changes are tested, the traffic will route 100% via Route53 to the new\\nELB. For the figure you can visit:\\nhttps://d0.awsstatic.com/whitepapers/AWS_Blue_Green_Deployments.pdf\\n18. An enterprise has its application, which is hosted on EC2\\ninstances behind ELB. Now, some errors are observed in the\\napplication. In order to diagnose the error, the enterprise wants to\\ncheck the ELB access logs but they are empty. What might be the\\nreason behind this?","options":{"A":"The enterprise does not have the appropriate permissions to\\naccess the logs","B":"The enterprise does not have the CloudWatch metrics\\n\\n\\fcorrectly configured","C":"Access logging is an optional feature of ELB that is disabled by\\ndefault","D":"ELB access logs are only available for the maximum of one\\nweek"},"answer":"C","explanation":"ELB provides an access log that captured the detailed\\ninformation about the request sent to load balancer. Access logging is\\ndisabled by default but can be enabled, it captures the logs and stores them in\\nthe S3 bucket. That log contains information such as the requester\'s IP,\\nlatencies, request paths and server responses at the time the request was sent.\\nYou can use these logs to evaluate patterns of traffic and to resolve problems."},{"id":"19","question":"Harry stores sensitive information on an EBS volume attached\\nto EC2 instance in his application. What can he do to protect this\\ndata? (Choose 2)","options":{"A":"Unmount the EBS volume, take a snapshot and encrypt it. Remount the Amazon EBS volume","B":"Copy the unencrypted snapshot and check the box to encrypt\\nthe new snapshot. Volumes restored from this encrypted\\nsnapshot will also be encrypted","C":"It is not possible to encrypt an EBS volume, you use a lifecycle\\npolicy to transfer data to S3 for encryption","D":"Create and mount a new encrypted Amazon EBS volume.\\nMove the data to the new volume. Delete the old Amazon EBS\\nvolume"},"answer":"B and D","explanation":"By following these two methods given below, he can protect\\nhis data:\\nTo migrate data from encrypted to un-encrypted volume, create a\\ndestination volume and attach the destination volume to the\\ninstance that hosts the data to migrate. Copy the data from the\\nsource directory to the destination volume. For copying, use bulk-\\n\\n\\fcopy utility\\nCreate a snapshot of the unencrypted volume and copy snapshots\\nwith an encrypted parameter. Now, restore the encrypted snapshot\\nto the new encrypted volume"},{"id":"20","question":"Company ABC works on multiple projects for different\\nclients, the development environment of each project is different and\\nrequires multiple OS, tools, programming languages, and runtime.\\nNow, the company is planning to use AWS CodeBuild to build an\\nartifact for each client. From the following, which option is not a\\nsupported Docker platform for AWS CodeBuild?","options":{"A":"Redhat OS","B":"Amazon Linux 2","C":"Windows Server Core 2016","D":"Ubuntu 18.04"},"answer":"A","explanation":"Redhat OS is not supported in AWS CodeBuild. Amazon\\nLinux 2, Windows Server Core 2016 and Ubuntu 18.04 are supported\\nplatforms in AWS CodeBuild."},{"id":"21","question":"There is a large multi-tiered Windows-based web application\\nsituated behind a load balancer, running on an EC2 instance. The\\nproblem of slow customer page load time occurs, and your manager\\nasks you to sort this problem out. You must ensure that customer load\\ntime is not affected by too many requests per second. Which one of\\nthe following techniques should you use to solve this problem?","options":{"A":"Re-deploy your infrastructure using the AWS CloudFormation\\ntemplate. Configure Elastic load balancing health check to\\ninitiate a new AWS CloudFormation stack when health checks\\nreturn fail","B":"Re-deploy your infrastructure using an AWS CloudFormation\\ntemplate. Spin up a second AWS CloudFormation stack.\\nConfigure Elastic load balancing spillover functionality to spill\\n\\n\\fover any slow connections to the second AWS CloudFormation\\nstack","C":"Re-deploy your application using an Auto-scaling template.\\nConfigure the Auto-scaling template to spin up a new Elastic\\nBeanstalk application when the customer load time surpasses\\nyour threshold","D":"Re-deploy your infrastructure using AWS CloudFormation,\\nElastic Beanstalk, and Auto-scaling. Set up your Auto-scaling\\ngroup policies to scale based on the number of requests per\\nsecond as well as the current customer load time"},"answer":"D","explanation":"Auto-scaling is used to ensure that there will be a valid number\\nof EC2 instances, which can handle the load of the application. The desired\\nnumber of instances can be specified at the time of the creation of the group\\nor thereafter. In Auto-scaling group, you need to define both minimum and a\\nmaximum number of instances as per the requirement of your application.\\nAuto-scaling can launch or terminate the instances as the demand for your\\napplication increases or decreases if you specify scaling policies.\\nOptions A and B are invalid because Auto-scaling is required for an\\napplication to be able to handle the traffic. Option C is also invalid because\\nthere is no Auto-scaling template."},{"id":"22","question":"A company is concerned with the extreme increasing cost as\\nits management has reported the increase in monthly bills. After\\nreviewing the billing report, it is noticed that there is an increase in\\ndata transfer costs. Now, how can Alan provide management with\\nbetter insight into data transfer use?","options":{"A":"Update Amazon CloudWatch metrics to use 5-second\\ngranularity, which will give better-detailed metrics that can be\\ncombined with the billing data to pinpoint anomalies","B":"Deliver custom metrics to Amazon CloudWatch per application\\nthat breaks down application data transfer into multiple, more\\nspecific data points","C":"Use Amazon CloudWatch logs to run a map-reduce on logs to\\ndetermine high usage and data transfer","D":"Using Amazon CloudWatch metrics, pull elastic load balancing\\n\\n\\foutbound data transfer metrics monthly, and include them with\\ntheir billing report to show, which application is causing higher\\nbandwidth usage"},"answer":"B","explanation":"The custom metrics can be published to CloudWatch using\\nCLI and API and can be viewed statistically with the AWS management\\nconsole. If you have custom metrics specific to your application, you can\\ngive a breakdown to the management on the exact issue."},{"id":"23","question":"Currently, an infrastructure is running on EC2 instance behind\\nan Auto-scaling group. The application logs are written on ephemeral\\nstorage. The company experienced a major bug, which triggered the\\nAuto-scaling group up and down before successfully retrieving the\\nlogs of the server. What technique is suitable to retrieve logs?","options":{"A":"Configure the ephemeral policies on your Auto-scaling group\\nto back up on terminate","B":"Configure your Auto-scaling policies to create a snapshot of all\\nephemeral storage on terminate","C":"Install the CloudWatch logs agent on your AMI, and configure\\nCloudWatch logs agent to stream your logs","D":"Install the CloudWatch monitoring agent on your AMI, and set\\nup a new SNS alert for CloudWatch metrics that trigger the\\nCloudWatch monitoring agent to backup all logs on the\\nephemeral drive"},"answer":"C","explanation":"CloudWatch logs are used to monitor applications and systems\\nusing log data. It can track the number of errors that occurred in the\\napplication and send a notification when the errors are exceeded from the\\nspecified threshold value. CloudWatch log uses log data, therefore, no\\nchanges in code are required."},{"id":"24","question":"Susan has a project in which she needs to deploy an\\ninfrastructure on the AWS CloudFormation template. The\\ninfrastructure supports multi-tier applications. She needs to perform\\n\\n\\fthe task of organizing AWS CloudFormation resources for the future\\nso that different departments such as Networking and Security can\\nreview the architecture before it goes to Production. How should she\\nuse the already existing workflows to accommodate each\\ndepartment?","options":{"A":"Separate the AWS CloudFormation template into a nested\\nstructure that has individual templates for the resources that are\\nto be governed by different departments, and use the outputs\\nfrom the networking and security stacks for the application\\ntemplate that you control","B":"Organize the AWS CloudFormation template so that related\\nresources are next to each other in the template, such as VPC\\nsubnets and routing rules for networking and security groups\\nand IAM information for security","C":"Organize the AWS CloudFormation templates, so that related\\nresources are next to each other in the template for each\\ndepartment’s use, leverage your existing continuous integration\\ntool to constantly deploy changes from all parties to the\\nproduction environment, and then run tests for validation","D":"Use a custom application and the AWS SDK to replicate the\\nresources defined in the current AWS CloudFormation\\ntemplate, and use the existing code review system to allow\\nother departments to approve changes before altering the\\napplication for future deployments."},"answer":"A","explanation":"When your infrastructure is growing, common patterns emerge\\nin each template, where you declare the same components. You can\\nseparately create templates for these common components. Thus, you can\\nmix and match the various templates and you can also create a single unified\\nstack using nested stacks. Stacks that are nested are stacks that create\\nadditional stacks. Use AWS::CloudFormation::Stackresource to link other\\ntemplates in your template to build nested stacks."},{"id":"25","question":"Peter has a team for developing some Java applications for\\nmicro-services. AWS CodeBuild is responsible for the construction\\nof the CI / CD pipeline and supplies S3 storage artefacts. The team\\n\\n\\fmanages multiple build-specific files instead of adding explicitly the\\nbuild commands when the build is running. What description is\\nINCORRECT about the use of buildspec files?","options":{"A":"Build spec files must be expressed in YAML format","B":"There should be only one build spec file for a given build\\nproject","C":"The build spec file can be placed in any location under the top\\nlevel directory","D":"Users can override the default build spec file name such as\\nbuildspec_test.yml"},"answer":"C","explanation":"AWS CodeBuild service has used buildspec files as a\\ncollection of build commands and related settings. So buildspec files are\\nstored in root level directory and cannot be placed at any other location."},{"id":"26","question":"An organization has messages in the SQS queue, which are\\nprocessed by instances placed in an Auto-scaling group. Depending\\nupon the queue size, the group performs scaling. In the processing,\\nthere is a third-party service calling, which tells about the failed and\\nrepeated calls from their side. The organization found that instances\\nwere terminated as the team scales during processing. What\\neconomic solution can they use to reduce the number of incomplete\\ntrials?","options":{"A":"Create a new Auto-scaling group with minimum and maximum\\nof 2s and instances running web proxy software. Configure the\\nVPC route table to route HTTP traffic to these web proxies","B":"Modify the application running on the instances to put itself\\ninto an Auto-scaling standby state while it processes a task and\\nreturn itself to ‘Inservice’ when the processing is complete","C":"Modify the application running on the instances to enable\\ntermination protection while it processes a task and disables it\\nwhen the processing is complete","D":"Increase the minimum and maximum size for the Auto-scaling\\n\\n\\fgroup. Change the scaling policies, so they scale less\\ndynamically"},"answer":"B","explanation":"You can put the instances into a standby state. After the\\nprocessing is completed, the instances come back to the state where the Autoscaling group governs them, in order to avoid termination of processing\\ninstances."},{"id":"27","question":"To maintain the version control and achieve automation for the\\napplication in your organization, you are requested to use\\nCloudFormation. How can you best maintain multiple environments\\nwhile keeping the cost down while using CloudFormation?","options":{"A":"By using CloudFormation custom resources to handle\\ndependencies between stacks","B":"By creating multiple templates in one CloudFormation stack","C":"By combining all resources into one template for version\\ncontrol and automation","D":"By creating separate templates based on functionality and\\nnested stacks with CloudFormation"},"answer":"D","explanation":"When your infrastructure is growing, common patterns emerge\\nin each template, where you declare the same components. You can\\nseparately create templates for these common components. Thus, you can\\nmix and match the various templates and you can also create a single unified\\nstack using nested stacks. Stacks that are nested are stacks that create\\nadditional stacks. Use AWS::CloudFormation::Stackresource to link other\\ntemplates in your template to build nested stacks."},{"id":"28","question":"Martin works as a DevOps engineer for a company focused on\\nartificial intelligence services. As part of the AI products, several\\nfunctions of Lambda are being developed. He is responsible for the\\ndevelopment of Lambda through AWS CodeDeploy. And a new\\nbuild is waiting for online deployment every week. There are a\\nnumber\\nof\\nsteps\\nfor\\n\\n\\fCodeDeploy:\\nSpecifying an AppSpec file, which contains instructions\\nincluding the Lambda functions to be deployed\\nCreating a deployment, which is the process of installing\\ncontents\\nCreating a CodeDeploy application\\nSpecifying a deployment group for settings and configurations\\nWhich sequence does he follow for proper configuration?","options":{"A":"3-4-1-2","B":"1-2-3-4","C":"4-3-2-1","D":"1-3-2-4"},"answer":"A","explanation":"In order to use AWS CodeDeploy, first create an application,\\nspecify the deployment group and deployment configuration then specify the\\nAppspec file. In the end, create a deployment."},{"id":"29","question":"Robert is using an application, which performs workflow and\\noperation but takes a long time for completing. Which service in\\nElastic Beanstalk environment can be used to perform this task","options":{"A":"Manages the ELB and runs a daemon process on each instance","B":"Manages Lambda functions and runs a daemon process on each\\ninstance","C":"Manages an Amazon SQS queue and runs a daemon process on\\neach instance","D":"Manages an Amazon SNS Topic and runs a daemon process on\\neach instance"},"answer":"C","explanation":"Elastic Beanstalk makes this process easier through the\\nAmazon SQS queue management and the execution of a daemon system for\\n\\n\\feach instance reading for you from the queue. The HTTP POST request is\\nsent to http:/localhost/, with the content of the queue in the context, when the\\ndaemon pulls an object from the queue. The long run job in response to\\nPOST is what the application has to do."},{"id":"30","question":"John runs an online store with Elastic Beanstalk. The store is\\nbased on an e-commerce open source platform and is installed in an\\nauto-scaling group in several instances. For the online store, your\\ndevelopment team will generate new \\"extensions\\". These extensions\\ninclude both PHP source code and an SQL upgrade script to update\\nthe database schema. He has noticed that there is an error in the\\nextension deployment when the SQL upgrade script is running. After\\nfurther investigation, he realized that the SQL script is executed in all\\nAmazon EC2 instances. Now, he has to make sure that the SQL script\\nis only executed once per deployment regardless of instances running\\nat that time. What can he do to achieve this?","options":{"A":"Make use of the Amazon EC2 metadata service to query\\nwhether the instance is marked as the leader in the Autoscaling group. Only execute the script if “true” is returned","B":"Use a “Container Command” within an Elastic Beanstalk\\nconfiguration file to execute the script, ensuring that the\\n“leader only” flag is set to true","C":"Use a “Solo Command” within an Elastic Beanstalk\\nconfiguration file to execute the script. The Elastic Beanstalk\\nservice will ensure that the command is only executed once","D":"Update the Amazon RDS security group to only allow write\\naccess from a single instance in the Auto-scaling group; that\\nway, only one instance will successfully execute the script on\\nthe database"},"answer":"B","explanation":"Container Command runs after the application and web server\\nhave been setup, and the application version archive has been extracted\\nbefore the application version is deployed. The container-commands key is\\nused to execute commands that affect the application source code. Before the\\napplication\'s source code is extracted, non-container commands and other\\n\\n\\fcustomization operations are carried out.\\nYou can use \'leader_only\' to execute the command on one instance or to set\\nup a test to only execute this command if a test command is valid. Leaderonly container commands are only executed during environment creation and\\ndeployments, while other commands and server customization operations are\\nperformed every time an instance is provisioned or updated. Leader-only\\ncontainer commands are not executed due to launch configuration changes,\\nsuch as a change in the AMI ID or instance type."},{"id":"31","question":"A number of AWS products have already been used by your\\nclient. Nonetheless, Jenkins installs on local servers in most of the\\ncurrent pipelines. The team will move Jenkins to the AWS\\nCodePipeline in the next quarter. All new pipelines should be of three\\nstages: source, build and deployment. In which combinations of\\nCodePipeline services can be used to form a new pipeline? (Choose\\n3)","options":{"A":"Source(S3) - Build(CodeBuild) - Deploy(CodeDeploy)","B":"Source(GitHub) - Build(ECS) - Deploy(Elastic Beanstalk)","C":"Source(CodeCommit)\\nBuild(CodeBuild)\\nDeploy(CloudFormation)","D":"Source(Bitbucket) - Build(Jenkins) - Deploy(S3)","E":"Source(ECR) - Build(Jenkins) - Deploy(OpsWorks)\\n\\n-\\n\\nAnswer: A, C, and E\\nExplanation: When a new pipeline is created, you need to configure the\\nsource, build and deploy stages:\\nSource stage includes- CodeCommit, ECR, GitHub, and S3\\nBuild stage includes- Jenkins or CodeBuild\\nDeploy stage includes- CodeDeploy, ElasticBeanstalk,\\nCloudFormation, S3, ECS and Service Catalog\\n32. John has a CloudFormation template in AWS. Now he wants\\nto change the alarm threshold defined in the template under\\nCloudWatch Alarm. How can he achieve this?\\n\\n\\fA. By deleting the current clouformation template and creating a\\nnew one that will update the current resources\\nB. By updating the template and then updating the stack with the\\nnew template. Only those resources that need to be changed\\nwill be changed. All other resources, which do not need to be\\nchanged will remain as they are\\nC. By updating the template and then updating the stack with the\\nnew template. Automatically all resources will be changed in\\nthe stack\\nD. Currently, there is no option to change what is already defined\\nin Cloudformation templates"},"answer":"B","explanation":"In CloudFormation, whenever you perform changes like\\nsettings or resources, the stack is updated with a new template and changes\\nperform to only those resource on which changes occurred. Otherwise,\\nremaining resources work same as they are in the previous template."},{"id":"33","question":"In the context of your ongoing application, an I / O load\\nperformance test is performed before it is deployed to production\\nwith new AMIs. The app is running with one EBS PIOPS volume per\\ninstance and requires consistent I / O performance. To ensure that I /\\nO load performance tests produce the correct results repeatedly,\\nwhich of the following option must be carried out?","options":{"A":"Ensure that the I/O block sizes for the test are randomly\\nselected","B":"Ensure that snapshots of the Amazon EBS volumes are created\\nas a backup","C":"Ensure that the Amazon EBS volume is encrypted","D":"Ensure that the Amazon EBS volumes have been pre-warmed\\nby reading all the blocks before the test"},"answer":"D","explanation":"During the AMI creation process, the EC2 instance creates the\\nsnapshot of instance’s root volume and any other EBS attached to the\\ninstance. The new volumes receive maximum performance and do not\\n\\n\\frequire initialization also known as pre-warming. The storage blocks from\\nthe volume, restored from snapshots must be initialized before the accessing\\nof the block. This process takes time but increases the latency of an I/O\\noperation for every time the first block is accessed."},{"id":"34","question":"As the primary monitoring system for your web application,\\nyou use Amazon CloudWatch. After a recent software release, when\\nusing the web application, the users get sporadic 500 Internal Server\\nErrors. You want to create an alert CloudWatch and warn an on-call\\nengineer when this happens. How can you do this by using AWS?\\n(Choose 3)","options":{"A":"By installing a CloudWatch logs agent on your server to stream\\nweb application logs to CloudWatch","B":"By deploying your web application as an AWS Elastic\\nBeanstalk application and using the default Elastic Beanstalk\\nCloudWatch metrics to capture 500 internal servers then setting\\na CloudWatch alarm on that metric","C":"By using Amazon Simple Email Service to notify an on-call\\nengineer when a CloudWatch alarm is triggered","D":"By creating a CloudWatch logs group and metric filters that\\ncapture 500 internal server errors then setting a CloudWatch\\nalarm on that metric","E":"By using Amazon simple notification service to notify an oncall engineer when a CloudWatch alarm is triggered\\nAnswer: A, D, and E\\nExplanation: CloudWatch Logs is used to monitor the applications and\\nsystems via log data. It takes logs from log data; therefore, no code changes\\nare required. Amazon CloudWatch will send an email to you via Amazon\\nSNS. First, build and subscribed to an SNS topic. You may add this SNS\\ntopic when the CloudWatch alarm is being generated to send an email update\\nas the alarm changes.\\n35. An AWS proof-of-concept feature needs to be developed\\nquickly in a development team. Several team members are\\nresponsible for managing, supporting or only viewing the project\\n\\n\\fover time. Each user has their own IAM user account already. Instead\\nof using individual IAM accounts, it is best if the project has a\\ndashboard with an overall view of the project, including designing,\\nreviewing and deploying. The team leader has asked Alas about the\\napproaches the team should use. Which AWS system should Alas\\nsuggest?\\nA. AWS Elastic Beanstalk\\nB. AWS CodeStar\\nC. AWS CodePipeline\\nD. AWS CloudFormation"},"answer":"B","explanation":"For the development of the project, the AWS CodeStar project\\nintegrates other AWS products. The toolchain may include source control,\\nbuild, deployment and so on depending on the AWS CodeStar project model\\nused."},{"id":"36","question":"In order for live debugging in a highly safe environment,\\nRichard needs account-level access to production instances. What\\nshould he do?","options":{"A":"Place the credentials provided by Amazon EC2 onto an MFA\\nencrypted USB drive, and physically share it with each\\ndeveloper so that the private key never leaves the office","B":"Create a user account for each user on all instances and place\\nthe user\'s keys in the credentials file in the appropriate account","C":"Place an internally created private key into a secure S3 bucket\\nwith server-side encryption using customer keys and\\nconfiguration management, create a service account on all the\\ninstances using this private key, and assign IAM users to each\\ndeveloper so they can download the file","D":"Place the credentials provided by Amazon Elastic Compute\\nCloud (EC2) into a secure Amazon Simple Storage Service\\n(S3) bucket with encryption enabled. Assign AWS Identity and\\nAccess Management (IAM) users to each developer so they can\\n\\n\\fdownload the credential file"},"answer":"B","explanation":"The file is located at “./aws/credentials” on Linux, MacOS or\\nUnix or at “C:\\\\Users\\\\USERNAME\\\\aws\\\\credntials” on Windows. An instance\\nprofile is a container for an IAM role that you can use to pass role\\ninformation to an EC2 instance when the instance starts. A private S3 bucket\\ncan be created for each developer, the keys can be stored in the bucket and\\nthen assigned to the instance profile."},{"id":"37","question":"If an Auto-scaling group runs in Amazon EC2, then it easily\\nscales up and down to load in a 10-minute window; but after load\\npeaks, you start having issues with your configuration management\\nsystem where previously finished Amazon EC2 assets are still active.\\nWhat configuration management framework can you use for reliable\\nand efficient control of the cleanup of Amazon EC2 resources?\\n(Choose 2)","options":{"A":"Use Amazon Simple Workflow Service (SWF) to maintain an\\nAmazon DynamoDB database that contains a whitelist of\\ninstances that have been previously launched, and allow the\\nAmazon SWF worker to remove information from the\\nconfiguration management system","B":"Write a small script that is run during the Amazon EC2\\ninstance shutdown to de-register the resource from the\\nconfiguration management system","C":"Create an Auto-scaling group lifecycle hook to hold the\\ninstance in a terminating: wait state until your termination is\\ncomplete. Once termination is complete, notify Auto-scaling to\\ncomplete the lifecycle hook and move the instance into a\\nterminating: proceed state","D":"Configure an Amazon Simple Queue Service (SQS) queue for\\nAuto-scaling actions that has a script that listens for new\\nmessages and removes terminated instances from the\\nconfiguration management system","E":"Write a script that is run by a daily cron job on an Amazon\\nEC2 instance and that executes API Describe calls of the EC2\\n\\n\\fAuto-scaling group and removes terminated instances from the\\nconfiguration management system"},"answer":"B and C","explanation":"On scaling down, if the resources are not getting terminated,\\nyou can use Lifecycle Hooks. It allows you to carry out individual actions by\\nstopping instances when Auto-scaling Group starts or terminates them. If an\\ninstance has been terminated, it stays in a state of waiting until you either\\ncomplete the life cycle action via the entire life cycle action command CLI or\\nCompleteLifecycleAction API, or the timeout period ends.\\nIn order to clean up all resources during this termination process, you need to\\nexecute the script and in this way, also the extra cost is not required.\\nWrite a script and run the script at shutdown to clean resources. This means\\nthat we need to add a script inside the EC2 case and create a task scheduler\\njob and configure it when shut-down takes place."},{"id":"38","question":"Paul has a large number of web servers in an Auto-scaling\\ngroup behind a load balancer. For every visitor, he wants to collect\\ndata from the logs after processing. These logs are filtered and\\nprocess on an hourly basis. After collecting data, he put the data back\\nin a durable store to run the report. Web Servers are constantly\\nlaunching and terminating according to the defined policy, and Paul\\ndoes not want to lose any of the log data during this launching and\\ntermination. Which approaches can meet the demand? (Choose 2)","options":{"A":"On the web server, create a scheduled task that executes a\\nscript that rotates and transmits the logs to Amazon Glacier.\\nEnsure that the operating system shut down procedure triggers\\na log transmission when the EC2 instance is\\nstopped/terminated. Use the Amazon data pipeline to process\\nthe data in Amazon Glacier and run reports every hour","B":"On the web servers, create a scheduled task that executes a\\nscript to rotate and transmit the logs to an Amazon S3 bucket.\\nEnsure that the operating system shut down procedure triggers\\na log transmission when the EC2 instance is\\nstopped/terminated. Use the AWS data pipeline to move log\\ndata from the Amazon S3 bucket to Amazon Redshift in order\\n\\n\\fto process and run reports every hour","C":"Install an Amazon CloudWatch Logs agent on every web\\nserver during the bootstrap process. Create a CloudWatch Log\\nGroup and define a metric filter to create custom metrics that\\ntrack unique visitors for the streaming web server logs. Create\\na scheduled task on an Amazon EC2 instance that runs every\\nhour to generate a new report based on the CloudWatch custom\\nmetrics","D":"Install an AWS Data pipeline logs agent on every web server\\nduring the bootstrap process. Create a log group object in the\\nAWS Data pipeline, and define metric filters to move\\nprocessed log data directly from the web servers to Amazon\\nRedshift and run reports every hour"},"answer":"B and C","explanation":"You can download and configure the CloudWatch Logs agent\\nin an existing EC2 instance. You can publish your own custom metrics to\\nCloudWatch with a CLI or an API.\\nAmazon Redshift is a simple and cost-effective data store for the review of\\nall your data using the standard SQL and Business Intelligence (BI) software.\\nYou can run complicated analytical queries on petabytes of structured data\\nusing sophisticated database enhancement, high-performance column space\\non local drives, and massively parallel query performance. The bulk of\\nresults return in seconds. It allows you to run complex analytic queries\\nagainst petabytes of structured data, using sophisticated query optimization,\\ncolumnar storage on high-performance local disks, and massively parallel\\nquery execution. Most results come back in seconds."},{"id":"39","question":"A financial sector creates a range of web applications with\\nmultiple platforms and programming languages. To meet their\\nbusiness requirements, all applications must be developed and\\ndeployed rapidly and should be highly available. What methods\\nshould they use to quickly deploy these applications?","options":{"A":"Use the AWS CloudFormation docker import service to build\\nand deploy the applications with high availability and multiple\\navailability zones","B":"Develop each application’s code in DynamoDB, and then use\\nhooks to deploy it to Elastic Beanstalk environments with\\nAuto-scaling and elastic load balancing","C":"Store each application’s code in a Git repository, develop\\ncustom package repository managers for each application’s\\ndependencies, and deploy to AWS OpsWorks in multi\\navailability zones","D":"Develop the applications in Docker containers, and then deploy\\nthem to Elastic Beanstalk environments with Auto-scaling and\\nelastic load balancing"},"answer":"D","explanation":"Elastic Beanstalk supports deployment from Docker\\ncontainers. Docker containers have their run time environment; you can\\nchoose your platform, programming language and application dependencies\\nthat are not supported by other platforms. By using Docker with Elastic\\nBeanstalk, you are able to handle the details of provisioning of capacity, load\\nbalancing, scaling and health monitoring automatically."},{"id":"40","question":"There is a set of EC2 instances hosted on AWS. You create a\\nrole and assigned that role to a policy, but you are unable to use that\\nrole with any instance. What is the reason?","options":{"A":"You are not able to associate an IAM role with an instance","B":"You will not be able to use that role with an instance unless\\nyou also create a user and associate it with that specific role","C":"You need to create an instance profile and associate it with that\\nspecific role","D":"You will not be able to use that role with an instance unless\\nyou also create a user group and associate it with that specific\\nrole"},"answer":"C","explanation":"An instance profile is like a container for an IAM role, which\\nis used to pass the role information when the instance starts."},{"id":"41","question":"Alex has to use AWS CloudFormation for the deployment of\\n\\n\\fthe application, instead of OpsWorks and the Elastic Beanstalk. But\\nthere are some types of resources that are not supported by\\nCloudFormation. What should he do now?","options":{"A":"Use a configuration management tool such as Chef, Puppet, or\\nAnsible","B":"Specify the custom resource by separating the template into\\nmultiple templates by using nested stacks","C":"Create a custom resource type using template developer","D":"Specify more mappings and separate the template into multiple\\ntemplates by using nested stacks"},"answer":"C","explanation":"Custom resources allow you to write customized provisioning\\nlogic to templates, which AWS CloudFormation runs whenever you build,\\nmodify or remove the stacks. You can use Custom resources in order to\\ninclude those resources, which are not supported by AWS CloudFormation.\\nThis way, all of your associated resources can be handled in one stack."},{"id":"42","question":"A company application is running on instances with the Autoscaling group. The instances are dynamically booted and the\\nbootstrapping takes more than 15 minutes. You note that Autoscaling records instances as in operation prior to the completion of\\nthe bootstrapping phase. Once you finish bootstrapping, you receive\\nsoftware warnings relating to new instances that cause confusion.\\nYou find the cause: your application monitoring tool is polling the\\nAuto-scaling Service API for instances that are ‘In Service’, and\\ncreating alarms for new previously unknown instances. Which of the\\nfollowing will ensure that new instances are not added to your\\napplication monitoring tool before bootstrapping is completed?","options":{"A":"Increase the desired number of instances in your Auto-scaling\\ngroup configuration to reduce the time it takes to bootstrap\\nfuture instances","B":"Create an Auto-scaling group lifecycle hook to hold the\\ninstance in a pending: wait state until your bootstrapping is\\n\\n\\fcomplete. Once bootstrapping is complete, notify Auto-scaling\\nto complete the lifecycle hook and move the instance into a\\npending:proceed state","C":"Use the default Amazon CloudWatch application metrics to\\nmonitor your application’s health. Configure an Amazon SNS\\ntopic to send these CloudWatch alarms to the correct recipients","D":"Tag all instances on launch to identify that they are in a\\npending state. Change your application monitoring tool to look\\nfor this tag before adding new instances, and the use the\\nAmazon API to set the instance state to ‘pending’ until\\nbootstrapping is complete"},"answer":"B","explanation":"Lifecycle hooks allow you to carry out individual actions by\\nstopping instances when Auto-scaling Group starts or terminates them. After\\nadding it to Auto-scaling group, it starts working in the following ways:\\nAuto-scaling responds to events by scale out or in, by launching\\ninstances and by terminating instances respectively\\nAuto-scaling puts the instance into a wait state (Pending:Wait\\norTerminating:Wait). The instance remains in this state until either\\nyou tell Auto-scaling to continue or the timeout period ends"},{"id":"43","question":"Thomas wants to deploy his multi-tier web-application by\\nusing Blue/Green deployment. Each of them has its individual\\ninfrastructure: Amazon Elastic Compute Cloud (EC2) front-end\\nservers, Amazon ElastiCache clusters, Amazon Simple Queue\\nService (SQS) queues, and Amazon Relational Database (RDS)\\nInstances. What service combination would allow him to distribute\\ntraffic among different versions of his application?","options":{"A":"Create one Elastic Beanstalk application and all AWS\\nresources (using configuration files inside the application\\nsource bundle) for each web application. New versions would\\nbe deployed updating the Elastic Beanstalk application version\\nfor the current Elastic Beanstalk environment","B":"Using AWS CloudFormation templates, create one Elastic\\n\\n\\fBeanstalk application and all AWS resources (in the same\\ntemplate) for each web application. New versions would be\\ndeployed updating a parameter on the CloudFormation\\ntemplate and passing it to the cfn-hup helper daemon, and\\ntraffic would be balanced between them using Weighted Round\\nRobin (WRR) records in Amazon Route53","C":"Using AWS CloudFormation templates, create one Elastic\\nBeanstalk application and all AWS resources (in the same\\ntemplate) for each web application. New versions would be\\ndeployed using AWS CloudFormation templates to create new\\nElastic Beanstalk environments, and traffic would be balanced\\nbetween them using weighted Round Robin (WRR) records in\\nAmazon Route53","D":"Create one AWS Elastic Beanstalk application and all AWS\\nresources (using configuration files inside the application\\nsource bundle) for each web application. New versions would\\nbe deployed using Elastic Beanstalk environments and using\\nthe Swap URLs feature"},"answer":"C","explanation":"For Blue/Green deployment, use CloudFormation templates\\nwith all resources for each web application. Create one Elastic Beanstalk\\napplication. Now, with the use of templates, you can deploy new versions of\\nElastic Beanstalk environments. For distributing traffic among different\\nversions, you can use Route53 Weighted Round Robin. A weighted\\ndistribution enables a canary analysis to be performed where a small\\npercentage of production traffic is entered in a new environment. The new\\ncode can be tested and errors monitored to limit the burst if problems arise. It\\ncan also scale the green environment to support the entire production load if\\nyou use elastic load balancing."},{"id":"44","question":"You created the Lambda function using a template for AWS\\nServerless Application Model (AWS SAM). With AWS SAM\\nintegrated into AWS CodeDeploy, you would like to move users to a\\nnew function with secure Lambda deployments. A template piece is\\nas follows:\\nResources:\\n\\n\\fTheLambdaFunction:\\nType: AWS::Serverless::Function\\nProperties:\\nHandler: index.handler\\nRuntime: nodejs8.10\\nCodeUri: s3://mybucket/mycode.zip\\nAutoPublishAlias: live\\nDeploymentPreference:\\nType: Canary10Percent5Minutes\\nAlarms:\\n# Alarms to monitor\\n- !Ref AliasErrorMetricAlarm\\n- !Ref LatestVersionErrorMetricAlarm\\nHooks:\\n# PreTraffic: validation function before shifting the traffic\\n#PostTraffic: validation function after shifting the traffic\\nPreTraffic: !Ref PreTrafficLambdaForValidation\\nPostTraffic: !Ref PostTrafficLambdaForValidation\\nWhich statement will prove to be WRONG after this template is used?","options":{"A":"If PostTrafficLambdaForValidation has failed, the deployment\\nis rolled back","B":"If PreTrafficLambdaForValidation has failed, the deployment\\nis rolled back","C":"If an alarm such as AliasErrorMetricAlarm appears,\\nCodeDeploy rolls back the deployment","D":"Every 5 minutes, 10 percent of the traffic is shifted to the new\\nversion until all traffic has been shifted"},"answer":"D","explanation":"SAM is using for deploying the serverless application,\\nintegrated with CodeDeploy for save deployment. CodeDeploy supports\\nthree types of deployment Canary, Linear, and All-at-Once for Lambda\\n\\n\\fdeployment.\\nSo the statement: Canary10Percent5Minutes is wrong. This means that 10%\\nof the entire traffic is moved immediately to the new software version. After\\nfive minutes, all other traffic is shifted."},{"id":"45","question":"An enterprise wants to migrate its application from EC2\\ninstances to API Gateway/Lambda Serverless service. For EC2\\ndeployment, they use Blue/Green Deployment while for the\\nserverless application, they prefer to use canary deployment,\\nespecially for API Gateway. For example, only 10% of traffic for a\\nnew release is initially allocated to the Canary stage. How would you\\nset up a canary API Gateway deployment?","options":{"A":"Configure a canary strategy in the relevant API Gateway stage.\\nFor example, allocate 10% of the traffic to the new version.\\nWhen a new API is deployed, select the stage with the enabled\\nCanary","B":"In AWS Lambda console, when a new version is published,\\ncreate a Canary strategy to allocate 10% of the traffic to the\\nnew version. Add Canary stage variables if needed","C":"Use Elastic Beanstalk to deploy API Gateway/Lambda services\\nas Elastic Beanstalk has supported a Canary strategy when a\\nnew version is released for API Gateway/Lambda","D":"There is no straightforward way for API Gateway to implement\\nCanary deployment. Another service such as CodeDeploy, is\\nneeded."},"answer":"A","explanation":"Canary deployment is supported by AWS API Gateway. The\\nAPI is broken up into a Production release and a Canary release with a userconfigured ratio in the case of a Canary release implementation. Canary\\nsettings can be located on the Stage within the API Gateway. The request\\ndistribution of the Stage can, therefore, be adjusted."},{"id":"46","question":"What should be done to store credentials on Amazon EC2\\ninstances for connecting to an Amazon RDS MYSQL database\\ninstance?","options":{"A":"Give the Amazon EC2 instance an IAM role that allows read\\naccess to a private Amazon S3 bucket. Store a file with\\ndatabase credentials in the Amazon S3 bucket. Have your\\nconfiguration management system pull the file from the bucket\\nwhen it is needed","B":"Launch an Amazon EC2 instance and use the configuration\\nmanagement system to bootstrap the instance with the Amazon\\nRDS DB credentials. Create an AMI from this instance","C":"Assign an IAM role to your Amazon EC2 instance, and use this\\nIAM role to access the Amazon RDS DB from your Amazon\\nEC2 instances","D":"Store the Amazon RDS DB credentials in Amazon EC2 user\\ndata. Import the credential into the instance on boot"},"answer":"B","explanation":"To connect the DB instance or DB cluster with IAM users or\\nroles, am IAM policy must be created. Then attach that policy to the user or\\nrole. You can assign access to users, programs or services that usually do not\\nhave access to your AWS assets using roles. For instance, in your AWS\\naccount, you may want to allow users access to resources they do not usually\\nhave, or give users access to resources in another account on another AWS\\naccount."},{"id":"47","question":"John is using Docker to get high consistency between staging\\nand production for the application in EC2 instance, but you are asked\\nfor de-risk deployment due to accidental inconsistencies between\\nstaging and production, which can sometimes lead to unexpected\\nproduction behaviors even when stage test is passed. How do you\\nfurther de-risk the rest of the execution environment knowing that\\nAWS contains many service components that can be used beyond\\nEC2?","options":{"A":"Use AWS Config to force the staging and production stacks to\\nhave configuration parity. Any differences will be detected for\\nyou, so that you are aware of risks","B":"Use AMIs to ensure the whole machine, including the kernel of\\n\\n\\fthe virtual machines, is consistent, since Docker uses Linux\\nContainer (LXC) technology, and we need to make sure the\\ncontainer environment is consistent","C":"Use AWS ECS and Dockers clustering. This will make sure\\nthat the AMIs and machine sizes are the same across both the\\nenvironments","D":"Develop models of your entire cloud system in\\nCloudFormation. Use this model in staging and production to\\nachieve greater parity"},"answer":"D","explanation":"After resources and stack set up, the templates can be reused to\\nreplicate the infrastructure in multiple environments.\\nUse the parameters, mappings and conditions parts to make templates\\nreusable so that you can personalize the stacks when creating them. For\\nexample, you may choose a less cost-effective instance type for your\\ndevelopment environments, in comparison to your production environments,\\nbut all other setups are the same."},{"id":"48","question":"When not running in production during all template launches,\\nyou have to automatically create a Route53 record in\\nCloudFormation. What should you do to implement this?","options":{"A":"Create two templates, one with the Route53 record value and\\none with the null value for the record. Use the one without it\\nwhen deploying to production","B":"Use a parameter for the environment, and add a condition on\\nthe Route53 resource in the template to create the record only\\nwhen the environment is not production","C":"Use a parameter for the environment, and add a condition on\\nthe Route53 resource in the template to create the record with\\nthe null string when the environment is production","D":"Create two templates, one with the Route53 record and one\\nwithout it. Use the one without it when deploying to production"},"answer":"B","explanation":"The optional ‘Conditions’ section contains statements that are\\n\\n\\fdefined when creating a resource or defining a property. For instance, you\\ncan compare if a value is equal to a value. You can create resources based on\\nthe result of its condition. You may use conditions for reusing a model that\\ncan produce resources in a variety of contexts such as a test environment\\nversus a production environment if you have multiple conditions. An\\nEnvironmentType Parameter can be added to your template to accept either\\nthe prod or test as input.\\nYou may include Amazon EC2 in the production environment with certain\\ncapabilities; however, you should use reduced capabilities to save money for\\nthe test environment. You may describe the resource and how they are\\ncreated for each type of environment under conditions."},{"id":"49","question":"A finance sector has assigned a task to Thomas of\\nimplementing a Red / Black Deployment Strategy for a new EC2\\nproject. Two similar environments have been created in AWS. The\\nELB with Auto-scaling Group is attached to each environment. One\\nenvironment will run the live traffic and on the other, new software\\nrelease will be installed. Traffic between two environments is\\nswapped by upgrading Route53 DNS records. Which of the Red /\\nBlack deployment description from the following is not true?","options":{"A":"There is no down time as the old environment can still serve\\ntraffic when the new environment is not ready","B":"With Amazon Route53, only one of the versions gets traffic at\\nany point in time","C":"Route53 can switch the DNS record to both the Red and the\\nBlack environments at the same time","D":"If issues arise during the deployment, rollback can be easily\\nachieved by modifying Route53 to shift traffic back to the\\noriginal environment"},"answer":"C","explanation":"Red/Black deployment is a newer term for Blue/Green\\ndeployment by Netflix. In that, the DNS switch can only happen to one\\nenvironment at a time so the statement that it can switch to both\\nenvironments is wrong."},{"id":"50","question":"Any changes to the AWS resource configurations must be\\ntraced and documented to meet company regulatory requirements.\\nWhat steps are best practices for implementing Infrastructure as a\\nCode and monitoring the health of the environment of the\\norganization? (Choose 2)","options":{"A":"In AWS System Manager “Run Command”, create a JSON\\ncommand document in order to configure a customized logging\\nsystem for EC2 instances","B":"In AWS console, activate the versioning for S3 buckets so that\\nevery object stored in the S3 buckets is version controlled","C":"Create VPC peering between two VPCs so that instances in\\neither VPC can communicate with each other privately","D":"For disaster recovery purposes, a CloudFormation template is\\nused to build a replica of a production environment in another\\nAWS region","E":"Use AWS CLI to configure a VPC, which contains public\\nsubnets, private subnets and a bastion host, which is used for\\nremote access to the instances in private subnets"},"answer":"A and D","explanation":"For resource provisioning, the best practice is to use AWS\\nCloudFormation. In order to create the same resources whenever needed, you\\ncan use the same template. The JSON command document contains Systems\\nManager\'s parameters and action. Itcan also be re-used and version control\\nvia Git."},{"id":"51","question":"When the status of Trusted Advisor checks are changing, you\\nare to provide a solution to notify the team properly. For example,\\nwhen Trusted Advisor\'s cost optimization checks have just identified\\nan Amazon EC2 instance with a low use system, the operating team\\nshould be informed by the Slack Channel to respond to the changes in\\nstatus and potentially reduce cost. In order to satisfy this requirement,\\nwhat two combinations should the company’s DevOps Engineer use?\\n(Choose two)","options":{"A":"Create a new SNS topic, which is in charge of providing\\n\\n\\fcustomized notifications to the Slack channel","B":"Use a Lambda function to pass a customized notification to the\\nSlack channel when check status in Trusted Advisor has\\nchanged","C":"In AWS CloudWatch Logs, create a metric filter for any new\\nlogs, which contain “Check Item Refresh Status”","D":"In the Cost Optimization dashboard of Trusted Advisor,\\nconfigure a notification to an SNS topic when the status check\\nhas found a new event","E":"Create a new CloudWatch Events rule. Add event source as\\n\\"Trusted Advisor\\" and event type as \\"Check Item Refresh\\nStatus\\""},"answer":"B and E","explanation":"In this scenario, a new Amazon CloudWatch Events policy is\\nthe perfect way to monitor trusted advisor’s check. And the target for this\\npolicy is the Lambda function that can design as per the requirement of when\\nand how notifications are sent to the slack channel. Here, CloudWatch events\\nare the best tool for monitoring the changes in Trusted Advisor."},{"id":"52","question":"Juliet’s company needs to reduce costs and all teams must\\ndevelop an operational cost reduction plan, where possible. You work\\nwithin the DevOps team and your team manager asked the team to\\nthink of how to optimize the cost of usage of AWS resources. Which\\ntool from the following DOES NOT help in cost optimization?","options":{"A":"AWS Trusted Advisor","B":"AWS Config","C":"AWS Cost and Usage Report","D":"Amazon S3 Analytics"},"answer":"B","explanation":"While AWS Config offers an AWS resource inventory and\\nconfiguration changes, it does not provide details about saving costs. It can\\nalso be used to ensure compliance, but making suggestions on how to\\noptimize existing resources cost is not easy."},{"id":"53","question":"An organization has its application on AWS EC2 instance, and\\nthey give secure access to AWS Service APIs by defining IAM roles.\\nNow the organization wants to fetch the API keys for using with\\nAWS SDK. What should you as developer professional engineer do\\nto configure the application on the instance?","options":{"A":"When using AWS SDKs and Amazon EC2 roles, you do not\\nhave to explicitly retrieve API keys, because the SDK handles\\nretrieving them from the Amazon EC2 MetaData service","B":"When assigning an EC2 IAM role to your instance in the\\nconsole, in the “Chosen SDK” drop-down list, select the SDK\\nthat you are using, and the instance will configure the correct\\nSDK on launch with the API keys","C":"Within your application code, configure the AWS SDK to get\\nthe API keys from environment variables, because assigning an\\nAmazon EC2 role stores keys in environment variables on\\nlaunch","D":"Within your application code, make a GET request to the IAM\\nService API to retrieve credentials for your user"},"answer":"A","explanation":"When you use IAM roles, you do not need to manage security\\ncredentials for the applications access beacsue the IAM role securely allows\\nthe instance to make API calls."},{"id":"54","question":"Austin wants to deploy his new application in AWS with the\\nBlue/Green Deployment technique. For Blue/Green Deployment he\\nwants to use Java AWS SDK. The application requires services like\\nELB and Auto-scaling group for both current and new releases. But it\\nmay be possible that after the new ASG is attached to ELB, the tests\\nfail. From the following, which is an important consideration in failed\\ncondition?","options":{"A":"Raise a CloudWatch alarm to alert the team when there is a\\nfailure. Also, create a CloudWatch dashboard based on the\\nAuto-scaling group metrics","B":"Rollback the system to the original state by detaching the new\\nASG from ELB","C":"In your Java file, collect logs and send them to AWS\\nCloudWatch Logs, which can help with troubleshooting on the\\nfailure","D":"Use SNS to send a notification to your team so that the team\\ncan react to the failure in time"},"answer":"B","explanation":"Any script or program that manipulates AWS resources should\\nbe auto-healed, and if something goes wrong, the device can be returned to\\nits original state. In other words, the code must be sufficiently robust to\\nensure that when exceptions happen, the system can roll back automatically."},{"id":"55","question":"A company has video games, and for that, they are creating\\nnew APIs. The number of reads is 100 times more than writes. The\\ntop 1% of scores are 100 times more frequently hit than the rest of the\\nscores. How will you as developer professional engineer design the\\nuse of DynamoDB?","options":{"A":"DynamoDB table with roughly equal read and write\\nthroughput, with CloudFront caching","B":"DynamoDB table with roughly equal read and write\\nthroughput, with ElastiCache caching","C":"DynamoDB table with 100x higher read than write throughput,\\nwith CloudFront caching","D":"DynamoDB table with 100x higher read than write throughput,\\nwith ElastiCache caching"},"answer":"B","explanation":"With caching, you can miss a roughly equal number of read to\\nwrite beacuase the majority will hit 1% of scores. We must use AWS\\nElastiCache as we know that the value needs to be set to equal while using\\ncaching because it is able to cache DynamoDB query rather than a distributed\\nproxy cache for content delivery; CloudFront cannot directly cache\\nDynamoDB queries."},{"id":"56","question":"A large number of Lambda functions with an AWS Code\\nDeploy service are deployed by your DevOps team. In CodeDeploy,\\nthe team has already created new apps. References to the deployment\\ngroups that include configurations used during deployment are then\\nset up. Which methods are supported for the deployment\\nconfigurations? (Choose 3)","options":{"A":"CodeDeployDefault.LambdaCanary10Percent5Minutes (Shifts\\n10 percent of traffic in the first increment. The remaining 90\\npercent is deployed five minutes later)","B":"CodeDeployDefault.AllAtATime (Deploys the new version to\\nall Lambda functions at a time )","C":"CodeDeployDefault.LambdaAllAtOnce (Shifts all traffic to the\\nnew Lambda functions at once)","D":"CodeDeployDefault.LambdaHalf (Shifts half traffic to the new\\nLambda functions and then shifts the other half)","E":"CodeDeployDefault.LambdaLinear10PercentEvery10Minutes\\n(Shifts 10 percent of traffic every 10 minutes)","F":"CodeDeployDefault.Immutable (Shifts the traffic to the new\\nLambda functions by performing an immutable update)\\nAnswer: A, C, and E\\nExplanation: When the AWS CodeDeploy deployment group is created,\\nusers must adjust the rules for determining how quickly an application will\\nbe deployed which is also referred to as the \\"Deployment configuration\\".\\nOptions A, C, and E are correct because Canary is supported for shifting 10%\\nof traffic at first then the remaining will be shifted, Lambda AttatOnce option\\nis also best as it shifts all traffic at a time. With Linear, you can shift the\\ntraffic between each increase in equal amounts with an equal number of\\nminutes. You can choose from linear options that specify how much traffic\\nhas shifted in every increase and how many minutes each increase takes.\\n57. It takes more than four hours for your current log analysis\\napplication to report the top 10 users of your web application. You\\nwere asked to implement a system that would allow you to report this\\ninformation in real time, make sure that the report is always up to\\ndate and that the number of requests for your web application has\\n\\n\\fincreased so it will be able to handle it. Choose the cost-effective\\noption that can meet the requirements.\\nA. Configure an Auto-scaling group to increase the size of your\\nAmazon EMR cluster\\nB. Post your log data to an Amazon Kinesis data stream, and\\nsubscribe to your log-processing application so that it is\\nconfigured to process your logging data, configure your\\napplication to Auto-scale to handle the load on demand\\nC. Publish your data to CloudWatch Logs, and configure your\\napplication to Auto-scale to handle the load on demand\\nD. Publish your log data to an Amazon S3 bucket. Use AWS\\nCloudFormation to create an Auto-scaling group to scale your\\npost-processing application, which is configured to pull down\\nyour log files stored in Amazon S3"},"answer":"B","explanation":"For rapid data intake and real time processing of data like logs,\\nmarket data, website clickstreams, etc., you can use Kinesis Streams.\\nAmazon Kinesis makes collecting, processing, and analyzing data in real\\ntime easier and enables you to get timely insights and respond quickly to new\\ninformation. Amazon Kinesis offers key capabilities to process streaming\\ndata in a cost-effective manner, and flexibility to choose the tools that best fit\\nyour application. With Amazon Kinesis, you can enter into real times data\\nsuch as application logs, website clickstreams, IoT telemetry data, and more\\ninto your databases, data lakes and data warehouses, or build your own realtime applications using this data. Instead of having to wait for all your data to\\nbe collected before the processing can start, Amazon Kinesis can process and\\nanalyze data as it arrives and responds in real time. For handling of an\\nincrease in the load, you can use Auto-scaling for your application."},{"id":"58","question":"Management observed the increase in billing cost after\\nreviewing the last quarter monthly bills from Amazon. On\\nresearching for the increase in cost, you found that one of the new\\nservices is doing a lot of GET bucket API calls to S3 to build a\\nmetadata cache of all objects in the application bucket. You need to\\nprovide a solution, which reduces the amount of these GET bucket\\n\\n\\fAPI calls. What would be your strategy?","options":{"A":"Using Amazon SNS, create a notification on any Amazon S3\\nobjects that automatically updates a new DynamoDB table to\\nstore all metadata of the new object. Subscribe the application\\nto the Amazon SNS topic to update its internal Amazon S3\\nobject metadata cache from the DynamoDB table","B":"Update your Amazon S3 bucket’s lifecycle to automatically\\npush a list of objects to a new bucket, and use this list to view\\nobjects associated with the applications bucket","C":"Create a new DynamoDB table. Use the new DynamoDB table\\nto store all metadata of all objects uploaded to Amazon S3.\\nAnytime a new object is uploaded, update the application’s\\ninternal Amazon S3 object metadata cache from DynamoDB","D":"Upload all files to an ElastiCache file cache server. Update\\nyour application to now read all files metadata from the\\nElastiCache file cache server, and configure the ElastiCache\\npolicies to push all files to Amazon S3 for long-term storage"},"answer":"A","explanation":"The best option is to have a notification, which then triggers an\\nupdate to the application to update the DynamoDB accordingly. It is the best\\nway to reduce the usage of GET requests."},{"id":"59","question":"You have a complex system, involving multiple three-tier\\napplications, networking, and IAM policies. The requirements for the\\nnew system remain so you still do not know how many AWS\\ncomponents there will be in the final design. The AWS\\nCloudFormation is used to define these AWS resources so that your\\ninfrastructure can be automated and version controlled. How will you\\nuse AWS CloudFormation to provide your customers with agile new\\nenvironments in an economical and reliable way?","options":{"A":"Manually create one template to encompass all the resources\\nthat you need for the system, so you only have a single\\ntemplate to version-control","B":"Create multiple separate templates for each logical part of the\\n\\n\\fsystem, and provide output from one to the next using an\\nAmazon EC2 instance running the SDK for finer granularity of\\ncontrol","C":"Manually construct the networking layer using Amazon VPC\\nbecause this does not change often and then use AWS\\nCloudFormation to define all other ephemeral resources","D":"Create multiple separate templates for each logical part of the\\nsystem, create a nested stack in AWS CloudFormation, and\\nmaintain several templates to version-control"},"answer":"D","explanation":"As your infrastructure grows, common patterns may arise in\\nwhich each template you declare has the same components. You can separate\\nand create special templates for these common components. This allows you\\nto mix and match various templates, however using nesting stacks to create a\\nsingle, unified stack. Nested stacks are stacks that create other stacks. To\\ncreate nested stacks, use the AWS::CloudFormation::Stack resource in your\\ntemplate to reference other templates."},{"id":"60","question":"William’s team has several pipelines for AWS CodePipeline\\nservice but, even when some pipelines have failed, you have found\\nyour team is unable to get any notification. You want to set up a\\nfeature in order to receive an email and an SMS from your on-call\\nteam if the whole or a certain phase of the pipeline changes its status\\nto FAILED. To enforce this, what combinations of services should\\nyou use? (Choose 2)","options":{"A":"AWS CloudWatch Alarms","B":"AWS SQS","C":"AWS CloudTrail","D":"AWS SNS","E":"AWS CloudWatch Events"},"answer":"D and E","explanation":"Amazon CloudWatch Events may be used to identify and\\nrespond to the pipeline, stage or action changes. CloudWatch Events instead,\\nbased on rules, invokes for one or more target actions if a pipeline, phase, or\\n\\n\\faction enters the specified state. Then CloudWatch Events rule can be\\nconfigured as a target for SNS-topic. Therefore, users will be able to create a\\nsubscription of email or SMS alerts for this new issue, so as to alert the team\\nwhen the state changing is failing."},{"id":"61","question":"HTTP health check has been enabled for Elastic Load\\nBalancing. You see that all instances take health checks after looking\\nat the AWS management console, but your customers are informed\\nthat your site is not responding. What is the reason for this?","options":{"A":"Latency in DNS resolution is interfering with Amazon EC2\\nmetadata retrieval","B":"The application is returning a positive health check too quickly\\nfor the AWS Management Console to respond","C":"The health check in place is not sufficiently evaluating the\\napplication function","D":"The HTTP health checking system is misreporting due to\\nlatency in inter-instance metadata synchronization"},"answer":"C","explanation":"The custom health check is used to evaluate the functionality\\nof the application. If the application functionality is not working and you do\\nnot have custom health checks, the instances will still be seemed as healthy.\\nYou can send the data from your health checks to Auto-scaling if you have\\ncustom health checks, so Auto-scaling can use this data. For example, you\\ncan set the health status of the instance to \\"unhealthy\\" if you determine that\\nthe situation is not working as expected. The next time Auto-scaling conducts\\nan instance health check, the instance will be unhealthy and a replacement\\ninstance will be launched."},{"id":"62","question":"You decided that deployments in blue/green technique would\\nprove beneficial for your company, after a regular review with the\\ndevelopment team. What should you do to incorporate this\\ntechnique?","options":{"A":"Using an AWS CloudFormation template, re-deploy your\\napplication behind a load balancer, launch a new AWS\\n\\n\\fCloudFormation stack during each deployment, update your\\nload balancer to send half your traffic to the new stack while\\nyou test, update the load balancer to send 100% of the traffic to\\nthe new stack after verification, and then terminate the old\\nstack","B":"Create a new Auto-scaling group with the new launch\\nconfiguration and desired capacity the same as that of the\\ninitial Auto-scaling group and associate it with the same load\\nbalancer. Once the new Auto-scaling group instances is\\nregistered with ELB, modify the desired capacity of the initial\\nAuto-scaling group to zero and gradually delete the old Autoscaling group","C":"Re-deploy your application on AWS Elastic Beanstalk, and\\ntake advantage of Elastic Beanstalk deployment types","D":"Using an AWS OpsWorks stack, re-deploy your application\\nbehind an elastic load balancing load balancer and take\\nadvantage of OpsWorks stack versioning, create a new version\\nof your application during deployment, tell OpsWorks to\\nlaunch the new configuration behind your load balancer, and\\nwhen the new version is launched, terminate the old OpsWorks\\nstack"},"answer":"A","explanation":"The blue group is used to carry the production load, and the\\ngreen group is used for stage and deploy with the new code. When it is time\\nto deploy, you must attach the green group to the load balancer to introduce\\nthe traffic to the new environment. The load balancer favors the green Autoscaling group for HTTP / HTTPS listeners because it uses a less outstanding\\nalgorithm for routing requests."},{"id":"63","question":"You have been assigned to use AWS OpsWorks to implement\\nthe scalable distributed system. You have to scale up your distributed\\nsystem on demand. Every node must have a configuration file that\\nincludes the hostnames of the other instances in the layer, as it is\\ndistributed. How should AWS OpsWorks be configured to manage to\\nscale dynamically?","options":{"A":"Update this configuration file by writing a script to poll the\\nAWS OpsWorks service API service for new instances.\\nConfigure your base AMI to execute this script on the\\noperating system start-up","B":"Create a chef recipe to update this configuration file, create\\nyour AWS OpsWorks stack to use custom cookbooks, and\\nassign this recipe to execute when instances are launched","C":"Create a chef recipe to update this configuration file, configure\\nyour AWS OpsWorks stack to use custom cookbooks, and\\nassign this recipe to the configure life cycle event of the\\nspecific layer","D":"Configure your AWS OpsWorks layer to use the AWS\\nprovided recipe for distributed host configuration, and\\nconfigure the instance hostname and file path parameters in\\nyour recipe’s settings"},"answer":"C","explanation":"In the AWS OpsWorks stacks lifecycle event, each set has a\\nlayer of 5 lifecycle events, and each one is associated with a set of recipes\\nthat are specific to that layer. For recipes, you must use custom cookbooks.\\nFor this, you need to configure a lifecycle event, which occurs on all stack’s\\ninstances in the following cases:\\nAttaching an ELB to a layer or detaching one from the layer\\nAssociating an EIP with an instance or disassociating it\\nInstance entering or leaving the online state"},{"id":"64","question":"A company saves its code for web applications in the Git\\nRepository, and now needs to deploy this application in AWS, which\\nis Node.js. How will this be done? (Choose 2)","options":{"A":"Create an AWS CloudFormation template, which creates an\\ninstance with the AWS::EC2::Instance resource type and an\\nAMI with Docker pre-installed. With UserData, install Git to\\ndownload the Node.js application and then set it up","B":"Create a Docker file to install Node.js and gets the code from\\n\\n\\fGit. Use the Dockerfile to perform the deployment on a new\\nAWS Elastic Beanstalk application","C":"Create an AWS CloudFormation template, which creates an\\ninstance with the AWS::EC2::Container resources type. With\\nUserData, install Git to download the Node.js application and\\nthen set it up","D":"Create an Elastic Beanstalk application. Create a Docker file to\\ninstall Node.js. Get the code from Git. Use the command \\"aws\\ngit.push\\" to deploy the application"},"answer":"A and B","explanation":"During the launch of the instance, you can automate the\\nconfiguration tasks and script runs on an instance by passing user data. You\\ncan pass two types of user data to Amazon EC2: shell scripts and cloud-init\\ndirectives. You can also pass this data into the launch wizard as plain text, as\\na file (this is useful for launching instances using the command line tools), or\\nas base64-encoded text (for API calls). Elastic Beanstalk supports the use of\\nDocker container web applications. You can create your own runtime\\nenvironment with Docker containers. You can choose your own platform,\\nprogramming language and any software dependencies that are not provided\\nby other systems such as package managers or tools. Dockers are\\nindependent and include all the settings and software your web application\\nneeds to run. Dockers are free of charge. So using the CloudFormation\\ntemplate to create an instance with pre-installed Docker AMI is a good\\noption. You can also use Docker container with Elastic Beanstalk. In a\\ncontainer, you define all requirements and use this container with Elastic\\nBeanstalk to deploy an application."},{"id":"65","question":"You use CloudFormation to organize the resources of your\\napplication. During your test phase, your Amazon RDS type instance\\nwas changed and the instance was re-created, which resulted in the\\nloss of test data. How are you to avoid this in the future?","options":{"A":"Within the AWS CloudFormation parameter with which users\\ncan select the Amazon RDS instance type, set Allowed Values\\nto only contain the current instance type","B":"Update stack using ChangeSets","C":"In the AWS CloudFormation Template, set the\\nAWS::RDS::DB instances class property to be read-only","D":"Use an AWS CloudFormation stack policy to deny updates to\\nthe instance","E":"Subscribe to the AWS CloudFormation notification\\n“BeforeResourceUpdate”, and call CancelStackUpdate if the\\nresources identified are the Amazon RDS instance"},"answer":"D","explanation":"By default, when you create a stack, all update actions are\\nallowed. So any one can perform updates on the stack’s resources. In order to\\navoid any accidental delete or update of any resource in the stack, you need\\nto use Stack Policy. It is a JSON document in which you defined a specific\\nresource, you can perform update action by explicitly defining the ALLOW\\npermission. When you use Stack policy, it protects all resources in the stack\\nby default. Only one stack policy can be defined per stack and it only applies\\nduring an update of stack."},{"id":"66","question":"The use of AWS CodeStar for the central control and\\nmonitoring of a new application on an AWS system is in\\nconsideration. It is a web application, which is Node.js and deployed\\nin a Lambda function. The project was designed smoothly in 10\\nminutes with a template given by CodeStar. What part of the\\ninformation should you view and track in the AWS CodeStar\\ndashboard for this project?","options":{"A":"The CodePipeline status including the stages of source, build\\nand deploy","B":"The application activity status provided by AWS CloudWatch","C":"The commit history from CodeCommit","D":"All of the above"},"answer":"D","explanation":"The CodeStar project in Lambda is based on the CodeCommit\\ntools (source), CodeBuild (build), CloudFormation (Deploy) and\\nCloudWatch (Monitor) for a Node.js application:\\nThe information in the AWS CodeStar dashboard are:\\n\\n\\fCommit history provided by CodeCommit, which users can use to\\nget a rough idea about the latest code commit\\nContinuous deployment status from CodePipeline, like source,\\nbuild and deploy\\nCloudWatch is integrated with CodeStar, which offers an\\napplication and resource monitoring solution for the company"},{"id":"67","question":"You are working for a start-up that has developed a new\\nmobile app for photo sharing. Your application has grown in\\npopularity in recent months, resulting in a decrease in application\\nperformance due to the increase in load. Your application features a\\ntwo- tier architecture consisting of an Auto-scaling PHP application\\ntier and MySQL RDS instance initially deployed with\\nCloudFormation. The auto-scaling group has a min value of 4 and a\\nmax value of 8. Due to the high CPU usage of the instances, the\\ndesired capacity is now 8. Once analyzed, you are confident that\\nperformance problems stem from a CPU capacity restriction, while\\nmemory use is low. You thus decide to move from M3 instances to\\nC3 instances that are computer-optimized.\\nHow would you deploy this change to reduce interruptions for your endusers as minimum as possible?","options":{"A":"Update the launch configuration specified in the AWS\\nCloudFormation template with the new C3 instance type. Run a\\nstack update with the new template. Auto-scaling will then\\nupdate the instances with the new instance type","B":"Update the launch configuration specified in the AWS\\nCloudFormation template with the new C3 instance type. Also,\\nadd an UpdatePolicy attribute to your Auto-scaling group that\\nspecifies an Auto-scalingRollingUpdate. Run a stack update\\nwith the new template","C":"Sign into the AWS Management Console, copy the old launch\\nconfiguration, and create a new launch configuration that\\nspecifies the C3 instances. Update the Auto-scaling group with\\nthe new launch configuration. Auto-scaling will then update the\\ninstance type of all running instances","D":"Sign into the AWS Management Console and update the\\nexisting launch configuration with the new C3 instance type.\\nAdd an UpdatePolicy attribute to your Auto-scaling group that\\nspecifies an Auto-scalingRollingUpdate"},"answer":"B","explanation":"The AWS::Auto-scaling::Auto-scalingGroup resource supports\\nan UpdatePolicy attribute, which defines how an Auto-scaling group resource\\nis updated when an update to the CloudFormation stack occurs. A common\\napproach is executed, which is a rolling update for updating an Auto-scaling\\nGroup by defining the Auto-scalingRollingUpdate policy. This keeps the\\nsame Auto-scaling Group and, according to the specified parameters,\\nreplaces old instances with new ones."},{"id":"68","question":"You are responsible for an insurance company for the daily\\noperation of the online quota system used in your company to offer\\ninsurance quotes to the public. Your company wishes to use the\\napplication logs created by the system to better understand customer\\nbehavior. You have developed a log management system that meets\\nthe following requirement: All log entries must be kept by the system\\neven during an unplanned instance failure. The consumer monitoring\\nteams need direct access to logs from the last seven days, and\\neventually, all past logs must be accessible to the fraud investigation\\nteams, but they will wait until 24 hours before the logs are usable.\\nWhat steps would you take to achieve these requirements? (Choose\\n3)","options":{"A":"Create a house keeping script that runs on T2 micro instance\\nmanaged by an Auto-scaling group for high availability. The\\nscript uses the AWS API to identify any unattached Amazon\\nEBS volumes containing log files. Your house keeping script\\nwill mount the Amazon EBS volume, upload all logs to\\nAmazon S3, and then delete the volume","B":"Configure your application to write logs to the instances\\nephemeral disk, because this storage is free and has good write\\nperformance. Create a script that moves the logs from the\\ninstance to Amazon S3 once an hour","C":"Write a script that is configured to be executed when the\\ninstance is stopped or terminated and that will upload any\\nremaining logs on the instance to Amazon S3","D":"Create an Amazon S3 lifecycle configuration to move log files\\nfrom Amazon S3 to Amazon Glacier after seven days","E":"Configure your application to write logs to the instance’s\\ndefault Amazon EBS boot volume, because this storage already\\nexists. Create a script that moves the logs from the instance to\\nAmazon S3 once an hour","F":"Configure your application to write logs to a separate Amazon\\nEBS volume with the “delete or termination” field set to false.\\nCreate a script that moves the logs from the instance to\\nAmazon S3 once an hour\\nAnswer: A, D, and F\\nExplanation: The glacier is the best option because all logs must be stored\\nindefinitely. The data can be streamed from S3 to Glacier with Lifecycle\\nevents. You can specify the lifecycle management of objects inside a bucket\\nby configuring a lifecycle. The configuration is a set of rules that define an\\nAmazon S3 action for a group of objects in each rule.\\nWe can use a cost-effective EC2 instance to ensure the minimum memory\\nrequirements for the OS and script execution. In this case, a t2.micro instance\\ncan be used, taking into account the computing resource requirements of the\\ninstance and the cost factor.\\nThe EC2 uses the EBS volumes, and the logs are stored in EBS volumes\\nmarked for non-termination. It is one of the ways to fulfill the requirement.\\n69. A company wants to make easier deployment and reduce the\\ntime taken by the deployment for the improvement of deployment as\\nthey deploy five times a week at max. They hired you as the DevOps\\nengineer to create a CI pipeline that can build AMIs. How will you\\ndo this?\\nA. By having the CI system launch a new instance, then\\nbootstrapping the code and dependencies on that instance, and\\ncreating an AMI using the CreateImage API call\\nB. By using OpsWorks to launch an EBS-backed instance, then\\n\\n\\fusing a recipe to bootstrap the instance, and then having the CI\\nsystem use the CreateImage API call to make an AMI from it\\nC. By using a dedicated EC2 instance with an EBS Volume, then\\ndownloading and configuring the code, and creating an AMI\\nout of that\\nD. By uploading the code and dependencies to Amazon S3,\\nlaunching an instance, downloading the package from Amazon\\nS3, then creating the AMI with the CreateSnapshot API call"},"answer":"A","explanation":"As the number of calls for deployment is less, open sources\\nlike Jenkins can be used for CI-based systems. It is used as an automation\\nserver and as a simple CI. For using the CI system to launch the instance and\\nfor user data, define bootstrap code and then create AMI by using\\nCreateImage API."},{"id":"70","question":"John has an ELB with an Auto-scaling group and he needs to\\nphase-out all old instances and replace them with new types of\\ninstance. What steps should he take to fulfill his task? (Choose 2)","options":{"A":"Use the Newest Instance to phase out all instances that use the\\nprevious configuration","B":"Attach an additional Auto-scaling configuration behind the\\nELB and phase in newer instances while removing older\\ninstances","C":"Attach an additional ELB to the Auto-scaling configuration and\\nphase in newer instances while removing older instances","D":"Use the Oldest Launch Configuration to phase out all instances\\nthat use the previous configuration"},"answer":"B and D","explanation":"Auto-scaling terminates the instances that have an old\\nconfiguration policy while using the OldestLaunchConfiguration policy. This\\nis helpful when you are updating a group and phasing out the instances from\\nthe previous configuration."},{"id":"71","question":"Alan has created a web app and stores it for static website\\n\\n\\fhosting in an Amazon S3 bin. This application can access the data in\\nAmazon DynamoDB table using the AWS SDK for JavaScript in the\\nbrowser. How can you ensure that API keys are kept secure to access\\nDynamoDB data?","options":{"A":"By creating an Amazon S3 role in IAM with access to the\\nspecific DynamoDB tables, and assigning it to the buckets\\nhosting the website","B":"By configuring S3 bucket tags with AWS access keys for the\\nbucket hosting the website so that the application can query\\nthem for access","C":"By configuring a web identity federation role within IAM to\\nenable access to the correct DynamoDB resources and\\nretrieving temporary credentials","D":"By storing AWS keys in global variables within the application\\nand configuring the application to use these credentials when\\nmaking requests"},"answer":"C","explanation":"You do not need custom sign-in software to build or manage\\nyour own user identities with the web identity federation. You can instead\\nsign in with an IDP — such as login to Amazon, Facebook, Google or any\\nother OpenID Connect (OIDC) compliant IDP— and you will obtain an\\nauthentication token, then exchange that token for temporary security\\ncredentials in AWS that map to an IAM role with permissions to use the\\nresources in your AWS account. Using an IDP helps you keep your AWS\\naccount secure because you do not have to embed and distribute long-term\\nsecurity credentials with your application."},{"id":"72","question":"An educational institution uses AWS to host its application. It\\nuses CloudFormation templates and Auto-scaling. Now, the institute\\nobserves that the number of students using applications is increasing\\nand it is facing many performance issues. If the EC2 instance type is\\nchanged to C3, the performance will improve. What is the proper way\\nto change the instance type?","options":{"A":"Update the AWS CloudFormation template that contains the\\n\\n\\flaunch configuration with the new C3 instance type. Run a\\nstack update with the updated template, and Auto-scaling will\\nthen update the instances one at a time with the new instance\\ntype","B":"Update the existing launch configuration with the new C3\\ninstance type. Add an UpdatePolicy attribute to your Autoscaling group that specifies an Auto-scaling RollingUpdate in\\norder to avoid downtime","C":"Update the launch configuration in the AWS CloudFormation\\ntemplate with the new C3 instance type. Add an UpdatePolicy\\nattribute to the Auto-scaling group that specifies an AutoscalingRollingUpdate. Run a stack update with the updated\\ntemplate","D":"Copy the old launch configuration, and create a new launch\\nconfiguration with the C3 instances. Update the Auto-scaling\\ngroup with the new launch configuration"},"answer":"C","explanation":"To change the instance type, you need to change the template\\nand also the UpdatePolicy attribute of Auto-scaling in which you specify to\\nperform updates on Auto-scaling on updating in the stack. This is done by\\ndefining Auto-scalingRollingUpdate Policy to replace the old instance with\\nnew."},{"id":"73","question":"Louis deployed his application in AWS. The application has\\ntwo ASGs attach to an ELB. From these two ASGs, ASG1 is live and\\nASG2 is idle. The new version is deployed on ASG2 and fully\\nreviewed once new releases are published. If no error is found, ASG2\\ncomes alive and ASG1 gets idle. What kind of deployment / delivery\\nmethod is possible to use in this case? (Choose 2)","options":{"A":"Canary Deployment","B":"Blue/Green Deployment","C":"Rolling Deployment","D":"A/B Testing","E":"Red/Black Deployment"},"answer":"B and E","explanation":"We know that the Blue / Green implementation has two similar\\nsurroundings and can be switched to a new environment for all traffic. In this\\nexample, the Blue / Green deployment has been carried out using two ASGs.\\nThe same concept is represented both in red / black and in blue / green. The\\nRed version (ASG1) is created live in this case. The machine guides all\\ntraffic to ASG2 when it (the Black version) is fully operational."},{"id":"74","question":"A company has an application, in which the changes occur, but\\nif the changes fail, then they rollback the updates, but it takes 5-6 hrs\\nfor rolling back. How will you as a DevOps engineer provide a\\nsolution to reduce the rolling back duration?","options":{"A":"Use OpsWorks and re-deploy using the rollback feature","B":"Use S3 to store each version and then re-deploy with Elastic\\nBeanstalk","C":"Use Elastic Beanstalk and re-deploy using Application\\nVersions","D":"Use CloudFormation and update the stack with the previous\\ntemplate"},"answer":"C","explanation":"Using Elastic Beanstalk is best for development as it quickly\\ndeploys the application in the AWS Cloud and your management complexity\\nreduces. You simply upload your application, and the details of capacity\\nprovisioning, load balance, scaling, and health controls are automatically\\nmanaged by AWS Elastic Beanstalk."},{"id":"75","question":"You have just joined an IT company to take responsibility for\\nthe management of AWS resources with another DevOps engineer.\\nAfter a while, you find that an Elastic Load Balancer health check\\ntimer needs to be changed. The ELB was set up a year ago with Autoscaling settings and you can find the CloudFormation template for\\nthis setup, which is in a CodeCommit repository. During the year,\\nsuch configurations can be highly altered by somebody. However, the\\nmodification details for the CloudFormation stack are not recorded.\\nWhat are you supposed to do to update the ELB health check timer?","options":{"A":"Edit the template properly with the new ELB health check\\ntimer. Create a Change Set for the CloudFormation stack using\\nthe new template. If the Change Set is ok, execute the Change\\nSet","B":"Use “Detect drift” to understand the changes since the stack\\nwas created. Update the template accordingly together with the\\nnew health check timer and submit the new code to\\nCodeCommit. Update the stack using the new template","C":"Modify the template with the new Elastic Load Balancer health\\ncheck timer. Update the stack using the new template. Submit\\nthe new template to the CodeCommit repository","D":"Delete the CloudFormation stack since the changes in the stack\\nare not tracked in git. Rewrite the CloudFormation template\\nwith the new health check timer and create a new\\nCloudFormation stack using the new template. Commit the\\ncode changes to CodeCommit repository."},"answer":"B","explanation":"\'Detect drift\' is useful to understand the changes to the stack\\nresources, it generally detects the changing in stack’s configuration outside\\nthe AWS CloudFormation. The CodeCommit file, which is a best practice for\\nInfrastructure as a Code, then tracks and monitors all changes. Detect drift\\ntakes time depending on the resources in the stack."},{"id":"76","question":"You will be working for a SaaS organization as the new head\\nof operations. Your CTO asked you to simplify and speed up\\ndebugging every part of the operation. She complains she does not\\nknow what is happening in a complex, service-based architecture\\nbecause the developers only log onto the disk and with so many\\nservices, it is very difficult to find mistakes in logs. How can you best\\nfulfill your CTO’s requirement?","options":{"A":"Begin using CloudWatch logs on every service. Stream all log\\ngroups into an AWS elastic search service domain running\\nKibana 4 and perform log analysis on a search cluster","B":"Copy all log files into AWS S3 using a cron job on each\\n\\n\\finstance. Use an S3 notification configuration on the put bucket\\nevent and publish an event to AWS Lambda. Use the Lambda\\nto analyze logs as soon as they come in and flag issues","C":"Begin using CloudWatch logs on every service. Stream all log\\ngroups into S3 objects. Use AWS EMR cluster jobs to perform\\nadhoc map reduce analysis and write new queries when needed","D":"Copy all log files into AWS S3 using a cron job on each\\ninstance. Use an S3 notification configuration on the put bucket\\nevent and publish an event to AWS Kinesis. Use apache sparks\\non AWS EMR to perform at-scale stream processing queries on\\nthe log chunks and flag issues."},"answer":"A","explanation":"The Elasticsearch Service from Amazon makes it easy to\\nimplement, operate and scale Elasticsearch for log-analysis. The Amazon\\nElasticsearch Service offers the easy-to-use APIs and real-time functionality\\nof Elasticsearch, together with the availability, scalability, and security\\nrequired by production workloads. It provides integrated solutions with\\nKibana, Logstash, Amazon Kinesis Firehose, AWS Lambda, and Amazon\\nCloudWatch so that you can quickly move from raw data to feasible insights."},{"id":"77","question":"In order to meet an ongoing compliance audit in the business,\\nWayne needs to test and set up CloudWatch alarms for the AWS\\nresources in production like EC2, DynamoDB, Lambda, etc. To save\\ntime, he plans to use existing CloudWatch Metrics and create\\nCloudWatch alarms accordingly. Which scenarios are suitable for\\nthis? (Choose 2)","options":{"A":"When there are some critical Tomcat errors happening in EC2\\ninstances, immediately send an SMS to the on-call support","B":"When the average EC2 instance memory usage exceeds 95\\npercent for five minutes, trigger an alarm and launch another\\nEC2 instance","C":"When someone in the development team has provided\\ncomments on commit or pull requests in AWS CodeCommit\\nservice, notify relevant team members","D":"If the latency of ELB exceeds 10 seconds over two minutes,\\n\\n\\fcreate an alarm and send an email notification to your team","E":"Notify the team when excessive throttling occurs for a\\nDynamoDB table, which stores users’ subscription data"},"answer":"D and E","explanation":"As Wayne wants to use the existing CloudWatch metrics, then\\nhe should use the ELB metric, which is the Average Latency metric. On the\\nbasis of this metric, he must create an alarm and send notification via email.\\nThen he should use the CloudWatch metric that is supported by DynamoDB\\nfor throttled read/write events or requests."},{"id":"78","question":"An organization has an application of which it released a new\\nfeature, making it a highly available application. For testing the new\\nfeature, A/B testing is used. The logs from each updated instance\\nneed to analyze real-time to observe the working. If the behavior of\\nthe log is anomalous, the instances are changes to a more stable one.\\nWhat would be your strategy in this scenario?","options":{"A":"Ship the logs to a large Amazon EC2 instance and analyze the\\nlogs in a live manner","B":"Ship the logs to Amazon CloudWatch Logs and use Amazon\\nEMR to analyze the logs in a batch manner each hour","C":"Ship the logs to Amazon S3 for durability and use Amazon\\nEMR to analyze the logs in a batch manner each hour","D":"Ship the logs to an Amazon Kinesis Stream and have the\\nconsumers analyze the logs in a live manner"},"answer":"D","explanation":"For rapid data intake and real time processing of data like logs,\\nmarket data, etc., you can use Kinesis Streams. You can use data collected\\ninto Kinesis Streams for simple data analysis and reporting in real time.\\nKinesis Streams offers a quick intake of data feed; before uploading data for\\nintake, you do not need to batch the data on the servers."},{"id":"79","question":"Jordan needs to find a solution to cost optimization without\\nimpacting the services of AWS. What should he do? (Choose 2)","options":{"A":"Turn off certain non-production instances during weekends\\n\\n\\fwhen no one is actually using them and turn on these instances\\non Monday morning","B":"To save cost, use spot instances instead of on-demand instances\\nfor Jenkins, which is the production CI/CD pipeline","C":"In order to decide the right size of EC2 instances, create a\\nCloudFormation stack, with EC2 instances, to get the\\nutilization data from CloudWatch and upload it to the Amazon\\nRedshift cluster for right-sizing analysis. Delete the stack after\\nthe analysis result is fetched","D":"For non-production environments, use default General Purpose\\nSSD (gp2) instead of EBS Throughput Optimized HDD (st1)\\nstorage as SSD costs half the price","E":"To save the cost of the VPN connections between VPC and onpremises servers, replace the VPN connections with a Direct\\nConnect, which is cheaper and also more reliable"},"answer":"A and C","explanation":"Because unused EC2 instances can be stopped in order to save\\nmoney, this task may automatically be executed by a script or pipeline to stop\\nand start the instances automatically.\\nYou can also use the Redshift cluster because this is an automated method of\\nfinding the existing data from CloudWatch and getting 3 recommendations\\nfor the right size of EC2 instances."},{"id":"80","question":"The design of a CI / CD pipeline is your responsibility for the\\ncurrent request. One of the main requirements is that the AWS\\napplications should be highly available and they should auto-heal\\nwhen errors occur. Which AWS resources will be helpful for you?\\n(Choose 2)","options":{"A":"Create a CloudWatch alarm based on the CPU metrics and\\nnotify the on-call team via an SNS subscription","B":"Create an Elastic Load Balancer and attach an Auto-scaling\\ngroup to it","C":"Create an OpsWorks stack with the auto-healing feature\\nenabled for its layer","D":"Create an Elastic Beanstalk application with the auto-healing\\nfeature enabled in the environment configuration","E":"Use a CloudFormation template to launch a stack for both\\ninfrastructure and application"},"answer":"B and C","explanation":"In order to help users to deploy fully accessible, fault tolerant\\nand self-healing software, AWS has supplied a number of services. The key\\nto auto-healing is to automatically start a new one whenever an error occurs\\nwhen the problematic instance is stopped.\\nSince OpsWorks has provided its layer with an auto-healing feature, if an\\nagent fails to communicate with the OpsWorks service for some time, AWS\\nOpsWorks Stack automatically replaces the failed instance when the feature\\nof auto-healing is enabled.\\nELB and ASG will contribute very quickly to achieving an elastic selfhealing process. If any instance does not perform well and the ELB health\\ntest fails, ASG stops and starts a new one to replace it."},{"id":"81","question":"Joel plans to use the Amazon RDS facility for fault tolerance\\nof the application and needs its features. What is one of the benefits\\nof Amazon RDS Multi-Availability Zone?","options":{"A":"A second standby database is deployed and maintained in a\\ndifferent availability zone from the master, using asynchronous\\nreplication","B":"A second standby database is deployed and maintained in a\\ndifferent region from the master, using asynchronous\\nreplication","C":"A second, standby database is deployed and maintained in a\\ndifferent region from the master, using synchronous replication","D":"A second, standby database is deployed and maintained in a\\ndifferent availability zone from the master, using synchronous\\nreplication"},"answer":"D","explanation":"The multi-AZ implementations of Amazon RDS have\\nimproved Database (DB) availability and durability, making them ideal for\\n\\n\\fwork loads in production databases. Amazon RDS automatically generates a\\nprimary DB instance and synchronizes the data into a standby instance in an\\nalternative Availability Zone (AZ) when providing the Multi-AZ DB\\nInstance. Each AZ operates on its own physical, separate and very reliable\\ninfrastructure. An RDS would automatically fail to the standby (or read\\nreplica in Amazon Aurora), in case of an infrastructure failure, in order to\\nrestart server operations as soon as the failover is complete."},{"id":"82","question":"A company has an application in which whenever a user\\nuploads a photo in the app, a message is sent immediately to their\\nfriends. To save the photo information per user, an AWS DynamoDB\\ntable has been created and the DynamoDB stream is enabled to\\ncapture the item changes. Which service can be integrated with\\nDynamoDB in order to process and notify events?","options":{"A":"An S3 Bucket","B":"A Lambda Function","C":"An SNS Notification","D":"A CloudWatch Event Rule"},"answer":"B","explanation":"In DynamoDB, you can enable the stream feature and after\\nthat, a time-ordered sequence of item-level operations is captured, which can\\nbe accessed via its API. For Lambda function, you can configure DynamoDB\\nStream as a trigger as so Lambda picks up a new stream record and gets it\\nprocessed."},{"id":"83","question":"The firm has a DevOps department that handles all other\\ndepartments of the company\'s AWS accounts. A number of\\nCloudWatch Event Rules are defined in the master AWS account to\\nallow CloudWatch Events to be sent in other AWS accounts. In order\\nto do this, permissions are set up in the master account, which is in\\nregion ap-southeast-2 with default CloudWatch event bus. Which\\nentities can be added to allow submitting events on the master\\naccount in CloudWatch Event Bus? (Choose 2)","options":{"A":"IAM Groups","B":"An AWS Account, which creates a CloudWatch Events rule in\\nregion ap-south-1 and sends events to the master account","C":"Everybody (all AWS accounts)","D":"Another AWS Organization","E":"Certain IAM users in the same AWS Organization"},"answer":"C and D","explanation":"You first need to update the privileges on the default event bus\\non your account to accept events from other accounts or organizations. The\\ndefault event bus accepts the events from AWS services, other authorized\\nAWS accounts, and PutEvents calls.\\nYou may assign account IDs or Organization IDs when you change your\\ndefault event bus to grant permission to other AWS accounts. Or, you can\\nreceive events of all AWS accounts. For permission, you need to select\\neverybody, which includes all AWS accounts."},{"id":"84","question":"For the integration of thousands of Microservices, AWS ECS\\nwas used by a major IT company. There is a new management\\nrequirement that every state change must be reported to a group\\nchannel of Microsoft teams either by ECS container instances or ECS\\ntasks. When a change occurs, the alert notifications should be\\nreadable and sent directly. Which two methods in combinations meet\\nthis requirement? (Choose 2)","options":{"A":"Create a Lambda function to analyze the state change event in\\nECS and then provide an appropriate message to the channel in\\nMicrosoft Teams","B":"Set up alarms in ECS CloudWatch Metrics. When the state of\\neither ECS instance or ECS task changes, generate an alarm to\\ntrigger an SNS topic","C":"Configure a new SNS topic called ECS_Status_Change.\\nRegister the SNS as the trigger of a Lambda function to\\ngenerate custom readable alarms","D":"Create a T2 micro instance to handle the customized\\nnotifications. Integrate it with Microsoft Teams via its\\nIncoming Webhook","E":"Add a new CloudWatch Events rule for all event types of ECS.\\n\\n\\fAdd a Lambda function as the target when an event is triggered"},"answer":"A and E","explanation":"The combination of the Lambda and CloudWatch event will be\\nthe best solution for the given requirement. In this case, notifications are\\nexpected to be sent immediately when there is a state change for ECS.\\nCloudWatch Events should be considered as it integrates with ECS and can\\ntrigger a CloudWatch Event for its target. As CloudWatch Events can trace\\nthe status change of both ECS container instance and ECS task, Lambda can\\nbe used to generate the message so you can integrate it with CloudWatch\\nevents."},{"id":"85","question":"An enterprise has a hybrid network consisting of multiple EC2\\ninstances and Raspberry Pi devices. In order to manage all servers\\nlike patching strategy on both cloud and an on-premises environment,\\nthe enterprise plans to use AWS System Manager. The agents are\\nalso installed on all servers. Now, it wants to identify which server is\\nthe Raspberry Pi server from the Managed Instances Console. Which\\nfeature of the Raspberry Pi distinguishes it from others?","options":{"A":"The machines prefixed with \\"i-\\" are Raspberry Pi devices","B":"The Raspberry Pi instances have the IP addresses that are not\\nwithin the VPC IP range","C":"The machines prefixed with \\"mi-\\" are Raspberry Pi devices","D":"The machines that have the platform name as CentOS Linux\\nare Raspberry Pi devices"},"answer":"C","explanation":"Within a hybrid environment, the AWS Systems Administrator\\nwill configure EC2 instances or on-site servers. Support is provided for\\nvarious Linux distributions, including Raspberry Pi devices and Microsoft\\nWindows Server. As the \\"mi-\\" suffix is for hybrid instances, the suffix \\"i-\\" is\\nfor the Amazon EC2 instances."},{"id":"86","question":"An Auto-scaling group is configured to launch EC2 instances\\nfor the application, but you observed that the Auto-scaling group is\\nnot launching the instances in the right proportion. Instances are\\n\\n\\flaunched too fast. How will you resolve this issue? (Choose 2)","options":{"A":"By setting a custom metric, which monitors a key application\\nfunctionality for the scale-in and scale-out process","B":"By adjusting the CPU threshold set for the Auto-scaling scalein and scale-out process","C":"By adjusting the memory threshold set for the Auto-scaling\\nscale-in and scale-out process","D":"By adjusting the cool down period set for the Auto-scaling\\ngroup"},"answer":"B and D","explanation":"The Auto-scaling cooldown period is a configuration setting\\nthat makes sure that Auto-scaling does not launch or terminate any additional\\ninstance until the previous activity takes place."},{"id":"87","question":"In a very large organization, there are multiple applications,\\nwhich are constructed in different programming languages. How can\\nyou deploy these applications as fast as possible?","options":{"A":"By developing each app in a separate docker container and\\ndeploying them using Elastic Beanstalk","B":"By developing each app in one docker container and deploying\\nthem using Elastic Beanstalk","C":"By creating a Lambda function deployment package consisting\\nof code and any dependencies","D":"By developing each app in a separate docker container and\\ndeploying them using CloudFormation"},"answer":"A","explanation":"EBS supports the deployment of a web application from a\\ndocker container. In docker, you can define your run time environment where\\nyou can choose your platform, programming language and application\\ndependencies that are not supported by other platforms. Docker containers\\nare self-contained and include all the configuration information and software\\nyour web application requires to run."},{"id":"88","question":"Aidan has a company in which he helps his IT team for setting\\nup the AWS CodeCommit repositories. He and his team choose to\\npull or push code with their current SSH keys, however, two team\\nmembers told him that they could NOT run successfully on their own\\nLinux machines on CodeCommit repositories. How would he solve\\nthis problem? (Choose 2)","options":{"A":"Check whether the public SSH key has been uploaded to the\\nIAM Security Credential tab","B":"If Git is used, check if the IAM users are able to access AWS\\nCodeCommit using their AWS credentials","C":"Check if the IAM user has a proper policy to access\\nCodeCommit resource","D":"In the IAM Security Credential tab, check if the user’s private\\nSSH key has been uploaded","E":"Check if the IAM user has activated MFA"},"answer":"A and C","explanation":"CodeCommit can be accessed by several methods: HTTPS,\\nSSH and AWS access keys. You need to ensure for SSH that IAM rules, such\\nas AWSCodeCommitFullAccess, are associated with the IAM user. To\\nauthenticate the repository, access to the public should be uploaded.\\nWith SSH connections, you create public and private key files on your local\\nmachine that Git and CodeCommit use for SSH authentication."},{"id":"89","question":"A number of instances are running on your OpsWork stacks. If\\nyou want to install security updates, what is the AWS\\nrecommendation in accordance with this task? (Choose 2)","options":{"A":"Create a CloudFormation template, which can be used to\\nreplace the instances","B":"Create and start new instances to replace your current online\\ninstances. Then delete the current instances","C":"On Linux-based instances in Chef 11.10 or older stacks, run\\nthe Update Dependencies stack command","D":"Create a new Opswork stack with the new instances"},"answer":"B and C","explanation":"By default, after an instance booting is completed, AWS\\nOpsWorks stacks automatically install the most recent updates during setup.\\nAWS OpsWorks stacks do not automatically install updates after an instance\\nis online, in order to prevent interruptions such as restarting of the application\\nserver. Instead, you should manage online updates yourself so that any\\ndisturbance can be minimized.\\nYou may use one of the following for an update:\\nCreate and launch new instances to replace your current online\\ninstances. Then delete the existing instances\\nRun the command Update Dependencies Stack on Linux based\\ninstances in Chef 11.10 or previous stacks, which installs a\\ncurrent set of security patches and updates in the instance you\\nspecified"},{"id":"90","question":"AWS CodeDeploy is used to manage several deployment\\nphases, including feature testing, system integration testing, and\\nmanufacturing. Every phase has a CodeDeploy deployment group.\\nEach group is supplemented with tags with relevant targets. Like\\nservers containing the product, tag is deployed only if the Production\\nDeployment Group creates a new deployment. Which targets\\nCANNOT be added to Deployment Group by Tags in AWS\\nCodeDeploy? (Choose 2)","options":{"A":"Red Hat Enterprise Linux (RHEL) on-premises instances","B":"EC2 Instance (Microsoft Windows Server 2016)","C":"Lambda Function written in Python 3.6","D":"EC2 Auto-scaling Groups","E":"ECS Cluster with an Amazon Linux AMI"},"answer":"C and E","explanation":"Users can select the targets via tags during the creation of\\nDeployment Group in CodeDeploy from any Amazon EC2 Auto-scaling\\nGroups, Amazon EC2 instances or on-site instances but Lambda function and\\nECS cannot be selected."},{"id":"91","question":"The Docker Containers for a Java Application is controlled by\\nAWS ECS within your business. Several important features have\\nrecently been developed to satisfy the market requirements and the\\nteam is under pressure to deliver new releases. When the ECS role\\ncomes into stopped status, the manager asks you to alert the team\\nboth on the slack channel and email so that the relevant staff can take\\nimmediate action. What is the best way for you to do this?","options":{"A":"Configure the ECS cluster instances to send log files in\\n/var/log/docker/ to CloudWatch Logs. Create a Metric Filter to\\nsearch for the keyword “STOPPED”. Create an alarm to trigger\\nan SNS notification whenever there is a match. Subscribe a\\nLambda function to the SNS topic, which sends a message to\\nthe Slack channel and email list","B":"Create a CloudWatch Event rule as below. Use a Lambda\\nfunction as the target to send notifications to both the Slack\\nchannel and the relevant email list.\\n{\\n\\"source\\": [</p>\\n\\"aws.ecs\\"</p>\\n],\\n\\"detail-type\\": [\\n\\"ECS Container Instance State Change\\"\\n]\\n\\"detail\\": {\\n\\"desireStatus\\": [\\n\\"STOPPED\\"\\n]\\n}\\n}","C":"Configure the ECS cluster instances to send log files in\\n/var/log/ecs/ to CloudWatch Logs. Create a Metric Filter to\\nsearch for the keyword “STOPPED”. Create an alarm to trigger\\n\\n\\fa Lambda function whenever there is a match. The Lambda\\nfunction sends customised notifications to the Slack channel\\nand email list","D":"Create the following CloudWatch Event rule. Add a target of\\nLambda function to send a notification in Slack and another\\ntarget of SNS to send the email.\\n{\\n\\"source\\": [</p>\\n\\"aws.ecs\\"</p>\\n],\\n\\"detail-type\\": [\\n\\"ECS Task State Change\\"\\n]\\n\\"detail\\": {\\n\\"lastStatus\\": [\\n\\"STOPPED\\"\\n]\\n}\\n}"},"answer":"D","explanation":"In order to provide immediate notification, you can integrate\\nAmazon ECS with CloudWatch events. Once ECS reaches the \\"STOPPED\\"\\nstate, the rule will immediately trigger the targets (Lambda and SNS) with\\nthis CloudWatch Event rule."},{"id":"92","question":"Phillip is using AWS Systems Manager to maintain EC2\\ninstances. For example, for instances with a tag of \\"QA\\", you can run\\ncommand to execute a shellscript. However, you want to limit the\\nutilization of the \\"Run Command\\" feature for certain IAM users on\\nsecurity issues. For these specific users, you need an IAM policy to\\nonly allow them to run command for instances that have the\\n“department” tag of “dev1” or “dev2”. Which IAM policy can help\\nyou to achieve this requirement?","options":{"A":"{\\n\\"Version\\":\\"2012-10-17\\",\\n\\"Statement\\":[\\n{\\n\\"Effect\\":\\"Allow\\",\\n\\"Action\\":[\\n\\"ssm:SendCommand\\"\\n],\\n\\"Resource\\":\\"*\\",\\n\\"Condition\\":{\\n\\"StringLike\\":{\\n\\"ssm:resourceTag/department\\":[\\n\\"dev1\\"\\n]\\n\\"ssm:resourceTag/department\\":[\\n\\"dev2\\"\\n]\\n}\\n}\\n}\\n]\\n}","B":"{\\n\\"Version\\":\\"2012-10-17\\",\\n\\"Statement\\":[\\n{\\n\\"Effect\\":\\"Allow\\",\\n\\"Action\\":[\\n\\"ssm:RunCommand\\"\\n],\\n\\"Resource\\":\\"*\\",\\n\\"Condition\\":{\\n\\"StringLike\\":{\\n\\n\\f\\"ssm:resourceTag/department\\":[\\n\\"dev1\\"\\n]\\n\\"ssm:resourceTag/department\\":[\\n\\"dev2\\"\\n]\\n}\\n}\\n}\\n]\\n}","C":"{\\n\\"Version\\":\\"2012-10-17\\",\\n\\"Statement\\":[\\n{\\n\\"Effect\\":\\"Allow\\",\\n\\"Action\\":[\\n\\"*\\"\\n],\\n\\"Resource\\":\\"*\\",\\n\\"Condition\\":{\\n\\"StringLike\\":{\\n\\"ssm:resourceTag/department\\":[\\n\\"dev1\\"\\n]\\n\\"ssm:resourceTag/department\\":[\\n\\"dev2\\"\\n]\\n}\\n}\\n}\\n]\\n}","D":"{\\n\\"Version\\":\\"2012-10-17\\",\\n\\"Statement\\":[\\n{\\n\\"Effect\\":\\"Allow\\",\\n\\"Action\\":[\\n\\"ssm:SendCommand\\"\\n],\\n\\"Resource\\":\\"*\\",\\n\\"Condition\\":{\\n\\"StringNotEquals\\":{\\n\\"ssm:resourceTag/department\\":[\\n\\"dev1\\", \\"dev2\\"\\n]\\n}\\n}\\n}\\n]\\n}"},"answer":"A","explanation":"If the Systems Manager Run Command needs a restriction, the\\nbest practice is to develop the appropriate IAM policy for IAM users and\\ngroups.\\nOption A allows only tags 1 and 2 to run the command."},{"id":"93","question":"There is a number of CloudFormation templates. Most of them\\nare used to manage AMI IDs for various regions and instance types\\nwith mappings. The AMI IDs can, however, change regularly; for\\nexample, when there are software updates. In that case, all related\\nCloudFormation templates must be updated, which takes time. You\\nplan to have an automated way to search and get the correct\\nCloudFormation AMI IDs. Which two strategies will help you to\\naccomplish this together? (Choose 2)","options":{"A":"Add a new AMI ID parameter in the CloudFormation\\n\\n\\ftemplates. When creating CloudFormation stacks, input the\\nlatest AMI ID parameter","B":"Use a CloudWatch Event rule to execute a Lambda function\\nevery day to get the latest AMI IDs","C":"In the CloudFormation template, create a custom resource type\\nto invoke and send input values to a Lambda function in order\\nto get the correct AMIs. After the custom resource gets a\\nproper response, the stack proceeds with other resources","D":"Prepare a shell script to fetch the latest AMIs for any region\\nand instance type by using AWS CLI commands such as ec2\\ndescribe-images","E":"Create a Lambda function to get the latest AMIs for a given\\nregion and instance type"},"answer":"C and E","explanation":"As they want the automated method of fetching AMI IDs for\\nEC2 resources, creating custom resource can associate a Lambda function to\\nget the AMI IDs. The stack can only proceed after the AMIs are received\\nfrom the Lambda. This Lambda function can use region and instance type as\\ninputs and get the related AMIs. It then returns the IDs to the custom\\nresource."},{"id":"94","question":"A new employee has joined AWS codecommit in several\\nAWS region for the management of code repositories. To link to\\nSSH\'s CodeCommit, the newest version of a GIT has been installed\\nin the Linux machine, an ssh-keygen key pair has been created and a\\npublic key has been added to its IAM users’ security identifiers. But\\nstill, no repositories using SSH can be cloned. What ~/.ssh/ config\\nfile does the Linux computer need to create?","options":{"A":"Host git-codecommit.*.amazonaws.com\\nUser IAMUSERIDEXAMPLE\\nIdentityFile ~/.ssh/id_rsa.pub","B":"Host git-codecommit.*.amazonaws.com\\n\\n\\fUser APKAXXXXXXEXAMPLE\\nIdentityFile ~/.ssh/id_rsa","C":"Host git-codecommit.ap-south-1.amazonaws.com\\nUser APKAXXXXXXXEXAMPLE\\nIdentityFile ~/.ssh/id_rsa","D":"Host *\\nUser IAMUSERIDEXAMPLE\\nIdentityFile ~/.ssh/id_rsa"},"answer":"B","explanation":"GIT SSH Access is a common approach for connecting from a\\nremote server to AWS CodeCommit. So, Option B is the correct\\nconfiguration for SSH."},{"id":"95","question":"Arthur has a large number of AWS accounts that are managed\\nby him. Some AWS accounts belong to AWS Organization and some\\ndo not. When there is a new EC2 termination event for any other\\naccount, it is necessary to set up the CloudWatch rule in the master\\naccount. In each account, you plan to set a CloudWatch Events rule,\\nand send the event to the master account in the default Event Bus.\\nWhat is NOT the solution to work with in this requirement?","options":{"A":"In CloudWatch Events, in order to send the event to the master\\naccount, the target should be configured as “Event bus in\\nanother AWS account”","B":"A proper IAM role is needed for the sender account to send\\nevents to the Event bus in the master account","C":"All accounts should belong to certain AWS Organizations","D":"A master account should permit the CloudWatch events to be\\nreceived from other accounts"},"answer":"C","explanation":"The CloudWatch Event Bus is a helpful service in order to\\nsend / receive CloudWatch events to / from other AWS accounts. For that,\\n\\n\\fyou need permission from the master account for events to be received.\\nFor CloudWatch Events to submit the event, the sender must have a suitable\\nIAM role. We also need to configure the target as an “Event bus in another\\nAWS account” in order to send the event to another account’s default Event\\nbus."},{"id":"96","question":"A big financial company runs a Hybrid system. Separate\\ndeployment mechanisms for the on-site and AWS servers have\\npreviously been used. The deployment management must be\\nincreased in all instances with the same method. For this necessity,\\nyou suggested using AWS CodeDeploy because all on-site machines\\nrun under Ubuntu LTS 16.04. To configure the instances on site with\\nAWS CodeDeploy, what pre-conditions must be met? (Choose 2)","options":{"A":"A VPN connection or a Direct Connect should be established\\nbetween the on-premises environment and AWS VPC","B":"The local account used to configure the on-premises instance\\nmust be able to run either as a sudo or root","C":"The IAM identity to register the on-premises instance in\\nCodeDeploy service must be granted proper permissions","D":"The on-premises instance must open port 80 for the outbound\\ntraffic to connect to public AWS service endpoints","E":"Java 8 should be installed in order for the CodeDeploy agent to\\nwork properly"},"answer":"B and C","explanation":"AWS CodeDeploy assists in handling on-site deployment and\\nEC2 deployment. Nevertheless, there are certain pre-conditions for on-site\\nservers to be met:\\nFor the CodeDeploy system, the administrative control is needed\\notherwise the deployment will not succeed\\nSuitable permission must be granted to IAM Identity in order to\\nregister the instance with AWS CodeDeploy Service"},{"id":"97","question":"All instances in an organization have certain latest security\\npatches that need to be installed in time. Nevertheless, it is necessary\\n\\n\\ffor patches to be installed in dev and test environment for one week\\nbefore deployment in production instances. It is possible to\\ndistinguish all EC2 instances by tags. How can the AWS System\\nManager be implemented in the best way possible?","options":{"A":"Use the Systems Manager session manager to configure the\\npatching for EC2 instances. Apply required patches\\naccordingly after you remotely login into the server","B":"Use a pre-defined default Patch Baseline. Add tags of dev, test,\\nand production to relevant EC2 instances. Associate the Patch\\nBaseline with EC2 instances via tags","C":"Tag the instances using dev, test, and production. In Systems\\nManager, run the command of “AWS-InstallPatches” based on\\nthe tags","D":"Create a customized Patch Baseline. Create several Patch\\nGroups for dev, test, and production instances. Associate the\\nPatch Groups with the new Patch Baseline. Schedule the\\npatching in a maintenance window as required"},"answer":"D","explanation":"The patch manager in the Systems Manager should be the best\\ntool to apply patches. As some patches need to be selected, in this case, it is\\nhighly possible to first create a custom patch baseline. Create multiple patch\\ngroups for instances of dev, test, and production. Associate the new Patch\\nBaseline with the Patch Groups. User Groups make it possible to apply the\\ncorrect patches to the appropriate instances in the corresponding patch\\nbaseline rule."},{"id":"98","question":"A CloudFormation stack has included several AWS resources\\nincluding EC2 instances. In the past, AMI ID management for\\ndifferent regions and instance types was done via a mapping table.\\nThe AMI ID is obtained in the function \\"Fn::FindInMap\\" during the\\ncreation of the EC2 resource. In this method, however, if a new AMI\\nID is available, the mapping table must be updated. You look for\\nbetter ways to get your updated AMIs automatically. You already\\nhave a Lambda feature, which can obtain the latest AMI with the\\ninput as a region and instance type. How can the Lambda function\\n\\n\\fbest be used in order to achieve this?","options":{"A":"Manually trigger the Lambda function and store its output in\\nthe Systems Manager Parameter Store. Modify the\\nCloudFormation template to get the latest AMI ID from the\\nParameter store","B":"Create a Custom resource in the CloudFormation template. In\\nits RequestId property, specify the name of the Lambda\\nfunction to associate it with the Custom resource","C":"Create a Custom resource in the CloudFormation template.\\nAssociate the Lambda with the Custom resource by specifying\\nthe Amazon Resource Name (ARN) of the Lambda function\\nfor the ServiceToken property","D":"Before updating/creating the CloudFormation stack, use a shell\\nscript to run the Lambda function to get the correct AMI ID.\\nUse the ID as a parameter for the CloudFormation template"},"answer":"C","explanation":"This case requires the best way to use the Lambda, meaning\\nthat several methods will work in theory. Nonetheless, we must find out\\nwhich alternative requires less manual effort and is easier to implement.\\nLambda can provide the correct AMI ID with the Custom asset and continue\\ncreating EC2 based on it."},{"id":"99","question":"The Jenkins server was maintained by your team in an EC2\\ninstance. The Jenkins server is used mainly to build Java-based\\nartefacts. Currently, this Jenkins server has no Disaster Recovery\\nStrategy. A new disaster recovery plan must be drawn up for your\\nteam, both RTO and RPO, within 24 hours. What strategy should the\\nteam choose to recover from disasters?","options":{"A":"Backup & Restore strategy. For example, backup the Jenkins\\nconfiguration files every day to an S3 bucket. Use\\nCloudFormation templates to provision the necessary AWS\\nresources","B":"Hot Standby (Multi Site) strategy. Launch a fully operational\\n\\n\\fEC2 instance for the Jenkins server. Suspend its tasks unless a\\nfailover happens","C":"Warm Standby strategy. Launch a smaller size EC2 instance in\\nthe same region but different VPC as a standby","D":"Pilot Light strategy. Use an AMI to launch an EC2 instance in\\nanother region and stop the instance to save cost. Start the Pilot\\nLight instance when there is an outage in the original Jenkins\\nserver"},"answer":"A","explanation":"The EC2 has installed a Jenkins server that is used as a CI / CD\\ndatabase in this situation. It is not a production application, so it has a very\\nlow impact. There is plenty of time to recover the server using a backup\\nwhen there is an outage as RPO and RTO are both 24 hours. This scenario is\\nsufficient for the Backup & Restore strategy. There are various server backup\\nmethods, such as the Jenkins configuration files, EC2-instance AMI, daily\\nEBS snapshots, etc."},{"id":"100","question":"A Service is designed that aggregates the clickstream data in\\nbatch and deliver reports to the subscriber through the emails. The\\ndata is extremely spikey, high-scaled, geographically distributed and\\nunpredictable. How will you design this system?","options":{"A":"Use a large shift cluster to perform the analysis, and a fleet of\\nLambdas to perform record inserts into the Redshift tables.\\nLambda will scale rapidly enough for the traffic spikes","B":"Use API Gateway invoking Lambdas, which Put records into\\nKinesis, and EMR running spark performing Get record on\\nKinesis to scale with spikes. Spark on EMR outputs the\\nanalysis to S3, which are sent out via email","C":"Use a CloudFront distribution with access log delivery to S3.\\nClicks should be recorded as query string GETs to the\\ndistribution. Reports are built and sent by periodically running\\nEMR jobs over the access logs in S3","D":"Use AWS Elasticsearch service and EC2 Auto-scaling groups.\\nThe Auto-scaling groups scale based on click throughput and\\nstream into the Elasticsearch domain, which is also scalable.\\n\\n\\fUse Kibana to generate reports periodically"},"answer":"C","explanation":"The ideal approach of getting the data onto EMR is to use S"},{"id":"3","question":"Since the data is extremely spikey and highly-scaled, using edge location\\nthrough CloudFront distribution is the best way to fetch the data. When you\\nare building the report or analyzing data from a large data set, you need to\\ndefine EMR because this service is built on the Hadoop framework, which is\\nused to process a large set of data.\\n101. Eugene had his application migrate to AWS. This app is in the\\nap-southeast-1 region. The DevOps team used a warm-standby\\napproach in the disaster recovery strategy and built an additional\\nfunctional environment in the ap-southeast-2 region. If a production\\nsystem is inefficient and failure is required, what steps can be seen as\\neffective recovery measures? (Choose 2)","options":{"A":"Check Amazon Route 53 to make sure that all traffic is routed\\nto region ap-southeast-2","B":"Select tools to backup data into AWS S3. Enable encryption for\\nsensitive data","C":"Use EBS Lifecycle Manager to regularly create EBS snapshots","D":"Adjust Auto-scaling groups to accommodate the increased\\ntraffic","E":"Create custom AMIs and start the application in Amazon EC2\\ninstances"},"answer":"A and D","explanation":"We already know that warm-standby is an extension of Pilot\\nlight. The standby is fully operational, but the system uses a minimum\\ndatabase size to reduce costs. You need to adjust the ASG for this\\nrequirement. The DNS records on Route53 that require manual changes or a\\nhealth check is set up to automatically help with failover. This is an\\nimportant step in ensuring that the site is fully restored."},{"id":"102","question":"In its AWS account and on-premises, your organization has\\nstored a large amount of data. The data includes daily transactions,\\n\\n\\fdata from clients, etc. You have a responsibility to build an AWS\\nQuickSight service that allows other teams to analyze data, view data\\nvia dashboards, and find hidden trends through machine learning\\ntechnologies. What data cannot be supplied for analysis by\\nQuickSight?","options":{"A":"A MariaDB RDS instance","B":"YAML files stored in S3 buckets","C":"An AWS Redshift cluster","D":"A MySQL 5.1 database in the customer’s data center, which is\\ninternet accessible"},"answer":"B","explanation":"AWS QuickSight can allow analyzing by means of machine\\nlearning techniques by using various relational data stores as a data source.\\nThese include CSV/TSV, XLSX, ELF/CLF, and JSON."},{"id":"103","question":"Russell has shifted his services to AWS. A failure of the AWS\\nhardware that affected one of the EBS volumes was observed last\\nweek. The AWS Personal Health Dashboard warned of this problem\\nbut the team needed several hours to process this information. Now,\\nRussell is searching for a solution from the AWS Personal Health\\nDashboard that notifies the group when an open issue is happening.\\nWhich solution are you going to recommend?","options":{"A":"Create a new CloudWatch Event, which monitors Trusted\\nAdvisor service and trigger an SNS notification when there is a\\nnew issue","B":"Configure a Lambda function, which periodically checks open\\nissues via AWS Health API and triggers an SNS notification if\\na new issue has been found","C":"In AWS Personal Health Dashboard console, configure an SNS\\nnotification when specific open issues appear","D":"Create a CloudWatch Event, which monitors AWS Health\\nEvent and trigger an SNS notification when there is a new\\nevent"},"answer":"D","explanation":"The AWS Personal Health Dashboard is able to show AWS\\ndata that might be important to your AWS resources such as open issues,\\nscheduled changes and event logs. Therefore, on the AWS Health Service\\nbasis, a CloudWatch rule is created and users can identify a particular event\\nservice, such as EBS."},{"id":"104","question":"You have customized some system and application logs in EC2\\ninstances and delivered them to several Log Groups in CloudWatch\\nLogs. You find that the AWS CloudWatch Logs Console just makes\\nit very difficult to locate useful information. You prefer the logs to a\\ndownstream processing system, which can provide the operating team\\nwith more reliable and important information. To which services\\ncould CloudWatch logs be configured and data streamed? (Choose 2)","options":{"A":"AWS DynamoDB","B":"AWS S3","C":"AWS Lambda Function","D":"AWS CloudTrail","E":"AWS Elasticsearch"},"answer":"C and E","explanation":"In order to deliver the log events to any other service, you can\\nuse Subscription in CloudWatch logs in real-time. AWS Logs has supported\\nstreaming data to Lambda that’s why you can use it or you can use AWS\\nElasticsearch with AWS CloudWatch logs to stream the logs data."},{"id":"105","question":"An application is deployed in an AWS Auto-scaling group in a\\nfinancial company. The request traffic is most often quite smooth and\\nthe ASG is not scaled in or out. But EC2 instances may fail the\\nElastic Load Balancer health check, and then be terminated by ASG,\\nas per the latest release. In order to access the instance and solve the\\nproblem, the development team has created an ASG life-cycle hook.\\nAfter the instance, which AWS services can be configured to\\nautomatically receive notification in the \\"Terminating: Wait\\" state?\\n(Choose 2)","options":{"A":"AWS CloudWatch Events","B":"AWS CloudWatch Alarms","C":"AWS SNS","D":"AWS SQS","E":"AWS CloudTrail"},"answer":"A and C","explanation":"The auto-scaling lifecycle hook offers users the opportunity to\\nsuspend the scaling process. It is recommended that CloudWatch Event\\nreceive notifications when an event occurs and that SNS may then be added\\nas the notification target with a the-notification-target-arn option in the AWS\\nCLI command put-lifecycle-hook. The ARN of the target notification can be\\nan SQS queue or SNS subject according to —notification-target-arn."},{"id":"106","question":"Paul is using the AWS platform to deploy a website using AWS\\nELB and ASG. The ELB is configured with the following health\\nchecks:\\nPing Target:\\nHTTPS:443/healthcheck.htm, Timeout: 20 seconds, Interval: 30\\nseconds, Unhealthy threshold: 3 and healthy threshold: 3.\\nThe Auto-scaling group also uses ELB as its health check type. Last\\nweekend, there was very high traffic due to a promotional event.\\nNonetheless, on launching new instances by ASG, the ELB health check\\nfailed because of the traffic congestion and eventually was again\\nterminated by ASG. As a result, it was difficult to launch new instances,\\nwhich made the matter worse. Which approaches can help resolve this\\nissue under high traffic? (Choose 2)","options":{"A":"Modify Ping target to use HTTP instead of HTTPS","B":"Increase the Interval to 35 seconds","C":"Decrease the Unhealthy threshold to 2","D":"Increase the Healthy threshold to 5","E":"Increase the Timeout to 25 seconds"},"answer":"B and E","explanation":"In the configuration of the health checks for the Classic Load\\nBalancer, you can set health check parameters in the console. We should\\nprovide the EC2 instance with more time to pass a health check in this\\nscenario; The timer for ELB is regulated by Timeout to test the set Ping\\nTarget. By increasing the timer, you can allow the EC2 instance to react to\\nthe ELB health check successfully. So, Option B and E are the same but\\nOption B gives more time to the EC2 instance for responding."},{"id":"107","question":"To perform ad-hoc business analytics queries on well-structured\\ndata, the data comes in constantly at a high velocity. Knowing that\\nyour business intelligence team understand SQL, which service(s)\\nshould you look at first?","options":{"A":"Kinesis Firehose + RDS","B":"EMR using Hive","C":"EMR running Apache Spark","D":"Kinesis Firehose + Redshift"},"answer":"D","explanation":"Kinesis Firehose is the easiest way to load streaming data into\\nAWS. It can capture, transform and load the streaming data into Kinesis\\nanalytics, S3, Redshift, and Elasticsearch service. Whereas Redshift is a fully\\nmanaged, petabyte scale data warehouse service in the cloud. This enables\\nyou to use your data to acquire new insights for your business and customers."},{"id":"108","question":"A company has given you the task to configure an AWS Elastic\\nBeanstalk work tier for easy debugging but you are facing problems\\nin finishing queue jobs. What should you do to overcome this issue?","options":{"A":"Configure a Dead Letter Queue","B":"Configure Enhanced Health Reporting","C":"Configure Blue-Green Deployments","D":"Configure Rolling Deployments"},"answer":"A","explanation":"Elastic Beanstalk worker environment supports Amazon SQS\\nDead Letter Queues. In the dead letter queue, other queues can send\\nmessages that for some reasons could not be processed. Messages that are\\n\\n\\funsuccessful in the processing are targeted from the source queue to the\\ndead-letter queue. You can gather these types of messages in dead-letter\\nqueues to find the reason for their failure."},{"id":"109","question":"The highest possible network performance is required for cluster\\ncomputing applications. Allen already selected homogenous instance\\ntype supporting 10Gb enhanced networking. He made sure that the\\nworkload is network bound and put the instances in the placement\\ngroup. What would be the last optimization that he should make?","options":{"A":"Segregate the instances into different peered VPCs while\\nkeeping them all in a placement group, so each one has its own\\ninternet gateway","B":"Bake an AMI for the instances and relaunch, so the instances\\nare fresh in the placement group and do not have noisy\\nneighbors","C":"Use 9001 MTU instead of 1500 for jumbo frames, to raise\\npacket body to packet overhead ratios","D":"Turn off SYN/ACK on your TCP stack or begin using UDP for\\nhigher throughput"},"answer":"C","explanation":"Jumbo packet allows the data more than 1500 bytes by\\nincreasing the payload size per packets. And increasing the percentage of the\\npacket that is not the packet overhead. The same amount of usable data\\nrequires fewer packets to send. However, you will experience a maximum\\ntrajectory of 1500 MTUs apart from a given AWS region, a single VPC or a\\nVPC peering connection. VPN connections and traffic sent over an internet\\ngateway are limited to 1500 MTU. If packets are over 1500 bytes, they are\\nfragmented, or they are dropped if the Don\'t Fragment flag is set in the IP\\nheader."},{"id":"110","question":"Richard wants to automate the 3 layers of large cloud\\ndeployment, make it capable of tracking all the changes over time in\\ndeployment, and carefully control any alteration. Which is the best\\nway to achieve these requirements?","options":{"A":"Use OpsWorks stacks with 3 layers to model the layering in\\nyour stack","B":"Use AWS config to declare a configuration set that AWS\\nshould roll out to your cloud","C":"Use Elastic Beanstalk Linked Applications, passing the\\nimportant DNS entries between layers using the metadata\\ninterface","D":"Use CloudFormation Nested Stack templates, with three child\\nstacks to represent the three logical layers of your cloud"},"answer":"D","explanation":"When your infrastructure is growing, common patterns emerge\\nin each template, where you declare the same components. You can\\nseparately create templates for these common components. Thus, you can\\nmix and match the various templates and also create a single unified stack\\nusing nested stacks. Stacks that are nested are stacks that create additional\\nstacks. Use AWS::CloudFormation::Stackresource to link other templates in\\nyour template to build nested stacks.\\nSo as the question said that Richard needs to automate the stack over a period\\nwithout recreating the stack when there are any changes, then Nested stack is\\nthe best tool to reuse Common Template Pattern. AWS also recommends that\\nnested stack updates should be executed from the parent stack.\\nAWS CloudFormation updates the top-level stack and initiates an update to\\nits nesting pillars when you upgrade the top-level stack. AWS\\nCloudFormation updates nested stack resources but does not update the\\nresources of nested stacks that have remained unmodified."},{"id":"111","question":"In the region, ap-southeast-1 Steve has migrated a MySQL\\npremise database to AWS RDS MySQL. Key information like\\ncustomer number, address, and date of birth have been stored in the\\ndatabase. A read replica in the ap-southeast-2 area was created to\\ndecrease the read load in the master DB instance. It can also be\\nsupported as a data recovery scheme if the DB source instance fails.\\nIn what scenarios can the Read Replica recover data successfully?\\n(Choose 2)","options":{"A":"An AWS regional failure happening in both ap-southeast-1 and\\n\\n\\fap-southeast-2","B":"The master database instance has been mistakenly deleted","C":"AWS account is compromised","D":"Data has been deleted mistakenly by a bug in application code","E":"An RDS hardware failure on the master instance"},"answer":"B and E","explanation":"Users can select another region when creating the Read\\nReplica so that updates to the source DB instance are copied into the Read\\nReplica in this new region. This Read Replica can be promoted as a new,\\nstandalone instance if necessary. So if RDS hardware failure occurs, it only\\neffects master node and you can promote Read Replica to recover DB. Or, if\\nthe master database is accidentally deleted, you can use Read Replica as\\nfunctional DB."},{"id":"112","question":"There is a serious outage at AWS. The EC2 is not affected, but\\nthe EC2 instance deployment script stopped working with the outage\\nin the region. What can be the reason?","options":{"A":"S3 is unavailable, so you cannot create EBS volumes from the\\nsnapshot you used to deploy new volumes","B":"The AWS console is down, so your CLI commands do not\\nwork","C":"AWS turns off the deploy code API call when there are major\\noutages, to protect from system floods","D":"None of the other answers make sense. If EC2 is not affected, it\\nmust be some other issues"},"answer":"A","explanation":"The EBS snapshots are stored in S"},{"id":"3","question":"If you write the script,\\nwhich deploys EC2 instances, then the EBS volume needs to be constructed\\nfrom snapshot stored in S3.\\nBy using a point-in-time snapshot, you can protect information for Amazon\\nEBS volumes in Amazon S3. Snapshots are incremental backups that only\\nstore the frames on the system that have been modified after your last\\nsnapshot. The time needed for creating the snapshot is reduced and storage\\n\\n\\fcosts are saved by not duplicating the data. Only the data unique to that\\nsnapshot can be removed when you delete your snapshot. All information\\nrequired to restore the data (from the moment the snapshot has been taken) to\\nthe new EBS volume, is provided in every snapshot.\\n113. There is an Asynchronous application using Auto-scaling and\\nAmazon SQS. The Auto-scaling scales as per the depth of the queue.\\nThis results in the completion velocity going down, and the Autoscaling group size to be maxed out, but there is no increase in\\ninbound velocity. What is the reason?","options":{"A":"The routing table changed, and none of the workers can process\\nevents anymore","B":"Someone changed the IAM role policy on the instances in the\\nworker group and broke permissions to access the queue","C":"The scaling metric is not functioning correctly","D":"Some of the new jobs coming in are malformed and\\nunprocessed."},"answer":"D","explanation":"As the velocity in not increasing then the only reason is that\\nthe new jobs coming in are malformed and unprocessed. Option D is correct\\nbecause in all the other options, no job is getting completed."},{"id":"114","question":"All modifications in consumer banking information have to be\\naudited in a file. To save this customer banking information, you\\nneed DynamoDB. Due to server failures, it is important not to lose\\nany data. What can you do to achieve this?","options":{"A":"Before writing to DynamoDB, do a pre-write\\nacknowledgement to a disk on the application server, removing\\nsensitive information before logging. Periodically rotate these\\nlog files into S3","B":"Use a DynamoDB stream specification and periodically flush\\nto an EC2 instance store, removing sensitive information\\nbefore putting the objects. Periodically flush these batches to\\n\\n\\fS3","C":"Use a DynamoDB stream specification and stream all changes\\nto AWS Lambda. Log the changes to AWS CloudWatch logs,\\nremoving sensitive information before logging","D":"Before writing to DynamoDB, do a pre-write\\nacknowledgement to disk on the application server, removing\\nsensitive information before logging. Periodically pipe these\\nfiles into CloudWatch logs"},"answer":"C","explanation":"You are able to execute Lambda Functions by using\\nDynamoDB table streams as a trigger. Triggers are customized actions in\\nresponse to DynamoDB table changes. You must first activate Amazon\\nDynamoDB Streams in your table to create a trigger. Then, to process the\\nupdates published on this stream, you must create a Lambda function."},{"id":"115","question":"How can you achieve a gigabit network throughput on EC2,\\nwhen you already selected cluster-compute, 10 GB instances with\\nenhanced networking, network-bounded workload but you do not\\ndetect a 10-gigabit speed?","options":{"A":"Enable biplex networking on your servers, so packets are nonblocking in both directions, and there is no switching over-head","B":"Use a placement group for your instances so the instances are\\nphysically near each other in the same availability zone","C":"Ensure the instances are in different VPCs so you do not\\nsaturate the internet gateway on any one VPC","D":"Select PIOPS for your drives and mount several so that you can\\nprovision sufficient disk throughput"},"answer":"B","explanation":"A placement group consists of a logical group of instances\\nwithin the single AZ. For applications benefiting from the low network\\nlatency, high-network performance or both, placement groups are\\nrecommended. Choose an instance that supports enhanced networking to\\nensure the lowest latency and the maximum packet per-second network\\nperformance for your placement group."},{"id":"116","question":"Your CTO asked you to ensure that you know what all AWS\\naccount users are doing at all times to change the resources. She\\nneeds to have a report as wide as possible, on who does what over\\ntime, sent to her once a week. How are you going to do that?","options":{"A":"Use CloudWatch events rules with an SNS topic subscribed to\\nall AWS API calls. Subscribe the CTO to an email type\\ndelivery on this SNS topic","B":"Use AWS IAM Credential reports delivering a CSV of all uses\\nof IAM user tokens over time to the CTO","C":"Use AWS Config with an SNS subscription on a Lambda, and\\ninsert these changes over time into a DynamoDB table.\\nGenerate reports based on the contents of this table","D":"Create a global AWS CloudTrail Trail. Configure a script to\\naggregate the log data delivered to S3 once per week and\\ndeliver this to the CTO"},"answer":"D","explanation":"AWS CloudTrail is a service that helps you enabling\\ngovernance, compliance and operational and risk auditing on your account.\\nCloudTrail is used to view, search, archive, download, analyze and respond\\nto the account activity across your AWS infrastructure. A user, role, or AWS\\nservices activities are recorded as CloudTrail events. Actions in AWS\\nManagement Console, AWS Command Line Interface and SDKs and APIs\\nare included in events."},{"id":"117","question":"A CloudFormation template has been used by a DevOps\\ndeveloper to build an RDS source for a new web app. A PostgreSQL\\nengine was used by the RDS server and encryption is disabled.\\nNonetheless, the database needs to be updated to enable encryption\\nfor certain security considerations. The CloudFormation template is\\nupdated accordingly (StorageEncrypted is true). The data should be\\nrestored from the latest DB snapshot in order to prevent data losses\\nduring the update of the CloudFormation stack. In accordance with\\nthat necessity, which two steps should be taken? (Choose 2)","options":{"A":"Add the DBSnapshotIdentifier property with the ID of the used\\n\\n\\fDB snapshot","B":"Make sure that automated snapshots are working properly and\\nrecord the last snapshot ARN ID","C":"Add a DeletionPolicy of Snapshot in the CloudFormation\\ntemplate","D":"Add a Stack Policy in the CloudFormation stack to prevent the\\nDB resource from being deleted","E":"Deactivate any applications that are using the DB instance and\\nthen create a manual snapshot"},"answer":"A and E","explanation":"The DB instance will be deleted and replaced by a new one if\\nStorageEncrypted is modified and the CloudFormation updated. Meanwhile,\\nthe DBSnapshotIdentifier template should be used to point to the DB\\nsnapshot used by the CloudFormation stack. As the manual snapshot is able\\nto maintain the data well, the CloudFormation template will use the snapshot\\nARN. From the snapshot defined by DBSnapshotIdentificier, AWS\\nCloudFormation can create a new database."},{"id":"118","question":"An organization has 2000-engineer and plans to use AWS for\\nthe first time on a large scale. Now, the organization wants to\\nintegrate its identity management system, which is running on\\nMicrosoft active directory with AWS. As you know that the 2000\\nengineers are the power-users of Active Directory, how can you\\neasily manage AWS Directories?","options":{"A":"By using AWS Directory Service Simple AD","B":"By using a sync domain running on AWS Directory Service","C":"By using an AWS Directory Sync Domain running on AWS\\nLambda","D":"By using AWS Directory Service AD Connector"},"answer":"D","explanation":"AD Connector is a directory gateway, by which you can\\nredirect the directory request to your existing Microsoft Active Directory\\nwithout caching any information in the cloud. It is available in two sizes:\\n\\n\\flarge and small. Small AD Connector is designed for the organization up to\\n500 users while the larger supports up to 5000 users in an organization.\\nWith an AD connector, you can get multiple benefits like:\\nYour end users and IT administrators can access AWS\\napplications such as Amazon WorkSpaces, Amazon WorkDocs or\\nAmazon WorkMail using their existing credentials\\nYou can also use it to enable multi-factor authentication, by\\nintegrating into your existing RADIUS-based MFA infrastructure,\\nto provide a further layer of security in access to AWS\\napplications for your users"},{"id":"119","question":"During regional AWS failures, your API requires the ability to\\nremain online. Stateless API is stored, and you have to add\\ninformation from other sources as you do not have a DB. How can\\nthis Uptime goal be achieved simply but efficiently?","options":{"A":"Create a Route53 Latency Based Routing Record with Failover\\nand point it to two identical deployments of your stateless API\\nin two different regions. Make sure both regions use Autoscaling Groups behind ELBs","B":"Create a Route53 Weighted Round Robin record, and if one\\nregion goes down, have that region redirect to the other region","C":"Use an ELB and a cross-zone ELB deployment to create\\nredundancy across datacenters. Even if a region fails, the other\\nAZ will stay online","D":"Use a CloudFront distribution to serve up your API. Even if the\\nregion your API is in goes down, the edge locations\\nCloudFront uses will be fine"},"answer":"A","explanation":"Failover routing lets you route traffic to a resource when the\\nresource is healthy or to a different resource when the first resource is\\nunhealthy. The primary and secondary resource record sets can route traffic\\nto anything from an Amazon S3 bucket that is configured as a website to a\\ncomplex tree of records."},{"id":"120","question":"A serverless architecture is included the AWS API Gateway,\\nAWS Lambda, and AWS DynamoDB and experienced a large\\nincrease in traffic to a sustained 3000 requests per second, which\\nincreased the failure rate. The request on the operation lasts for 500\\nmilliseconds on average. The DynamoDB table did not exceed 50%\\nthroughput provision, and primary keys are assigned correctly. What\\ncan be the reason for failure?","options":{"A":"Your API gateway deployment is throttling your request","B":"Your AWS API Gateway deployment is bottlenecking on\\nrequest (de)serialization","C":"You did not request a limited increase on concurrent Lambda\\nfunction executions","D":"You used consistent read requests on DynamoDB and are\\nexperiencing a semaphore lock"},"answer":"C","explanation":"Every Lambda function is associated with a fixed amount of\\nallocated resources regardless of memory allocation, and each of the\\nfunctions is allocated with a fixed amount of code storage per function."},{"id":"121","question":"A CI is needed to build the AMIs with the pre-installed images\\non every new code push, and you need to do this at a lower cost. How\\ncan you do this?","options":{"A":"Have the CI launch a new on-demand EC2 instance when new\\ncommits come in, perform all instance configuration and setup,\\nthen create an AMI based on the on-demand instance","B":"Bid on spot instances just above the asking price as soon as\\nnew commits come in, perform all instances configuration and\\nsetup, then create an AMI based on the spot instance","C":"Purchase a Light Utilization Reserved Instance to save money\\non the continuous integration machine. Use these credits\\nwhenever you create AMIs on instances","D":"When the CI instance receives commits, attach a new EBS\\nvolume to the CI machine. Perform all setup on this EBS\\n\\n\\fvolume so that you do not need a new EC2 instance to create\\nthe AMI"},"answer":"B","explanation":"You can bid on spare Amazon EC2 computing capacity in\\nAmazon EC2 Spot instances. Because spot instances are often available at a\\ndiscount on demand, you can reduce app running costs substantially, increase\\nthe computer capacity and performance of your application for the same\\nbudget, and allow new types of cloud computing applications."},{"id":"122","question":"The operation and development team wants a place where it can\\nshow both the operation system and application logs. How can you\\nactivate this service using AWS? (Choose 2)","options":{"A":"Using AWS CloudWatch and configuration management, set\\nup remote logging to send events through UDP packets to\\nCloudTrail","B":"Using configuration management, set up remote logging to\\nsend events to Amazon Kinesis and insert these into Amazon\\ncloud search or Amazon RedShift, depending on the available\\nanalytic tool","C":"Using AWS CloudFormation, create a CloudWatch Log, log\\ngroup and send the operating system and application logs of\\ninterest using the CloudWatch logs agent","D":"Using AWS CloudFormation, merge the application logs with\\nthe operating system logs, and use IAM roles to allow both\\nteams to have access to view console output from Amazon EC2"},"answer":"B and C","explanation":"Amazon CloudWatch logs are used to monitor, store or access\\nyour log files from Amazon EC2 instances, CloudTrail, and other sources.\\nYou can also retrieve the associated log data from the CloudWatch Log."},{"id":"123","question":"The development team is using an access key to develop an\\napplication that can access S3 or DynamoDB. A new security policy\\ndeclared that the credentials should not be older than two months and\\nmust be rotated. How can this be achieved?","options":{"A":"Use a script, which will query the data keys that are created. If\\nolder than two months, delete them and create new keys","B":"Use the application to rotate the keys in every two months via\\nthe SDK","C":"Delete the user associated with the keys after every two\\nmonths. Then recreate the user again","D":"Delete the IAM role associated with the keys after every two\\nmonths. Then recreate the IAM role again"},"answer":"A","explanation":"To get the control keys, you can use CLI command-list-access\\nkeys. The CreateDate of the key can be restored as well. When CreateDate is\\nolder than two months, it is deleted. Using the command CLI return-listaccess key, data about the Access ID for the particular IAM user is retrieved.\\nThe operation returns with the empty list if there is none."},{"id":"124","question":"There is an application deployed using Elastic Beanstalk. Sam\\nhas to deploy a new application and ensure that the Elastic Beanstalk\\nhas been detached from the current instance and then re-attached to\\nthe new instance. But the new instances are not receiving any kind of\\ntraffic. What is the case?","options":{"A":"The instances are of the wrong AMI hence, they are not being\\ndetected by the ELB","B":"You need to create a new Elastic Beanstalk application because\\nyou cannot detach and then reattach instances to an ELB within\\nan Elastic Beanstalk application","C":"The instances needed to be reattached before the new\\napplication version is deployed","D":"It takes time for the ELB to register the instances hence, there\\nis a small time frame before your instances can start receiving\\ntraffic"},"answer":"D","explanation":"Before the traffic starts receiving on an EC2 instance, instances\\nare checked by the ELB health checks, and if the health checks are\\n\\n\\fsuccessful, the EC2 instance changes their state to an In-service state, and\\nthen the instances start receiving the traffic."},{"id":"125","question":"James configures AWS Inspector, to continuously evaluate EC2\\ninstances (both Linux and Windows) to determine if there are\\nsecurity-related faults and then fix potential issues in time to improve\\nthe security of applications deployed on the company\'s AWS\\nplatform. When he defined a new AWS Inspector console assessment\\ntarget, he chose all AWS EC2 instances. The option to install AWS\\ninspector agents in all instances has also been selected. To produce\\nthe inspector\'s evaluation report, what conditions must be met?\\n(Choose 2)","options":{"A":"The EC2 instances should have a role to allow SSM Run\\nCommand","B":"The EC2 instances need to configure an IAM role to have the\\nAWS Inspector full access","C":"All EC2 instances need to have the AWS Systems Manager\\n(SSM) Agent installed","D":"The security group in the EC2 instances should allow SSH port\\n22","E":"All EC2 instances should have AWS CLI commands preinstalled"},"answer":"A and C","explanation":"Amazon Inspector is an AWS tool that uses AWS-managed\\nrule packages to perform a security analysis of Amazon EC2 instances. The\\nAWS Inspector agents must be installed first in order for AWS Inspectors to\\nwork correctly. You can also select the \\"Install Agents\\" option so that the\\nagents are automatically installed. In order to install Inspector agent, SSM\\nRun command is used. For that, IAM role is needed at EC2 instance and all\\ninstances must have AWS CLI command pre-installed."},{"id":"126","question":"A large number of EC2 instances in various AWS accounts have\\nbeen maintained by a company. Some instances have been started and\\nno longer used for testing purposes. To save cost, they need to work\\nout an approach to quickly identify the EC2 instances that have a low\\n\\n\\futilization rate such as daily CPU utilization is 10% or less for\\nseveral days. What is the best method to choose from the following?","options":{"A":"In CloudWatch Logs, configure a filter to check the usage rate\\nof EC2 instances. Create an alarm if the utilization is low","B":"In Trusted Advisor, check the status of Low Utilization\\nAmazon EC2 Instances, which is part of Cost Optimization\\nChecks","C":"Create a Lambda function that checks the CPU utilization for\\neach instance and triggers an SNS notification if the average\\nCPU utilization rate is low","D":"In CloudWatch Metrics, for each instance, create an alarm and\\ntrigger an SNS notification when CPU utilization is below\\n10%"},"answer":"B","explanation":"As they want a method that quickly identifies instances with\\nlow utilization, repeatedly checking each instance should be avoided. By\\nusing AWS Trusted Advisor, you can easily identify the instance who has\\nlow utilization under Cost Optimization Checks."},{"id":"127","question":"There is a system, which automatically provisions EIPs to EC2\\ninstances on boot in VPC. The system provisions the whole VPC and\\nstack at once, and you have two of them per VPC. You attempt to\\ncreate a development environment that failed on your new AWS\\naccount after successfully creating a staging and production\\nenvironment in the same region. What is the cause behind this?","options":{"A":"You did not choose the development version of the AMI you\\nare using","B":"You did not set the development flag to true when deploying\\nEC2 instances","C":"You hit the soft limit of 2 VPCs per region and requested a\\nthird","D":"You hit the soft limit of 5 EIPs per region and requested a 6th"},"answer":"D","explanation":"By default, AWS accounts are limited to 5 Elastic IP addresses\\nper region. You can hit a maximum of 5 EIPs per region as internet addresses\\n(Public (IPv4)) are a scarce public resource. We strongly encourage you to\\nuse Elastic IP address primarily in case of instance failure to revert the\\naddress to another instance and to use DNS hostnames for all other\\ncommunication internodes."},{"id":"128","question":"How can Chris pass queue messages that are 1 GB each?","options":{"A":"By using AWS EFS as a shared pool storage medium and\\nstoring filesystem pointers to the files on disk in the SQS\\nmessage bodies","B":"By using SQS’s support for message partitioning and multipart uploads on Amazon S3","C":"By using the Amazon SQS Extended Client Library for Java\\nand Amazon S3 as a storage mechanism for message bodies","D":"By using Kinesis as a buffer stream for message bodies. Store\\nthe checkpoint ID for the placement in the Kinesis Stream in\\nSQS"},"answer":"C","explanation":"With Amazon S3, you are able to manage Amazon SQS\\nmessages. This is particularly useful for storing and consuming messages up\\nto 2 GB of message size. Use the Amazon SQS Extended Server Library for\\nJava to handle Amazon SQS messages using Amazon S"},{"id":"3","question":"You can use this\\nlibrary in particular to:\\nDelete the message object from S3\\nGet the message object from S3\\nWhether on Amazon S3 messages are stored or only if the size of a\\nmessage exceeds 256 KB\\nSend a message, which refers to an object that has been saved in\\nan Amazon S3 bucket\\n129. Nick and his team examine existing AWS tools to help them\\nbetter understand where and how AWS saves and accesses sensitive\\n\\n\\finformation. AWS Macie can satisfy the need to analysis, classify\\nand protect data using machine learning. It also offers a dashboard to\\nview various key points of interest such as high-risk S3 items and\\ncomplete user sessions. Which AWS products are AWS Macie data\\nsources? (Choose 2)","options":{"A":"AWS S3 Bucket","B":"AWS Config","C":"AWS CloudWatch","D":"AWS CloudTrail","E":"AWS EBS Volume"},"answer":"A and D","explanation":"In order to protect data stored in S3, you can use Amazon\\nMacie, which uses ML. For Amazon Macie, AWS CloudTrail is the data\\nsource because it contains API calls, which are provided to Macie for\\nanalyzing. AWS S3 bucket is also a data source because when you configure\\nMacie with AWS S3 bucket, then its objects will be classified and monitored."},{"id":"130","question":"A company needs to build a layer in software stack on AWS that\\nneeds to be able to scale depending on demand as quickly as possible.\\nThe code is running on an EC2 instance in the Auto-scaling group\\nwith ELB. Through which deployment method can this be done?","options":{"A":"Create a new Auto-scaling Launch Configuration with\\nUserData scripts configured to pull the latest code at all times","B":"Create a Dockerfile when preparing to deploy a new version to\\nproduction and publish it to S3. Use UserData in the Autoscaling Launch configuration to pull down the Dockerfile from\\nS3 and run it when new instances launch","C":"Bake an AMI when deploying new versions of code, and use\\nthat AMI for the Auto-scaling Launch Configuration","D":"SSH into new instances that come online, and deploy new code\\nonto the system by pulling it from an S3 bucket, which is\\npopulated by code that you refresh from source control on new\\npushes"},"answer":"C","explanation":"As they want to provide instances as quickly as possible, then\\nit is better to choose the creation of AMI rather than defining it in user data.\\nIn AMI, you define the information that is needed for the launching of\\ninstances. With AMI, you can launch as many instances as you need."},{"id":"131","question":"The app is installed in EC2 and sends AWS CloudWatch with\\ncustomized metrics. Based on these metric report data, you have\\nconfigured many AWS CloudWatch alarms. You find that sometimes\\nthere are numerous warnings of CloudWatch with the status of\\n\\"INSUFFICIENT DATA\\". With your team of developers, you have\\nagreed that some metric data are only produced intermittently by\\ndesign. What should you do to ignore these warnings of\\n\\"INSUFFICIENT DATA\\"?","options":{"A":"Create\\na\\nCloudWatch\\nEvent.\\nWhen\\nan\\n“INSUFFICIENT_DATA” CloudWatch alarm appears, use an\\nSNS to notify the team to react accordingly","B":"Create a Lambda function, which calls CloudWatch Alarm API\\nto check the reason for “INSUFFICIENT_DATA” and modify\\nthe alarm state to “OK” if there is no data received from EC2\\ninstance during that time","C":"Configure the CloudWatch alarms to change the state of\\n“INSUFFICIENT_DATA” to “OK” after 5 minutes","D":"Configure these CloudWatch alarms to treat missing data\\npoints as “ignore” so that “INSUFFICIENT_DATA” does not\\nshow up"},"answer":"D","explanation":"If you configure the missing data as ignore, then that data does\\nnot cause INSUFFICIENT_DATA alarm."},{"id":"132","question":"For a new application, Daniel has created a new Auto-scaling\\ngroup that sets the minimum capacity to 1 and the maximum capacity\\nto 20. ASG has been running smoothly for two weeks, though some\\ninstances have not been successfully resolved recently. If an instance\\nfails to terminate, Daniel must work out a way that notifies his team\\n\\n\\fvia email. What is the easiest way to do that?","options":{"A":"Create a CloudWatch alarm based on the metric of “Terminate\\nFailed”. Send a notification to an SNS topic when the alarm\\nstate is “ALARM”","B":"Configure the Auto-scaling group to send a notification to an\\nSNS topic whenever instances fail to terminate","C":"Configure the related Auto-scaling configuration to send a\\nnotification to an SNS topic whenever instances fail to\\nterminate","D":"Create a CloudWatch Event. When an event of “Terminate\\nUnsuccessful” happens, invoke a Lambda function to notify the\\nteam"},"answer":"B","explanation":"With ASG, you can configure AWS SNS to send a notification\\nwhenever scaling is performed. You can configure SNS topic whenever\\ninstances fail to terminate via ASG console."},{"id":"133","question":"There is a Java program, which deploys an Auto-scaling group\\nin AWS with EC2 instances that are m5.large type instances. The\\napplication\'s response time has recently been increased due to several\\nnew functions because of the high rate of use of the CPU. To resolve\\nthis issue, you must change the EC2 instance form to c5.large. What\\nis the best way to do so?","options":{"A":"Select the Auto-scaling launch configuration and choose\\n“Actions -&gt; Change Instance Type”. Modify the instance\\ntype to c5.large accordingly","B":"In AWS EC2 console, stop the EC2 instances, modify the\\ninstance type to c5.large in “Instance Settings -&gt; Change\\nInstance Type” and then start the instances","C":"Select the relevant launch configuration and choose “Actions &gt; Copy launch configuration”. Modify the instance type\\naccordingly in the new configuration. Select the new launch\\nconfiguration for the Auto-scaling group","D":"In AWS console, edit the Auto-scaling group by modifying the\\ninstance type from m5.large to c5.large"},"answer":"C","explanation":"One main feature of Auto-scaling is that it is not modifiable\\nafter creation. The best practice is to use an existing configuration as a base\\nfor a new setup to change the launch configuration and to update ASG in\\norder to use the new configuration. As the user can pick the current settings,\\ncopy the settings to new settings and change the instance type. Users do not\\nhave to set everything up from scratch with this approach."},{"id":"134","question":"For building a high score game table in DynamoDB that will\\nstore each user’s highest score of players, what will be the\\nDynamoDB structure?","options":{"A":"Game ID as the hash key, highest score as the range key","B":"Highest score as the hash/only key","C":"For each game within many games. And each of which has\\nsimilar usage and the same number Game ID as the hash/only\\nkey","D":"Game ID as the range/the only key"},"answer":"A","explanation":"It is best to choose the hash key as the column, which has a\\nwide range of values. You need to sort with the highest score so make the\\nhighest score as a sort key."},{"id":"135","question":"You have 10% of the written and 90% of readings in your web\\napplication. All requests are currently being served to an AWS ELB,\\nwhich is in the front route of the EC2 Auto-scaling group via a\\nRoute53 Alias Record. When traffic spikes occur during certain news\\nevents, many more people ask to read the same data from your\\napplication, all at the same time, making your system become\\nextremely costly. What can you do to reduce costs and scale spikes in\\nthe simplest and cheapest way?","options":{"A":"Create an S3 bucket and asynchronously replicate common\\nrequests responses into S3 objects. When a request comes in\\n\\n\\ffor a pre-computed response, redirect it to AWS S3","B":"Create another ELB and Auto-scaling group layer mounted on\\ntop of another system, adding a tier to the system. Serve most\\nread requests out of the top layer","C":"Create a Memcached cluster in AWS ElastiCache. Create cache\\nlogic to serve requests, which can be served late from the inmemory cache for increased performance","D":"Create a CloudFront distribution and direct Route53 to the\\ndistribution. Use the ELB as an origin and specify cache\\nbehaviors to proxy cache requests, which can be served late"},"answer":"D","explanation":"To provide strong reads for your application, use the\\nCloudfront distribution. Check how long your objects stay in a CloudFront\\ncache before CloudFront transmits another request to your origin. You can\\ncreate a zone apex record to point to the Cloudfront distribution. The\\nreduction of duration enables dynamic content to be served. Increasing the\\ntime means improving the performance of your users because your objects\\nwill be served from the edge cache. A longer life also lowers the load on your\\nsource."},{"id":"136","question":"A small IT company has an operating budget for AWS\\ninfrastructure that is minimal and therefore spot EC2 instances are\\nalways recommended. A new application is introduced through a load\\nbalancer and Auto-scaling group. Since this software controls the\\nauthentication of all other goods of the business, only spot instances\\nare unacceptable in this regard. According to you, what is the best\\nway to make the Auto-scaling group possible with a combination of\\non-demand and spot instances?","options":{"A":"It is impossible to create a combination of On-Demand and\\nSpot instances for this case. Only one type is allowed for an\\nASG configuration","B":"In Auto-scaling Launch Configuration, configure a suitable\\nOn-Demand/Spot percentage and then create the ASG with this\\nLaunch Configuration","C":"Create the ASG by a Launch Template and configure the OnDemand/Spot percentage accordingly","D":"Create two ASGs. One for On-Demand instances and one for\\nSpot instances"},"answer":"C","explanation":"You have two methods of configuring an Auto-scaling group:\\nLaunch Template and Launch Configuration.\\nLaunch Configuration does not support the combination of On-Demand and\\nSpot instances. While Launch Template is flexible to support combination\\ninstances types and On-Demand and Spot Pricing Options."},{"id":"137","question":"Your Auto-scaling group scales too quickly, too much, and\\nscales when the traffic is decreasing. What should you do to fix this?","options":{"A":"Set a longer cooldown period on the group, so the system stops\\novershooting the target capacity. The issue is that the scaling\\nsystem does not allow enough time for new instances to begin\\nservicing requests before measuring aggregate load again","B":"Calculate the bottleneck or constraint on the computer layer,\\nthen select that as the new metric, and set the metric thresholds\\nto the bounding values that begin to affect response latency","C":"Raise the CloudWatch alarm thresholds associated with your\\nAuto-scaling group, so the scaling takes more of an increase in\\ndemand before beginning","D":"Use larger instances instead of lots of smaller ones, so the\\ngroup stops scaling out and wasting resources as the OS level\\nsince the OS uses a higher proportion of resources on smaller\\ninstances"},"answer":"B","explanation":"In the ideal case, the right metric is not used to scale up and\\ndown. So in order to fix the issue, create a custom metric of that bottleneck."},{"id":"138","question":"George has created a DynamoDB table called \\"Global\\nTemperature\\" to monitor the highest/lowest temperatures in many\\ncities of different countries. The things in the table had a partition key\\n\\n\\f(CountryId) listed and there was no sort key. Recently, more features\\nhave been created and some queries need to be conducted based on a\\nnew partition key (CityId). How can George do this?","options":{"A":"By adding a Global Secondary Index with a partition key as\\nCityId and a sort key as HighestTemperature","B":"By adding a Global Secondary Index with partition key as\\nCityId and another Global Secondary Index with partition key\\nas HighestTemperature. Because the primary index only uses a\\nsimple primary key (partition key), the Secondary Index can\\nonly have one partition key as well","C":"By adding a Local Secondary Index with a partition key as\\nCityId and a sort key as HighestTemperature","D":"By modifying the existing primary index with partition key as\\nCityId and sort key as HighestTemperature"},"answer":"A","explanation":"It is normal for applications to have various queries by using\\ndifferent attributes as Query criteria. More global secondary indexes are\\nnecessary in this case. So Secondary Index is the best option to choose as it is\\ncomprising of Partition and Sort key."},{"id":"139","question":"On a new feature, Tony is currently working on to build with an\\nRDS MySQL server. The feature uses the database to store customer\\ndata for email transactions. Automated snapshots are designed and\\nfunction correctly for the server. A new snapshot has to be exchanged\\nand used in another AWS account due to business requirements. How\\ncan Tony share this RDS snapshot automatically?","options":{"A":"There is no way to share snapshots to another account for\\nautomated ones","B":"Create a manual snapshot for the RDS database and make sure\\nthat it is encrypted. Then share the encrypted snapshot to\\nanother account via AWS console or CLI","C":"Select the automated snapshot in the AWS console. Share the\\n\\n\\fsnapshot by “Actions -> Share Snapshot”","D":"Firstly, make a copy of the automated snapshot to turn it into a\\nmanual version. Then share the copy with the other AWS\\naccounts"},"answer":"D","explanation":"You cannot share the automated snapshot with other AWS\\naccount but after the snapshot is copied to the manual snapshot, you can use\\nthe Share snapshot option from the Actions to share it on another account."},{"id":"140","question":"An organization has multiple applications in the AWS account,\\nand wants to identify the cost per month to operate for a good\\nunderstanding of the business as it does not want to expend initial\\ndevelopment time. What can be done to achieve this?","options":{"A":"Use the AWS Price API and constantly running resource\\ninventory scripts to calculate the total price based on the\\nmultiplication of consumed resources over time","B":"Use AWS Cost Allocation Tagging for all resources, which\\nsupport it. Use the Cost Explorer to analyze costs throughout\\nthe month","C":"Use custom CloudWatch Metrics in your system, and put a\\nmetric data point whenever the cost is incurred","D":"Create an automation script, which periodically creates AWS\\nSupport tickets requesting detailed intra-month information\\nabout your bill"},"answer":"B","explanation":"By using the tag on resources you can make resources more\\norganized. To track your AWS costs in a detailed way, you can use tags to\\norganize your resources and costs allocations. After you activate the cost\\nallocation tags, you will be able to categorize AWS and track your AWS\\ncosts by using cost allocation tags in order to organize your resource costs in\\nyour cost allocation report. AWS provides two types of cost allocation tags,\\nan AWS generated and user defined. AWS defines, creates, and applies the\\nAWS-generated tag for you, and you define, create, and apply user-defined\\ntags. You must activate both types of tags separately before they can appear\\n\\n\\fin Cost Explorer or on a cost allocation report."},{"id":"141","question":"During total regional AWS failure, Christopher needs API\\nbackup by DynamoDB to stay online. Christopher can only tolerate a\\nfew minutes of failure or slowness, but the system should recover\\nthose minutes with normal operation. How can he achieve this?","options":{"A":"Set-up DynamoDB cross-region replication in a master standby\\nconfiguration, with a single standby in another region. Create\\nan Auto-scaling group behind an ELB in each of the two\\nregions for your application layer, in which DynamoDB is\\nrunning in. Add a Route53 latency DNS record with DNS\\nfailover, using the ELBs in the two regions as the resource\\nrecords","B":"Set-up a DynamoDB multi-region table. Create a cross region\\nELB pointing to a cross-region Auto-scaling group, and direct\\na Route53 latency DNS record with DNS failover to the crossregion ELB","C":"Set-up DynamoDB cross-region replication in a master standby\\nconfiguration, with a single standby in another region. Create a\\ncross-region ELB pointing to a cross-region Auto-scaling\\ngroup, and direct a Route53 latency DNS record with DNS\\nfailover to the cross-region ELB","D":"Set-up a DynamoDB global table. Create an Auto-scaling\\ngroup behind an ELB in each of the two regions for your\\napplication layer, in which the DynamoDB is running in. Add a\\nRoute53 latency DNS record with DNS failover, using the\\nELBs in the two regions as the resource records"},"answer":"D","explanation":"DynamoDB global tables provide fully-managed solutions for\\nthe deployment of multi-region, multi-master database. And you do not need\\nto maintain and build your own replication solution in DynamoDB global\\ntable. You define the region in AWS where the table is to be accessible when\\ncreating a global table. DynamoDB carries out all the tasks required in order\\nto construct identical tables and distribute ongoing data changes in these\\nregions."},{"id":"142","question":"To get CloudFormation stack status updates to show up in a\\ncontinues delivery system as close to real time as possible, what\\nshould you do?","options":{"A":"Subscribe your continuous delivery system to an SNS topic\\ninto which you also tell your CloudFormation stack to publish\\nevents","B":"Use a long-poll on the resources object in your\\nCloudFormation stack and display those state changes in the UI\\nfor the system","C":"Use a long-poll on the List stacks API call for your\\nCloudFormation stack and display those state changes in the UI\\nfor the system","D":"Subscribe your continuous delivery system to an SQS queue\\nthat you also tell your CloudFormation stack to publish events\\ninto"},"answer":"A","explanation":"Through monitoring the events of the stack, you will follow\\nthe progress of a stack update. The Events table displays every major step in\\ncreating and updating the stack, sorted with the latest events, by the time of\\neach event. An UPDATE IN PROGRESS event will mark the beginning of\\nthe stack update continuing. When you call CreateStack, use\\nNotificationARNs.member to push stack events into SNS in real-time."},{"id":"143","question":"George builds an application of photo posting and then images\\nof this application are stored in S3. Now, he wants a system that is\\nsimple and cost effective for the application. From the following\\noption, which will be helpful in implementing authentication and\\nauthorization to build photo sharing application?","options":{"A":"Use JWT or SAML compliant systems to build authorization\\npolicies. Users log in with a user name and password and are\\ngiven a token they can use indefinitely to make calls against the\\nphoto infrastructure","B":"Use the AWS API gateway with a constantly rotating API key\\n\\n\\fto allow access from the client-side. Construct a custom build\\nof the SDK and include S3 access in it","C":"Build the application out using AWS Congito and web identity\\nfederation to allow users to log in using Facebook or Google\\naccounts. Once they are logged in, the secret token passed to\\nthat user is used to directly access resources on AWS, like\\nAWS S3","D":"Create an AWS Auth service domain and grant public sign up\\nand access to the domain. During set-up add at least one major\\nsocial media site as a trusted identity provider for users"},"answer":"C","explanation":"You can easily add user sign-up and sign-in, and track your\\nmobile and web app permissions through Amazon Cognito. Inside Amazon\\nCognito, you can create your own user directory. Use SAML to identify\\nsolutions or use your own identity system to authenticate users via social\\nidentity providers like Facebook, Twitter or Amazon. Amazon Cognito also\\nenables you to locally store data on devices of users, enabling your apps to\\noperate even when devices go offline. You can then sync data across devices\\nso that you have consistent app experience, regardless of your phone."},{"id":"144","question":"Your team will start continuous delivery using CloudFormation\\nso that entire, versioned stacks or stack layers can be created and\\ndeployed automatically. You have a mission-critical system of 3\\nlevels. What is NOT the best practice for the use of CloudFormation\\nin continuous delivery?","options":{"A":"Use the AWS CloudFormation validate template call before\\npublishing changes to AWS","B":"Use CloudFormation to create a brand new infrastructure for all\\nstateless resources on each push, and run integration tests on\\nthat set of infrastructure","C":"Parametrize the template and use mappings to ensure your\\ntemplate works in multiple regions","D":"Model your stack in one template, so you can leverage\\nCloudFormation’s state management and dependency\\nresolution to propagate all changes"},"answer":"D","explanation":"CloudFormation’s best practices are “created a nested stack”\\nand “re-use templates”.\\nWhen your infrastructure is growing, common patterns emerge in each\\ntemplate, where you declare the same components. You can separately create\\ntemplates for these common components. Thus, you can mix and match the\\nvarious templates, but you can create a single unified stack using nested\\nstacks. Stacks that are nested are stacks that create additional stacks. Use\\nAWS::CloudFormation::Stackresource to link other templates in your\\ntemplate to build nested stacks.\\nYou can reuse your templates to replicate your infrastructure in several\\nenvironments after you have established your stacks and resources."},{"id":"145","question":"To deploy multiple stacks of AWS in a repeatable manner in\\nmultiple environments, you selected CloudFormation. Now you\\nfound that there is a type of resource that you need to create and\\nmodel, but it is unsupported by CloudFormation. What is the strategy\\nto overcome this challenge?","options":{"A":"Use a CloudFormation custom resource template by selecting\\nan API call to proxy for create, update and delete actions.\\nCloudFormation will use the AWS SDK, CLI, or API method\\nof your choosing as the state transition function for the\\nresource type you are modeling","B":"Create a CloudFormation custom resource type by\\nimplementing create, update and delete functionality, either by\\nsubscribing a custom resource provider to an SNS topic or by\\nimplementing the logic in AWS Lambda","C":"Submit a ticket to the AWS Forums. AWS extends\\nCloudFormation resource types by releasing tooling to the\\nAWS labs organization on GitHub. Their response time is\\nusually one day, and they complete requests within a week or\\ntwo","D":"Instead of depending on CloudFormation, use Chef, Puppet or\\nAnsible to author heat templates, which are declarative stack\\nresource definitions that operate over the open stack hypervisor\\n\\n\\fand cloud environment"},"answer":"B","explanation":"Custom resources enable you to write custom provision logic\\nin the templates that AWS CloudFormation can run anytime you create,\\nupdate or delete stacks. When you have changed your custom resource for\\nexample, you may wish to include resources that are not available as types of\\nAWS CloudFormation, these resources can be included by using a\\ncustomized resource. In that way, all your related resources can be managed\\nin one stack.\\nAWS::CloudFormation::CustomResource or Custom::String resource type to\\ndefine custom resources in your templates. Custom resources require one\\nproperty: the service token, which specifies where AWS CloudFormation\\nsends requests to, such as an Amazon SNS topic."},{"id":"146","question":"Christian met his operation team to discuss last month\'s data.\\nDuring the meeting, he realized that three weeks ago, his monitoring\\nsystem, which pings over HTTP from outside the AWS recorded a\\nlarge spike in latency on his 3 tier web service API. DynamoDB is\\nused for database layer, EBS, ELB, EC2 for the business logic tiers\\nand SQS, EC2, and ELB for the presentation layer. Which technique\\nwill not figure out what happened?","options":{"A":"Review CloudWatch metrics for one-minute interval graphs to\\ndetermine which component(s) slowed the system down","B":"Check your CloudTrail log history around the spikes time for\\nany API calls that caused slowness","C":"Review your ELB access logs in S3 to see if any ELBs in your\\nsystem saw the latency","D":"Analyze your logs to detect bursts in traffic at that time"},"answer":"A","explanation":"CloudWatch retention is:\\nData point with less than 60 seconds are available for 3 hours\\n60 seconds are available for 15 days\\n300 seconds are available for 63 days\\n\\n\\f3600 seconds are available for 455 days\\nData points that are initially published with a shorter period are aggregated\\ntogether for long-term storage. For example, if you collect data using a\\nperiod of 1 minute, the data remains available for 15 days with a 1-minute\\nresolution. After 15 days, this data is still available, but is aggregated and\\nis retrievable only with a resolution of 5 minutes. After 63 days, the data is\\nfurther aggregated and is available with a resolution of 1 hour"},{"id":"147","question":"A vendor requires access to the S3 bucket in your account. The\\nvendor already has an AWS account. How can you provide access to\\nthe bucket?","options":{"A":"By creating a new IAM user and granting the relevant access to\\nthe vendor on that bucket","B":"By creating a new IAM group and granting the relevant access\\nto the vendor on that bucket","C":"By creating an S3 bucket policy that allows the vendor to read\\nfrom the bucket from their AWS account","D":"By creating a cross-account role for the vendor account and\\ngrant that role access to the S3 bucket"},"answer":"D","explanation":"You can share resources in one account with the users in a\\ndifferent account. By cross-account access, you do not need to create\\nindividual IAM users in each account. The users do not have to sign out and\\nthen sign in in another account to get access to the AWS resources. You can\\nuse IAM roles and STS to set-up cross account access."},{"id":"148","question":"An application is deployed, which uses Auto-scaling for\\nlaunching the new instances. To change the instance type of the new\\ninstances, which of the action is deployed?","options":{"A":"Using Elastic Beanstalk to deploy the new application with the\\nnew instance type","B":"Creating a new launch configuration with the new instance type","C":"Using CloudFormation to deploy the new application with the\\n\\n\\fnew instance type","D":"Creating new EC2 instances with the new instance type and\\nattach it to the Auto-scaling group"},"answer":"B","explanation":"Create a new configuration, attach it with the existing Autoscaling group and then terminate the running instances."},{"id":"149","question":"There is an application hosted on AWS, on EC2 instance behind\\na load balancer. You add new features on this application, which\\ncauses the sites to slow down, and now you are receiving complains.\\nHow can you recover from this issue?","options":{"A":"By using CloudTrail to log all the API calls, and then\\ntraversing the log files to locate the issue","B":"By using CloudWatch and monitoring the CPU utilization to\\nsee the times when the CPU peaked","C":"By creating some custom CloudWatch metrics, which are\\npertinent to the key features of your application","D":"By reviewing the Elastic Load Balancer logs"},"answer":"C","explanation":"The issue could be relevant to the few features. Enabling\\nCloudWatch to monitor all the API calls of all services will not benefit the\\ncause. The monitoring of CPU utilization will re-verify that there is some\\nissue but will not resolve the issue. ELB logs do the same things. So, you\\nneed to create a custom metric in CloudWatch for this requirement."},{"id":"150","question":"A team is assisting you in creating a new application with the\\nAWS Aurora data base. The application\'s users are mainly from\\nEurope and North America. You suggest configuring a global Aurora\\ndatabase. The primary server is in the eu-west-1 area and the\\nsecondary one in the us-east-1 zone. What are the benefits of Amazon\\nAurora Online Database? (Choose 2)","options":{"A":"There is no charge for the replicated write I/Os between the\\nprimary region and each secondary region","B":"For disaster recovery, the secondary cluster can be easily\\npromoted to allow full read and write operations","C":"The clusters in both primary and secondary regions have the\\nsame read & write configured capacities","D":"The cluster in the secondary region us-east-1 enables lowlatency reads","E":"Aurora Global Database is available for either Aurora MySQL\\nor Aurora PostgreSQL"},"answer":"B and D","explanation":"In order to create a Global DB, you need to select the location\\nGlobal. Since users provided read-only services from the secondary AWS\\nRegion server, there is no write for the secondary region. With this Global\\nDB when required, the database in the secondary AWS Region can be\\npromoted to take full workloads within a minute."},{"id":"151","question":"Louis has used AWS CodeDeploy to deploy the latest software\\nrelease to several AWS EC2 instances. In this CodeDeploy\\napplication, there are two Deployment groups called Stage and Prod.\\nAt the moment, the only difference between Stage and Prod is the\\nlogging level, which can be configured in a file. What is the most\\nefficient way to implement the different logging levels for the two\\nDeployment groups?","options":{"A":"Create two script files. One version has its logging level setting\\nfor “Stage”. And the other one has its logging level setting for\\n“Prod”. Modify the Deployment Group configurations to use\\nthe correct script file","B":"In the hook script file, use the environment variable\\nDEPLOYMENT_ID to determine, which Deployment Group it\\nis. Then modify the logging level accordingly in the script","C":"For the script file in BeforeInstall hook, use the environment\\nvariable DEPLOYMENT_GROUP_NAME to determine the\\nDeployment Group. Then modify the logging level accordingly\\nin the script","D":"Create two source versions of script files in BeforeInstall hook.\\nOne version has its logging level setting for Deployment Group\\n\\n\\f“Stage”. And the other one has its logging level setting for\\n“Prod”. Choose the relevant source files when creating a new\\ndeployment"},"answer":"C","explanation":"Since the only difference between deployment groups is the\\nlogging level, the environment variable to decide the deployment group\\nshould use the same number of source files. Because the environment\\nparameter to evaluate the Deployment Group is DEPLOYMENT GROUP\\nNAME, in the script file, the following code can be used:\\n[ \\"$DEPLOYMENT_GROUP_NAME\\" == \\"Staging\\" ]\\nAfter this, modify the logging level for Staging Deployment Group."},{"id":"152","question":"The company you work for recently began using the AWS\\nsystem and used a basic support package for its AWS account. As\\nAWS resources are limited, your director requested to develop a\\nsolution to alert him to the low utilization rate of EC2 assets. You\\nintend to use AWS CloudWatch Events to define the lambda feature\\nas the target for these tools. The Lambda feature can provide the\\ndirector with personalized text. How are you to combine these two\\nchoices to fulfill your requirements? (Choose 2)","options":{"A":"Create a new CloudWatch Events rule with the event source as\\na Trusted Advisor. The event type is “Check Item Refresh\\nStatus”. Select the Lambda function as the target","B":"Create a CloudWatch rule with the event source as a Trusted\\nAdvisor. The event type is “All Events”. Select the Lambda\\nfunction as the target","C":"Upgrade the AWS account support plan to “Business” to access\\nall Trusted Advisor checks","D":"Upgrade the AWS account support plan to “Developer” to\\naccess all Trusted Advisor features","E":"Upgrade the AWS account support plan to “Enterprise” to\\nactivate all cost related features"},"answer":"A and C","explanation":"The Business and Enterprise Support Plan should be used for\\n\\n\\fthe Cost Optimization Checks for trusted advisors. The Trusted Advisor Cost\\nOptimization Checks can capture low-cost EC2 instances. To test the trusted\\nadvisor activities, the client just has to create a CloudWatch event policy and\\nprovide notification with Lambda."},{"id":"153","question":"In order to activate CloudTrail, the company wants to increase\\nvisibility in AWS user and resource activities. The trail was set up\\nonly to collect activities in the ap-south-1 region since most resources\\nwere used. However, customers are concerned that the data in the\\nCloudTrail could be lost if a disaster occurs in the ap-south region.\\nWhat measures should you take to tackle the problem?","options":{"A":"Create a Lambda Function, which can read files in the trail S3\\nbucket and copy over the log files to an S3 bucket in another\\nregion","B":"Enable Cross-Region Replication for the trail S3 bucket, which\\nautomatically copies objects in different AWS Region","C":"In CloudTrail console, add another S3 bucket in a different\\nregion as the target for the trail","D":"Enable the encryption with SSE-KMS for the S3 bucket of the\\nCloudTrail"},"answer":"B","explanation":"When failure occurs in the AWS region, you need to consider\\nthat there is no data loss in CloudTrail. So with Cross-Region Replication,\\nyour data asynchronously copies to another region."},{"id":"154","question":"You have to set up a solution that incorporates single sign-on\\nfrom your corporate AD or LDAP directory and does not allow\\naccess to each user to a designated user folder in a bucket. Choose 3\\nanswers to fulfill the given scenario.","options":{"A":"Set up a federation proxy or identity provider","B":"Tag each folder in the bucket","C":"Configure IAM role","D":"Use AWS STS service to generate temporary tokens","E":"Set up a matching IAM user for every user in your corporate\\n\\n\\fdirectory that needs access to a folder in the bucket\\nAnswer: A, C, and D\\nExplanation: Firstly, the access request is sent to the identity provider,\\nwhich directs the request to the corporate identity store, which authenticates\\nthe user and sends the request to STS, which issues the temporary token to\\nthe user, and then the user can login to the console and get access of the\\ndesired bucket.\\n155. Specific attention is needed to ensure that the security\\nvulnerability is not present for its AWS services. A rule is provided in\\nAWS Config to test if the assets of AWS are always according to\\nexpectations. The policy is very complex and the current AWSmanaged rule is not applicable to the needs of the organization. What\\nactions can this requirement achieve in combination? (Choose any 2)\\nA. In AWS Config, add a custom rule that runs every hour and\\nsends a message to an SQS queue\\nB. Create a t2.micro EC2 instance to implement the custom\\npolicies to check if AWS resources are not exposed to security\\nissues. The instance also listens to an SQS queue and starts\\nprocessing whenever there is a new message in the queue\\nC. In AWS Config, add a custom rule and specify the ARN of an\\nAWS Lambda function that checks AWS resources\\nD. Create an SNS topic and subscribe to the topic with an email\\nnotification to the team member in case there is a security issue\\nin AWS resources\\nE. Create an AWS Lambda function, which contains the logic of\\nthe custom rule to evaluate whether the AWS resources are\\ncompliant"},"answer":"C and E","explanation":"AWS config has the rules or custom rules managed by AWS.\\nA Lambda function can be set with the Config-Rule-Triggered blueprint. A\\nLambda ARN can be configured when a custom rule is created."},{"id":"156","question":"You have an instance in Auto-scaling group, in which lifecycle\\n\\n\\fhooks are enabled. Due to lifecycle hooks, initially, the instance is\\nput into Pending: Wait state, which means that the instance cannot\\nhandle the traffic in this state. In wait sate, other scaling actions are\\nalso suspended. Later, the instance is put into Pending: Proceed and\\nthen it is changed to InService, which means that instances in Autoscaling group can now serve the traffic, but you have observed that\\nthe bootstrapping process finishes earlier than the status Pending:\\nProceed is updated.\\nWhat can you do to check that the status of the instance is updated\\ncorrectly after the bootstrapping process?","options":{"A":"Use the complete-lifecycle-action call to complete the lifecycle\\naction. Run this command from Command Line Interface","B":"Use the complete-lifecycle-action call to complete the lifecycle\\naction. Run this command from another EC2 instance","C":"Use the complete-lifecycle-action call to complete the lifecycle\\naction. Run this command from an SQS queue","D":"Use the complete-lifecycle-action call to complete the lifecycle\\naction. Run this command from the Simple Notification\\nService"},"answer":"A","explanation":"Use the complete-lifecycle-action command to allow the Autoscaling Group to continue to launch or terminate the instance after\\ncompleting your custom action before the time period expires. You can use\\nthe following command to specify the lifecycle action token:\\nAws auto-scaling complete-lifecycle-action –lifecycle-action-result."},{"id":"157","question":"The AWS API Gateway and Lambda service both have been\\nimplemented by a team. The JSON input data can be saved to the\\ninternal database and S3. Some data in the JSON body are not used\\nafter the service runs for several years, and should no longer be\\nsaved. The Lambda, which is at the backend was modified to only\\nsupport the current JSON values. Before all users move to the new\\nAPI, the original API in API Gateway remains used for a while. How\\nshould the original API be set to still support the backend of the old\\napplication by Lambda?","options":{"A":"In Integration Response of the API, add a mapping template to\\nremove the obsolete data for the backend Lambda to support\\nthe old requests","B":"Configure a mapping template in Integration Request to\\nremove the obsolete data so that the original API requests are\\ntransformed to be supported by the new Lambda","C":"In the original API, add a stage for canary deployment to\\nunderstand how many users are still using the old JSON format\\nbefore the original API service is completely removed","D":"In the original API, use a new Lambda as an authorizer so that\\nonly the requests with valid JSON data can proceed to hit the\\nbackend"},"answer":"B","explanation":"The Lambda backend has already been updated and the\\noriginal API has been preserved for some time. However, Lambda supports\\nonly the new requests without any outdated information within the JSON\\nbody, meaning the original requests in the Integration Request have to be\\nmapped to a mapping template. Because the request can be converted to the\\ncorrect format with a mapping template, nothing has changed from the initial\\nviewpoint of users."},{"id":"158","question":"You want to design a .Net front end and DynamoDB back end\\napplication. You know that the application will run with a heavy load.\\nHow will you ensure the scalability of application for minimum\\nDynamoDB database load?","options":{"A":"By launching DynamoDB in Multi-AZ configuration with a\\nglobal index to balance writes","B":"By using SQS to assist and let the application pull messages\\nand then perform the relevant operation in DynamoDB","C":"By increasing the write capacity of DynamoDB to meet the\\npeak loads","D":"By adding more DynamoDB databases to handle the load"},"answer":"B","explanation":"SQS is the best option for scalability. DynamoDB is usually\\n\\n\\fscalable, messages in SQS can help in the management of the abovementioned situation due to the cost effective solution’s condition. Amazon\\nSimple Queue Service (SQS) is a fully managed service for the\\ncommunication of message queues between distributed microservices and\\nsoftware components at any scale. It is the best practical design for modern\\napplications.\\nSQS makes decoupling and coordinating the components of a cloud\\napplication simple and cost- effective. You can send, store and receive\\nmessages from software components at any volume via SQS without losing\\nmessages or demanding the availability of other services at any time."},{"id":"159","question":"To maintain an API endpoint, your team uses the AWS API\\nGateway and Lambda functionality. Recently, the Lambda function\\nhas been modified and some new data is applied to the response\\nbody. The Lambda feature has been updated to support the change\\nand a new API has been added. The team wants to keep API version\\n1 and version 2 accessible to your API consumers simultaneously. In\\norder to make the version 1 API compliant, how should you change it\\nin the API Gateway?","options":{"A":"Add a mapping template in the Integration Response of\\nversion1 API to remove the new data so that the response is\\ntransformed into the original format","B":"In the Integration Response of version1 API, add a new Header\\nMapping to remove the new data, which comes from the\\nbackend","C":"In the Integration Request of version1 API, add a mapping to\\ninform the backend Lambda to remove the new data in the\\nrelevant response","D":"Add a mapping template in the Method Response of version1\\nAPI to remove the new data in the response to keep the same\\nbehaviors as before"},"answer":"A","explanation":"To change the backend, the Integration Response of the old\\nAPI could add a mapping model. Because mapping template is suitable for\\nsetting the backend response, the response for the original API consumers is\\n\\n\\fexactly the same."},{"id":"160","question":"You are working in a company where you have to record all the\\nactivities occurring in AWS account and provide the access of the\\nloggings of events to the security officer across all regions in a simple\\nand secure way that no one else would be able to access those events\\nother than the security officer. How will you execute your task?\\nSelect the best solution from the given options.","options":{"A":"Use CloudTrail to log all events to an Amazon Glacier vault.\\nMake sure the vault access policy only grants access to the\\nsecurity officer’s IP address","B":"Use CloudTrail to log all events to separate S3 buckets in each\\nregion as CloudTrail cannot write to a bucket in different\\nregions. Use MFA and bucket policies on all the different\\nbuckets","C":"Use CloudTrail to log all events to one S3 bucket. Make this S3\\nbucket is only accessible by your security officer with a bucket\\npolicy that restricts access to his user only and also add MFA\\nto the policy for a further level of security","D":"Use CloudTrail to send all API calls to CloudWatch and send\\nan email to the security officer every time an API call is made.\\nMake sure the emails are encrypted"},"answer":"C","explanation":"CloudTrail is used to enable security analysis, track changes to\\nyour account and provide compliance auditing. You can log and monitor and\\nretain events relating to API calls throughout your AWS infrastructure\\ncontinuously with CloudTrail. A history of your account AWS API calls,\\nincluding API calls via the AWS Management console, AWS SDKs,\\ncommand line tools, and other AWS services, is provided by the CloudTrail.\\nThis history helps security analysis, tracking, and troubleshooting of\\nresources."},{"id":"161","question":"You are tasked to move an application to the world of AWS\\nElastic Beanstalk. The OS is Amazon Linux and package managers\\nneed ‘yum’ and ‘rubygems’ to download all packages required for the\\n\\n\\fprogram. In .ebextensions, you have created config files in which you\\nhave added a software package section. For the package segment in\\nthe configuration folder, what is the correct statement?","options":{"A":"Elastic Beanstalk only supports one package manager per\\nconfig file. So at least two config files are needed. One for yum\\nand the other for rubygems","B":"Within each package manager, the package installation order is\\nnot guaranteed","C":"For Amazon Linux Operating System, only the package\\nmanager yum is supported in the Package section. Elastic\\nBeanstalk does not support rubygems","D":"Only the latest supported version is installed. The version\\nnumber cannot be specified in the configuration file"},"answer":"B","explanation":"In the Package manager, the order of package installation is\\nnot guaranteed. For example, you define:\\nPackages:\\nYum:\\nApplication 1: []\\nApplication 2: []\\nIn that, it may be possible that application 2 installs first."},{"id":"162","question":"An IT administrator has the responsibility to create a\\ndevelopment environment, which would confirm the LAMP\\ndevelopment stack. Whenever a new instance is launched, the\\ndevelopment team should be updated with the latest version of the\\nLAMP stack. Choose 2 answers, which will meet the requirements in\\nthe best way possible.","options":{"A":"Use the User data section and use a custom script, which will\\nbe used to download the necessary LAMP stack packages","B":"Create a CloudFormation template and use the cloud-init\\ndirectives to download and install the LAMP stack packages","C":"Create an EBS volume with the LAMP stack and attach it to an\\n\\n\\finstance whenever it is required","D":"Create an AMI with all the artifacts of the LAMP stack and\\nprovide an instance to the development team based on AMI"},"answer":"A and B","explanation":"You can always ensure that the latest version of the LAMP\\nstack is downloaded and given to development teams using user data and\\ncloud-init directive. The AMI\'s version should always be the same, and you\\nmust create an AMI each time you change the version of the LAMP stack.\\nYou can transfer your user data to the instance when you launch an instance\\nin Amazon EC2, which can be used to carry out common automated\\nconfiguration tasks and even run scripts after the instance is started. The two\\ntypes of user data that can be passed to EC2 instance are shell scripts and\\ncloud-init directives. You can also transfer that data into a launch wizard\\neither as a file (which can be used to launch instances using the command\\nline tools), as simple text, or as a base64-encoded text (for API calls)."},{"id":"163","question":"If you are asked to build a social media mobile application,\\nwhich needs permission for every user login and storing their data in\\nDynamoDB. What would you choose to do from the following\\noptions in order to grant access to DynamoDB to your application\\nusers when required?","options":{"A":"Create an active directory server and an AD user for each\\nmobile application user. When the user signs in to the AD signon, allow the AD server to federate using SAML 2.0 to IAM\\nand assign a role to the AD user, which is the assumed with\\nAssumeRoleWithSAML","B":"During the install configuration process, each user creates an\\nIAM credential and assign the IAM user to a group with proper\\npermissions to communicate with DynamoDB.","C":"Create an IAM group that only gives access to your application\\nand to the DynamoDB tables. Then, when writing to\\nDynamoDB, simply include the unique device ID to associate\\nthe data with that specific user","D":"Create an IAM role with the proper permission policy to\\ncommunicate with the DynamoDB table. Use web identity\\n\\n\\ffederation, which assumes the IAM role using\\nAssumeRoleWithWebIdentity, when the user signs in, grant\\ntemporary security credentials using STS"},"answer":"D","explanation":"To access any AWS service, using a role is the prior way to\\napproach any application whereas web identity federation is used for any web\\napplication. To develop a web application, it is recommended that its long\\nterm AWS credentials should not be installed or distributed with apps even in\\nan encrypted store that a user downloads. Instead, build your app where the\\nAWS temporary security credentials dynamically web identity federation is\\nrequired. The temporary credentials mapping to an AWS role only allow\\nexecuting the tasks required by the mobile app."},{"id":"164","question":"You are building a pipeline for a new Web application that AWS\\nCodePipeline. The source provider in the source stage is GitHub and\\nits output artifact is designed as WebApp. In the Build process, two\\nsimultaneous development activities have been performed and the\\nservice providers have used AWS CodeBuild. Which input/output\\nartifact configuration is right for these two build actions?","options":{"A":"Build Action 1 (input artifact:\\nartifact:WebAppBuild); Build Action\\nWebApp, output artifact:WebAppBuild)\\n\\nWebApp, output\\n2 (input artifact:","B":"Build Action 1 (input artifact: WebApp, output\\nartifact:WebAppBuild1); Build Action 2 (input artifact:\\nWebApp, output artifact:WebAppBuild2)","C":"Build\\nAction\\n1\\n(input\\nartifact:\\nempty,\\noutput\\nartifact:WebAppBuild1); Build Action 2 (input artifact:\\nWebApp2, output artifact:WebAppBuild2)","D":"Build Action 1 (input artifact: WebApp, output\\nartifact:WebAppBuild1); Build Action 2 (input artifact:\\nWebApp2, output artifact:WebAppBuild2)"},"answer":"B","explanation":"To set up input/output artifacts of CodeBuild stage in\\nCodePipeline. One thing to note is that CodeBuild should have at least 1\\n\\n\\finput artifact. So Option B is correct as it has one input artifact for both\\nstages."},{"id":"165","question":"A DevOps engineer is constructing a pipeline for an ongoing\\nJava project. The CI/CD software is chosen as AWS CodePipeline.\\nCloudFormation stacks are used during the deployment process.\\nNonetheless, a variety of conditions must be taken into account. For\\nexample, if there is no stack, the stack may be generated. Check\\nwhether a stack already exists or should be created and then\\ninspected. The team managers tend to introduce an overviewed state\\nmachine and each state executes a Lambda operation. How can this\\nrequirement best be fulfilled?","options":{"A":"Use a shell script in EC2 to interface with AWS MQ service to\\nachieve the function of the state machine. Depending on the\\nrunning status of AWS MQ service, the script returns the\\nexecution result back to CodePipeline","B":"Establish several SQS queues to indicate running status. The\\nLambda function for each state gets the message from the\\nqueue and processes the deployment task accordingly. The\\nfinal result is returned to CodePipeline by a dedicated Lambda","C":"Use a Python script in EC2 to record the running status and\\nachieve the state machine feature. It calls different Lambda\\nfunctions depending on the current status. The script returns the\\nexecution status back to CodePipeline","D":"Use a Lambda Function to interface with an AWS Step\\nFunctions, which implements the workflow-driven state\\nmachines for CloudFormation stack deployment. The Lambda\\nFunction returns the execution status back to CodePipeline"},"answer":"D","explanation":"This question requires a visualized state machine. The best tool\\nused to achieve this is AWS Step Functions. It becomes easier for users to\\nunderstand which tasks were performed and why a state is reached because\\nof the AWS Step features. It can also communicate with Lambda and inform\\nAWS CodePipeline about its status."},{"id":"166","question":"Your company CTO assigns you that task to connect a MySQL\\nDatabase to a Wordpress application keeping in ming that the\\nenvironment must be fault tolerant and highly available. Select 2\\noptions from the following, which would individually play their role\\nto perform the required task.","options":{"A":"Create a MySQL RDS environment and create a Read Replica","B":"Create Multiple EC2 instances in the same AZ. Host MySQL\\nand enable replication via scripts between instances","C":"Create a MySQL RDS environment with Multi-AZ feature\\nenabled","D":"Create Multiple EC2 instances in separate AZ. Host MySQL\\nand enable replication via scripts between instances"},"answer":"C and D","explanation":"If you want high availability and fault tolerant environment,\\nthe instances must be located in multiple availability zones. Therefore, if you\\nhost your own MySQL, ensure that you have instances spread over several\\nAZs.\\nBy using Multi-AZ deployments, Amazon RDS delivers high availability and\\nfailover support for DB instances. Amazon’s failover technology is used by\\nMulti-AZ deployments for PostgreSQL, MySQL, Oracle, and MariaDB DB\\ninstances."},{"id":"167","question":"A company is designing loosely coupled system. For executing\\nthis successfully, which of the following design strategies are ideal?\\n(Choose 2)","options":{"A":"Having the web and worker roles running on the same set of\\nEC2 instances","B":"Having the web and worker roles running on separate EC2\\ninstances","C":"Using SQS to establish communication between the web and\\nworker roles","D":"Using SNS to establish communication between the web and\\nworker roles"},"answer":"B and C","explanation":"You can use SQS and separate environments for web and\\nworker processes. Communication between the web and worker roles is\\nmanaged by the SQS queue."},{"id":"168","question":"You assist a team in developing a new AWS system CI/CD\\nPipeline. The pre-requisite is that an orchestration system like origin\\ncontrol, design, and implementation handle the pipeline as a whole.\\nThroughout implementation, various workflow-driven activities are\\ntaken into account to ensure that the state machine engine is able to\\nhandle these actions correctly. The execution status of the pipeline\\nand state machine should be visualized. What two combined\\napproaches can implement this? (Choose 2)","options":{"A":"Configure AWS Step Functions State Machine to process the\\nstate transition. Return the running state when being asked","B":"Create an AWS SWF workflow for the state machine\\nexecution. Use workflow decider to control the activity steps","C":"Setup AWS CodeStar to manage the whole CI/CD services in\\nAWS","D":"Configure an AWS CodeDeploy application to process the\\ncontinuous deployment tasks","E":"Set up an AWS CodePipeline. In its deployment stage, call a\\nLambda function, which is the trigger for the state machine"},"answer":"A and E","explanation":"If an orchestration tool is required in AWS, CodePipeline\\nshould be considered as the best service to manage source control, building,\\nand deployment. AWS CodePipeline automates the steps towards the\\ncontinuous publishing of software changes, which is an ideal CI/CD tool.\\nYou can also integrate Step Functions with CodePipeline.\\nWith AWS Step Functions, it becomes easier for users to understand which\\ntasks were performed and why a state is reached. It can also communicate\\nwith Lambda and inform AWS CodePipeline about its status."},{"id":"169","question":"An organization has an Auto-scaling group with the following\\nsetting: Minimum capacity: 2, Desired capacity: 2 and Maximum\\n\\n\\fcapacity: 4.\\nIn the Autoscaling group, the total number of instances is two that are currently\\nrunning. You are advised to ensure that no new instances were\\nintroduced by the Auto-scaling team for a duration of an hour. Which\\ncombination of steps will avoid the release of new instances?\\n(Choose 2)","options":{"A":"Change the Minimum capacity to 2","B":"Suspend the Launch process of the Auto-scaling Group","C":"Change the Desired capacity to 4","D":"Change the Maximum capacity to 2"},"answer":"B and D","explanation":"You may pause the creation of new instances temporarily by\\nlimiting the maximum capacity to 2 so that the existing instances of 2 will be\\nequal to the maximum limit. Secondly, you can suspend the launch process\\nof the Auto-scaling Group."},{"id":"170","question":"How will you, as DevOps engineer, automate the creation of\\nEBS snapshot?","options":{"A":"By using Cloudwatch Events to trigger the snapshots of EBS\\nVolumes","B":"By using the AWS CodeDeploy service to create a snapshot of\\nthe AWS Volumes","C":"By using the AWSConfig service to create a snapshot of the\\nAWS Volumes","D":"By creating a powershell script, which uses the AWS CLI to\\nget the volumes and then run the script as a cron job"},"answer":"A","explanation":"The best thing to do is to use CloudWatch\'s built- in service as\\nCloudWatch events to automate the creation of EBS snapshots. With Option\\nA, you should only run the power shell script on Windows machines and\\nkeep the script itself. And you have the overhead to just run this script with a\\nseparate instance.\\n\\n\\fWhen you go to CloudWatch events, you can use the Target as EC2\\nCreateSnapshot API call.\\nCloudWatch Events provides an almost real-time stream of system events\\ndescribing changes in Amazon Web Services (AWS) resources. You can\\nmatch and route events to one or more target functions or streams by using\\nsimple rules that you can set up quickly."},{"id":"171","question":"You are a partner for AWS and assist a customer in the\\ndevelopment of applications with AWS Elastic Beanstalk network. In\\nthe Amazon Linux system, one significant software will be installed.\\nYou also built some config files in the.ebextensions folder because\\nthere are several Shell commands to execute before and after the\\nprogram version is extracted. What parts of the config file can you\\ninsert commands to make it work properly? (Choose 2)","options":{"A":"deploy section","B":"container command section","C":"file section","D":"services section","E":"commands section"},"answer":"B and E","explanation":"The application Elastic Beanstalk can be configured\\nin.ebextensions folder by the configuration of files. Before the application is\\nset up and the application version file is extracted, the commands specified in\\ncommands sections are executed. This section is therefore required. The\\ncontainer command main section supports executing commands after\\nextracting an application version file."},{"id":"172","question":"A DevOps engineer helps the development team build AWS\\nElastic Beanstalk\'s Node.js software. The maximum number of Autoscaling group instances was updated to 10 in AWS console during\\nenvironment development. In the source bundle, however, the\\n.ebextensions folder contains a configuration file setting the\\nmaximum number of instances to 5. Since the limit of 4 is standard\\nfor Elastic Beanstalk, how many ASGs can the DevOps engineer\\nbuild in the AWS console after the environment?","options":{"A":"5","B":"6","C":"4","D":"10"},"answer":"D","explanation":"To modify the configuration in the Elastic Beanstalk, you can\\nuse AWS CLI and Console. The priorities are “configurations in AWS\\nconsole” > “configurations in .ebextensions” > “default value”. Therefore,\\nthe maximum value of the ASG will be"},{"id":"10","question":"173. Allen created a CloudWatch\\nCloudFormation API call:\\n\\nevents\\n\\nrule\\n\\nto\\n\\ncapture\\n\\n{\\n“source”: [\\n“aws.cloudformation”\\n],\\n“detail-type”: [\\n“AWS API Call via CloudTrail”\\n],\\n“detail”: {\\n“eventSource”: [\\n“cloudformation.amazon.com”\\n]\\n}\\n}\\nThe Event rule has the Lambda Function as a target for slack channel\\nnotifications. Nevertheless, no notification was provided when a\\nCloudFormation stack was modified. Which one of the following could be\\nthe cause?","options":{"A":"Check if CloudTrail logging is turned on in this region. If it is\\nturned off, no AWS API action events can be received","B":"The rule should change in the following snippet\\n“source”: [\\n“aws.cloudtrail”\\n],\\n“detail”: {\\n“eventSource”: [\\n“cloudformation.amazon.com”\\n]\\n}\\n}","C":"CloudTrail can only trace the add and delete for a\\nCloudFormation stack. No API call is recorded for the stack\\nupdate so no notification was received","D":"AWS API Call via CloudTrail cannot be used in a CloudWatch\\nEvent rule"},"answer":"A","explanation":"The event type can be set to \'AWS API Call via CloudTrail\'\\nwhen a CloudWatch Event rule is created. Because the CloudWatch Events\\nRegulations \\"AWS API Call via CloudTrail\\" depends on CloudTrail\\nfunctions, the events rules cannot be triggered by CloudTrail first if\\nCloudTrail is not turned on."},{"id":"174","question":"A company wants to use a SaaS third-party app running on\\nAWS. The SaaS application must be able to issue several API\\ncommands to detect Amazon EC2 resources in the company\'s\\naccount. The company has internal security policies that require\\nexternal access to its environment and must ensure with the minimum\\nprivilege principles and controls, which ensure that the credentials\\nused by the SaaS vendor cannot be accessed by the third party. Which\\nof the following would satisfy all these requirements?","options":{"A":"Create an IAM user within the enterprise account assigning a\\nuser policy to the IAM user that allows only the actions\\nrequired by the SaaS application. Create new access and secret\\nkey for the user and provide these credentials to the SaaS\\nprovider","B":"From the AWS Management Console, navigate to the Security\\nCredentials page and retrieve the access and secret key for your\\naccount","C":"Create an IAM role for EC2 instances, assign it a policy that\\nallows only the actions required for the Saas application to\\nwork, provide the role ARN to the SaaS provider to use when\\nlaunching their application instances","D":"Create an IAM role for cross-account access allowing the SaaS\\nprovider’s account to assume the role and assign it a policy that\\nallows only the actions required by the SaaS application"},"answer":"D","explanation":"Many SaaS platforms provide access to AWS resources via\\ncreated AWS cross account access. You will see the ability to add a crossaccount role if you go to Roles in your identity management."},{"id":"175","question":"A company wants to access the on-premises LDAP server from\\nan application launched on a VPC. The connection of VPC and onpremises location is established through an IPSec VPN. Choose 2\\ncorrect options for application user authentication.","options":{"A":"The application authenticates against LDAP and retrieves the\\nname of an IAM role associated with the user. The application\\nthen calls the IAM Security Token Service to assume that IAM\\nrole. The application can use the temporary credentials to\\naccess any AWS resources","B":"Develop an identity broker that authenticates against LDAP\\nand then calls IAM Security Token Service to get IAM\\nfederated user credentials. The application calls the identity\\nbroker to get IAM federated user credentials with access to the\\nappropriate AWS service","C":"Develop an identity broker that authenticates against IAM\\nSecurity Token Service to assume an IAM role in order to get\\ntemporary AWS security credentials. The application calls the\\nidentity broker to get AWS temporary security credentials","D":"The application authenticates against LDAP and then calls the\\nIAM Security Service to log in to IAM using the LDAP\\n\\n\\fcredentials. The application can use IAM temporary credentials\\nto access the appropriate AWS service"},"answer":"A and B","explanation":"If you need an on-premises environment to work with a cloud\\nenvironment, you usually have two artifacts for authentication:\\nAn Identity Store: This is the on- site store like Active Directory, where\\ninformation of all the users and groups are stored.\\nAn Identity Broker: It acts as an intermediate agent between the cloud\\nenvironment and the on-premises location. This facility is provided by a\\nsystem called Active Directory Federation services in Windows.\\nThe outside user is first authenticated by the Identity broker using active\\ndirectories. Then the temporary security token is issued to access console or\\nusers’ access APIs."},{"id":"176","question":"There are several AWS CloudFormation templates that Harry\\nhas to maintain. Two incidents occurred last month where someone\\nmodified assets in existing CloudFormation stacks without any alert\\nor notification. This had negative effects on the stacks of possible\\nchanges, and the changes were lost when the stacks were reset. This\\nis not compliant with company policy. His team leader has demanded\\nthat he alerts the team when drifts occur in the CloudFormation stack.\\nWhat is the best way to do this?","options":{"A":"Enable CloudTrail. Use a Lambda function to analyze the\\nCloudTrail logs. Send an email if it is found that the resources\\nin CloudFormation stacks are modified","B":"Create a CloudWatch event rule for CloudFormation. If any\\nevent happens for CloudFormation, trigger an email\\nnotification by SNS","C":"Create a rule in AWS Config to evaluate if the stack is\\nconsidered to have drifted for its resources. If the rule is\\nNON_COMPLIANT, notify the team via an SNS notification","D":"Use a Lambda function to check the drift status in each\\nCloudFormation stack every 10 minutes. If there is a drift, send\\nthe team an email via AWS SES service"},"answer":"C","explanation":"There is a rule operated by AWS in AWS Config\\n(cloudformation-stack-drift-detection-check) to see if drift occurs. So,\\nquickly set drift status rule and users can see whether or not the stacks are\\ncompliant. Notify everyone then via AWS SES."},{"id":"177","question":"You are using an EC2 instance, on which web and worker role\\ninfrastructure is defined. Jobs sent by the web role are managed\\nthrough SQS. Now, what way will be suitable to check that the jobs\\nare sent properly by the web role?","options":{"A":"Use CloudWatch monitoring to check the size of the queue and\\nthen scale out SQS to ensure that it can handle the right number\\nof jobs","B":"Use CloudWatch monitoring to check the size of the queue and\\nthen scale out using Auto-scaling to ensure that it can handle\\nthe right number of jobs","C":"Use ELB to ensure that the load is evenly distributed to the set\\nof web and worker instances","D":"Use Route53 to ensure that the load is evenly distributed to the\\nset of web and worker instances"},"answer":"B","explanation":"SQS can be used to manage communication between the web\\nand the roles of the worker. The number of messages in the SQS queue can\\nbe used to determine the number of instances the Auto-scaling group should\\nhave."},{"id":"178","question":"Jose is working in a company that has to do a major public\\nannouncement of a social media site in AWS. The website is running\\non EC2 instances launched in multiple availability zones with a\\nMulti-AZ RDS MySQL Extra Large DB Instance. The site executes a\\nhigh number of small reads and writes per second and depends on an\\neventual consistency model. After complete tests, he observes that\\nthere is read contention on RDS MySQL. Which approaches should\\nhe choose to meet these requirements? (Choose 2)","options":{"A":"Deploying ElasticCache in-memory cache running in each\\navailability zone","B":"Increasing the RDS MySQL Instance size and implementing\\nprovisioned IOPS","C":"Adding an RDS MySQL read replica in each availability zone","D":"Implementing sharding to distribute the load to multiple RDS\\nMySQL instances"},"answer":"A and C","explanation":"Enhanced performance and durability for database instances\\nare provided by Amazon RDS read replicas. For read-heavy database\\nworkloads, this replication feature makes it easy to scale out beyond a\\ncapacity constraint of a single DB instance.\\nAmazon ElastiCache is used to deploy, run and scale an in-memory data store\\nor cache in the cloud. It provides a high performance web application\\nbecause it allows you to fetch information from fast, managed, in-memory\\ndata stores."},{"id":"179","question":"A company has given you the task of designing a\\nCloudFormation template. In the template, it must be included that\\nthe CloudFormation stack is deleted and a snapshot of the Relational\\nDB is created as a part of the stack. What should you do to complete\\nthis task?","options":{"A":"Create a new CloudFormation template to create a snapshot of\\nthe relational database","B":"Create a snapshot of the relational database beforehand so that\\nwhen the CloudFormation stack is deleted, the snapshot of the\\ndatabase is present","C":"Use the Update policy of the CloudFormation template to\\nensure a snapshot is created of the relational database","D":"Use the DeletionPolicy of the CloudFormation template to\\nensure a snapshot is created of the relational database"},"answer":"D","explanation":"When the resource stack is deleted, you can store or (in some\\n\\n\\fcases) backup a resource with the DeletionPolicy attribute. For each resource\\nyou would like to control, you have to specify a DeletionPolicy attribute.\\nAWS CloudFormation can remove the resource by default if a resource has\\nno DeletionPolicy attribute. Please note that this ability also covers the\\nupdate operations, which results in the removal of resources."},{"id":"180","question":"AWS DevOps admins are working on building the infrastructure\\nof the company’s development team through CloudFormation, which\\nincludes the VPC and networking components, installing a LAMP\\nstack and securing the created resources. For designing this template,\\nchoose the best way among the following options.","options":{"A":"Create multiple CloudFormation templates based on the\\nnumber of development groups in the environment","B":"Create multiple CloudFormation templates for each set of\\nlogical resources, one for networking, and the other for LAMP\\nstack creation","C":"Create multiple CloudFormation templates based on the\\nnumber of VPCs in the environment","D":"Create a single CloudFormation template to create all the\\nresources since it would be easier from the maintenance\\nperspective"},"answer":"B","explanation":"Creating multiple CloudFormation templates is one of the\\nexamples of nested stacks. When infrastructure grows, common patterns may\\narise in which each of your templates declare the same components. You can\\ncreate dedicated templates and separate common components. In this way,\\nyou can mix and match various templates, but use nested stacks to create a\\nsingle stack. To create other stacks with in a stack, one should use nested\\nstacks. AWS::CloudFormation::Stack resource is used in the template to refer\\nother templates to create nested stacks."},{"id":"181","question":"A company is using OpsWork with several stacks for\\ndevelopment, staging, and production to deploy and manage the\\napplication. The company wants to start using python instead of Ruby\\nas it already as Ruby on Rail content management platform. Choose\\n\\n\\fthe correct solution to manage the new deployment.","options":{"A":"Create a new stack that contains a new layer with the Python\\ncode. To cut over the new stack the company should consider\\nusing Blue/Green deployment","B":"Update the existing stack with Python application code and\\ndeploy the application using the deploy life-cycle action to\\nimplement the application code","C":"Create a new stack that contains Python application code and\\nmanage separate deployments of the application via the\\nsecondary stack using the deploy life-cycle action to implement\\nthe application code","D":"Create a new stack that contains Python application code and\\nmanage separate deployments of the application via the\\nsecondary stack"},"answer":"A","explanation":"Blue/green deployment is the technique for application release\\nby shifting of traffic from different application versions in two identical\\nenvironments. Blue / green deployments can reduce common risks associated\\nwith software deployments, such as down time and rollback."},{"id":"182","question":"You are responsible for creating a cloud information template to\\nspin resources on your DevOps team\'s demand. The necessity is to\\ndistribute assets in different regions through this cloud creation\\nmodel. Which of the following aspects will help you develop a model\\nto spin the resources based on region?","options":{"A":"Use the metadata section in the Cloudformation template, so\\nthat based on the relevant region, the relevant resource can be\\nspun up","B":"Use the parameters section in the Cloudformation template, so\\nthat based on the relevant region, the relevant resource can be\\nspun up","C":"Use the mappings section in the Cloudformation template, so\\nthat based on the relevant region, the relevant resource can be\\n\\n\\fspun up","D":"Use the outputs section in the Cloudformation template, so that\\nbased on the relevant region, the relevant resource can be spun\\nup"},"answer":"C","explanation":"The optional \\"Mappings\\" segment matches a key to a set of\\nnamed values. For example, if you want to set a region-based value, you can\\ncreate a mapping using the name of the region as a key and containing values\\nfor each particular region. You use the intrinsic function Fn::FindInMap to\\nget values from a map."},{"id":"183","question":"The company for which you are working has an enormous\\ninfrastructure built on AWS. However, there are some security\\nconcerns regarding this infrastructure and an external auditor has\\nbeen given the task of thoroughly checking all the AWS assets of\\nyour company. Your company is located in the Asia-Pacific (Sydney)\\nregion of AWS whereas the auditor is in the USA. You have been\\nassigned the task of providing the auditor with login in order to check\\nall your VPC assets, in particular, security groups and NACLs.\\nChoose the best and secured solution for initiating this investigation.","options":{"A":"Give him root access to your AWS Infrastructure; As he is an\\nauditor, he will need access to every service","B":"Create an IAM user who will have read-only access to your\\nAWS VPC infrastructure and provide the auditor with those\\ncredentials","C":"Create an IAM user tied to an administrator role. Also, provide\\nan additional level of security with MFA","D":"Create an IAM user with full VPC access but set a condition\\nthat will not allow him to modify anything if the request is\\nfrom any IP other than his own"},"answer":"B","explanation":"It is only providing the required permissions as high level\\npermissions should be avoided. So you can choose this option as it is the best\\nfit for this requirement."},{"id":"184","question":"As an orchestration tool for the control of pipelines, AWS\\nCodePipeline has been used in a financial firm. Certain AWS\\nservices, like CodeCommit, CodeBuild, CodeDeploy, etc, can be\\ninstalled in pipelines. There is a safety policy in place to manage all\\nartifacts created during the execution of the pipeline in a durable and\\nhighly available location. What is the correct statement about\\nhandling an objects by AWS CodePipeline? (Choose 2)","options":{"A":"When CodeBuild is used in the build stage, the user can only\\nconfigure input artifacts but not output artifacts","B":"User can select a custom S3 bucket as the artifact store while\\ncreating a new pipeline","C":"When artifacts are saved in an S3 bucket, the bucket can\\nbelong to a different region","D":"Users can configure a default S3 bucket as the artifact store in\\nthe same region and account as the pipeline","E":"An artifact can be stored in S3 or EC2 EBS"},"answer":"B and D","explanation":"The client may choose an S3 bucket default or a custom S3\\nbucket to store artifacts when building pipelines. By default, for each\\npipeline, a dedicated folder is created in the s3 bucket."},{"id":"185","question":"You have designed a critical application with the requirement of\\nthe least downtime of rollback (if required). You want to rollout\\nupdates to your application introduced on Elastic Beanstalk\\nEnvironment. Which of the following deployment action is best\\nsuitable for this purpose?","options":{"A":"Create another parallel environment in Elastic Beanstalk. Use\\nthe swap URL feature","B":"Create a CloudFormation template with the same resources as\\nthose in the Elastic Beanstalk environment","C":"Create another parallel environment in Elastic Beanstalk.\\nCreate a new Route53 Domain name for the new environment\\nand release that URL to the users","D":"Use Rolling updates in Elastic Beanstalk so that if the\\ndeployment fails, the rolling update feature would roll back to\\nthe last deployment"},"answer":"A","explanation":"As the requirement is the least downtime, it is ideal for\\ncreating a blue green deployment environment and use the Swap URL\\nfunction to swap new deployment environments and then swap back in case\\nof deployment failure.\\nWhen you update your application versions, Elastic Beanstalk implements\\nthe in-place update, which may make your application unavailable for a short\\nperiod of time.\\nThis downtime can be prevented by a blue-green deployment where the new\\nversion is deployed in a different environment. CNAMEs of both\\nenvironments can be swapped, and the traffic can be redirected to a new\\nversion instantly."},{"id":"186","question":"The IT department of a company wants to launch instances in\\nthe Auto-scaling group. They need to setup lifecycle hooks for setting\\ncustom based software and do the required configuration on the\\ninstances. This setting would take an hour at maximum. Considering\\nthis scenario, how will you suggest to setup lifecycle hooks? Select 2\\nanswers.","options":{"A":"Configure the lifecycle hook to record heartbeats. If the hour is\\nup, restart the timeout period","B":"Configure the lifecycle hook to record heartbeats. If the hour is\\nup, choose to terminate the current instance and start a new one","C":"If the software installation and the configuration is complete,\\nthen send the signal to complete the launch of the instance","D":"If the software installation and the configuration is complete,\\nthen restart the time period"},"answer":"A and C","explanation":"The instance will wait for an hour by default, and Auto-scaling\\nwill carry on with the launch or terminate the process\\n(Pending:Proceed or Terminating:Proceed). You can reboot the timeout\\n\\n\\fperiod by recording a heartbeat if you need more time. You can finish the\\nlifecycle action that continues the launch or termination process if you finish\\nbefore the time limit ends."},{"id":"187","question":"Your company has announced a new IT procedure for EC2\\ninstances, which states that EC2 instances must be of the particular\\ninstance type. You want to find out the list of instances that do not\\nmatch the demanded instance type. From the following options,\\nwhich one would you use to obtain the list?","options":{"A":"Use TrustedAdvisor to check which EC2 instances do not\\nmatch the intended instance type","B":"Use VPC Flow Logs to check which EC2 instances do not\\nmatch the intended instance type","C":"Use AWS CloudWatch alarms to check, which EC2 instances\\ndo not match the intended instance type","D":"Use AWS Config to create a rule to check EC2 instance type"},"answer":"D","explanation":"You can create a rule in AWS Config that can check whether\\nEC2 Instances follow a particular type of instance."},{"id":"188","question":"Your company IT supervisor is interested in optimizing the cost\\nof running AWS resources. Which of the 2 options are suitable for\\nthis purpose?","options":{"A":"Use the Trusted advisor to see the underutilized resources","B":"Create a script, which monitors all the running resources and\\ncalculates the cost accordingly. It analyses those resources and\\nsees which can be optimized","C":"Create CloudWatch alarms to monitor underutilized resources\\nand either shutdown or terminate resources, which are not\\nrequired","D":"Create CloudWatch logs to monitor underutilized resources and\\neither shutdown or terminate resources, which are not required"},"answer":"A and C","explanation":"CloudWatch alarms can be used to see if the resources\\nthreshold level, for a long time period, is below or not. If it is below the\\nthreshold level, then you can decide whether to stop or terminate the\\nresources.\\nWith Trusted Advisor, you will obtain all kinds of checks that can be used to\\noptimize or reduce the costs of your AWS resources when you enable the\\nCost Optimization section."},{"id":"189","question":"Jenifer has an Auto-scaling group with the following setting:\\nMinimum capacity: 2, Desired capacity: 2 and Maximum capacity:\\n4.\\nThe launch setup\\nhas AMIs that are t2.micro-instance-based. The program that runs on\\nthese instances now faces problems and she has found that the\\nsolution is to change the type of instance in the Auto-scaling group.\\nFrom the following, which meets the desired requirement?","options":{"A":"Change the desired and maximum size of the Auto-scaling\\ngroup to 4. Make a copy of the launch configuration. Change\\nthe instance type in the new launch configuration. Attach that\\nto the Auto-scaling group. Change the maximum and Desired\\nsize of the Auto-scaling Group to 2","B":"Make a copy of the Launch configuration. Change the instance\\ntype in the new launch configuration. Attach that to the Autoscaling Group. Change the maximum and Desired size of the\\nAuto-scaling Group to 4. Once the new instances are launched,\\nchange the Desired and maximum size back to 2","C":"Delete the current Launch configuration. Create a new launch\\nconfiguration with the new instance type and add it to the\\nAuto-scaling Group. This will then launch new instances","D":"Change the Instance type in the current launch configuration.\\nChange the Desired value of the Auto-scaling Group to 4.\\nEnsure the new instances are launched"},"answer":"B","explanation":"The launch configuration must be copied and a new instance\\ntype added. The Auto-scaling group changes to include the new type of\\ninstance. Switch to value 4 as is the desired number in the Auto-scaling\\n\\n\\fgroup so that new instance instances can be started. Once it is released,\\nswitch the requested size back to 2 to delete the instances with the older\\nsetup. Please note that the current instances are equally distributed over\\nseveral AZs, as Auto-scaling uses the AZRebalance method as its first means\\nof terminating instances."},{"id":"190","question":"You are running a video processing application launched in\\nAWS. Users upload videos on site, which are then processed by using\\nthe built-in custom program in case if there are any failures in\\nprocessing, the program will be able to balance the situation. Now\\nconsidering the minimum cost budget, which of the following\\nmechanism should you use to deploy the instance for running video\\nprocessing activities?","options":{"A":"Create a launch configuration with Spot Instances. Ensure the\\nuser section data details the installation of the custom software.\\nCreate an Auto-scaling group with the launch configuration","B":"Create a launch configuration with Dedicated Instances. Ensure\\nthe user section data details the installation of the custom\\nsoftware. Create an Auto-scaling group with the launch\\nconfiguration","C":"Create a launch configuration with On-Demand Instances.\\nEnsure the user section data details the installation of the\\ncustom software. Create an Auto-scaling group with the launch\\nconfiguration","D":"Create a launch configuration with Reserved Instances. Ensure\\nthe user section data details the installation of the custom\\nsoftware. Create an Auto-scaling group with the launch\\nconfiguration"},"answer":"A","explanation":"The application should be able to recover the failures and\\nsolutions should be cost effective therefore, spot instances are best for this\\npurpose. The launch configuration can be used to request spot instances."},{"id":"191","question":"You are an AWS DevOps engineer in a company and have\\ncreated a job in Jenkins to use an Elastic Beanstalk script. The eb\\nconfig command was used to set up an Elastic Load Balancer\\n\\n\\fSecurity Group. After a while, one of your colleagues thought that the\\nsecurity group should be changed to another and modified the\\nconfiguration option in the .ebextensions folder with the config file.\\nThe Security Group, however, did not change to the new Jenkins job\\nwhen it was rebuilt. What is the issue and how are you going to\\nresolve it?","options":{"A":"The settings in .ebextensions folder cannot override that in the\\nEB CLI command. Remove the configuration settings for the\\nSecurity Group in eb config command","B":"When there is a conflict for the configurations in .ebextensions\\nand eb config, the default value of the option is used. The\\nSecurity Group settings in eb config should be removed to\\navoid a conflict","C":"The .ebextensions config file was processed first. The eb config\\nran next and overrode the .ebextensions config file. Modify the\\nscript in Jenkins so that the .ebextensions config file is\\nprocessed after eb config","D":"Security Group name cannot be changed in .ebextensions\\nconfig file. The eb config in the script needs to be modified to\\nuse the correct Security Group"},"answer":"A","explanation":"The key to this question is that when Elastic Beanstalk settings\\nare applied, there are different priorities. The AWS Management Console,\\nEB CLI, AWS CLI and SDK settings are the safest and most appropriate\\nsettings for those directly applicable to the system. So, Option A is the best\\nsuit for the given question."},{"id":"192","question":"On the SaaS platform, an email with important information is\\nsent to the user when an existing user modifies his/her subscription\\ninformation. The subscription data is stored in a DynamoDB table\\nand the stream allows the changes to table item to be recorded. The\\nstream data is tracked and emails sent accordingly through a Java\\nprogram. Which statement is correct about the use of DynamoDB\\nstreams?","options":{"A":"The information in the Stream log is stored forever","B":"DynamoDB Streams can only write a stream record when items\\nin the table are created or updated","C":"The data in DynamoDB Streams is also encrypted","D":"The Java application can only view the data items as they\\nappeared after they were modified"},"answer":"C","explanation":"The data in both DynamoDB table (at rest) and DynamoDB\\nStreams is fully encrypted. All other options are not correct."},{"id":"193","question":"There are many onsite servers operated by a company who plans\\nto move its main servers to AWS ap-southeast-2 in order to formulate\\na disaster recovery plan in the cloud. The servers are installed on\\nMicrosoft HyperV-managed virtual machines for Windows. The\\ncompany’s team wants to use an AWS platform to ease the migration\\nprocess by periodically creating AMIs. The tool can also schedule\\nreplications and monitor progress. Which tool should be used to\\nfulfill the requirement?","options":{"A":"AWS Database Migration Service","B":"AWS DataSync","C":"AWS Server Migration Service","D":"AWS Migration Hub"},"answer":"C","explanation":"AWS Server Migration Service is ideal to simplify the AWS\\nmigration by setting up AMI ready to deploy on Amazon EC2 for on-site\\nvirtual machines within Microsoft Hyper-V or SCVMM."},{"id":"194","question":"The technical team of your company is concerned with AWS\\naccount security. What will you suggest to prevent the account from\\nbeing hacked?","options":{"A":"Do not write down or remember the root account password\\nafter creating the AWS account","B":"Use a short but complex password on the root account and any\\nadministrators","C":"Use AWS IAM Geo-Lock and disallow anyone from logging in\\nexcept for in your city","D":"Use MFA on all users and accounts, especially on the root\\naccount"},"answer":"D","explanation":"The user can add an additional protective layer on top of the\\nusername and password using MFA. When MFA is enabled, the user is\\nprompted for a username and password (first factor, what user knows), and\\nauthentication code of the AWS MFA system (second factor, what user has)\\nto be registered on an AWS website."},{"id":"195","question":"Your company’s owner asked you to show all of the\\nCloudFormation stacks, which have a completed status. Which\\ncommand should you use?","options":{"A":"list-stacks","B":"stacks-complete","C":"list-templates","D":"describe-stacks"},"answer":"A","explanation":"This command returns the stack information of all the stacks\\nwhose status is matched with StackStatusFilter. If the command does not find\\nany stack filter, it will return the information of all stacks. Information of the\\ndeleted stacks is kept in record for 90 days."},{"id":"196","question":"A blue/green deployment approach for a new application is\\nbeing applied by the Allen. A CloudFormation stack with tools like\\nAuto-scaling group, launch setup and Classic Load Balancer allows\\nthe application to be deployed for every new release. Route53\\nconfigures a weighted routing system. For route53, a small\\npercentage of traffic enters the green environment and the weight\\nincreases until the full production traffic is borne by the green\\nenvironment. What is the downside of this approach?","options":{"A":"Only a classic load balancer is suitable for this approach. A\\nnetwork load balancer or application load balancer cannot be\\nused together with the Route53 weighted routing policy","B":"Route53 weighted routing policy is not suitable if a rollback is\\nrequired","C":"DNS TTL decides how long clients cache query results.\\nCertain sessions may still be tied to the previous environment\\nso that it may impact the speed to deploy a new environment or\\nrollback","D":"Weighted routing policy is complicated to implement in a\\npipeline"},"answer":"C","explanation":"One of the downsides of blue/green deployment is that older\\nclients may take longer timers until they use the new environment. DNS TTL\\ndecides how long the cache requests output for the cache users.\\nNonetheless, some sessions may still be related to the previous setting of\\nolder clients and potentially misbehaving clients in the wild.\\nFor further detail visit:\\nhttps://d1.awsstatic.com/whitepapers/AWS_Blue_Green_Deployments.pdf"},{"id":"197","question":"Leo has just started using AWS ECS/ECR for its Docker\\napplications. He is searching for a pipeline system that can use a\\nblue/green configuration to deploy the container software in the ECS\\ncluster. At present, all Docker pictures are stored as artifactories in\\nthe ECR system in pipelines. How is this pipeline to be applied the\\nquickest?","options":{"A":"In AWS CodeDeploy, create a CodeDeploy application and a\\ndeployment group to configure the blue/green deployment to\\nECS cluster using the image in the ECR repository","B":"In AWS CodePipeline, add a source stage for ECR docker\\nimage and a deployment stage for ECS where the deployment\\nruns with a CodeDeploy application and deployment group","C":"In AWS CodePipeline, configure a source stage for ECR, a\\nbuild stage with CodeBuild for the docker image and a\\n\\n\\fdeployment stage using CloudFormation to deploy ECS cluster","D":"Create a Jenkins server in EC2 instance. In the Jenkins job, add\\na source stage for ECR docker image and a deployment stage\\nfor the ECS cluster using AWS CLI commands"},"answer":"B","explanation":"In CodePipeline, two-stage services will fulfill the necessity.\\nThe action provider could be selected as Amazon ECS (Blue / Green) during\\nits deployment stage. The use of the Jenkin server is not good for this\\nrequirement. Using CloudFormation for deployment of Blue/Green\\ndeployment needs more effort as compared to the use of CodePipeline\\nbecause the pipeline cannot be complete in AWS CodeDeploy. The pipeline\\nneeds an origin step, for instance, if the new image is uploaded to Amazon\\nECR, it can deploy container images automatically."},{"id":"198","question":"What is the meaning of the given code in the CloudFormation\\ntemplate?\\n“SNSTopic” : {\\n“Type” : “AWS::SNS::Topic”,\\n“Properties” : {\\n“Subscription” : [{\\n“Protocol” : “sqs”,\\n“Endpoint” : { “Fn::GetAtt” : [ “SQSQueue” , “Arn” ] }\\n}]\\n}","options":{"A":"It creates an SNS topic and adds a subscription ARN endpoint\\nfor the SQS resource created under the logical name\\nSQSQueue","B":"It creates an SNS topic and then invokes the call to create an\\nSQS queue with a logical resource name of SQSQueue","C":"It creates an SNS topic that allows SQS subscription endpoints","D":"It creates an SNS topic, which allows SQS subscription\\nendpoints to be added as a parameter on the template"},"answer":"A","explanation":"The function Fn::GetAtt returns the value of an attribute from\\nany resource in the template."},{"id":"199","question":"A large number of aerial image data has been uploaded to S3 by\\nyour company. In the past, you used a dedicated group of servers in\\nyour local environment to process these data and used Rabbit MQ–\\nan open source message system to get job information to the servers.\\nThe data would go to the tape and be shipped offsite once processed.\\nNow your manager told you to use the current design along with\\nAWS archive storage and messaging to reduce costs. Which option is\\nright?","options":{"A":"Setup Auto-scale workers triggered by queue depth that use\\nspot instances to process messages in SQS. Once data is\\nprocessed, change the storage class of the S3 objects to Glacier","B":"Use SNS to pass job messages and use CloudWatch alarms to\\nterminate spot worker instances when they become idle. Once\\ndata is processed, change the storage class of the S3 object to\\nGlacier","C":"Use SQS for passing job messages. Use CloudWatch alarms to\\nterminate EC2 worker instances when they become idle. Once\\ndata is processed, change the storage class of the S3 objects to\\nReduced Redundancy Storage","D":"Change the storage class of the S3 objects to Reduced\\nRedundancy Storage. Setup Auto-scaled workers triggered by\\nqueue depth that use spot instances to process messages in\\nSQS. Once data is processed, change the storage class of the S3\\nobjects to Glacier"},"answer":"A","explanation":"RabbitMQ was used internally as a messaging service that is\\nwhy SQS should be used. Therefore option B is invalid as it is using SNS.\\nThe best option for reducing costs is Glacier, as everything was stored on\\ntape at the on- site location. So, Option C is therefore out as it is using RRS.\\nOption D is not valid as there is no need to use the storage class RRS to put\\nthe file in it.\\n\\n\\fHence option A is more suitable as it is using Glacier for processed data."},{"id":"200","question":"A company is using an Auto-scaling group to scale out and scale\\nin EC2 instances. The traffic peak occurs every Monday at 8 am and\\nit comes down before the weekend on Friday at 5 pm. If you have to\\nconfigure Auto-scaling group in this scenario, what would you do?","options":{"A":"Create a scheduled policy to scale up on Friday and scale down\\non Monday","B":"Create a scheduled policy to scale up on Monday and scale\\ndown on Friday","C":"Create dynamic scaling policies to scale up on Monday and\\nscale down on Friday","D":"Manually add instances in the Auto-scaling group on Monday\\nand remove them on Friday"},"answer":"B","explanation":"Scheduling based scaling allows you to scale according to the\\nchange in a load of the application. I.e., if the traffic level is high, then it will\\nscale up, and when it is low, it will scale down."},{"id":"201","question":"What is the possible cost-effective solution to storing a large\\nvolume of data that is accessible for a short period and archived\\nindefinitely after that?","options":{"A":"Keeping all your data in S3 since this is durable storage","B":"Storing your data in an EBS volume, and using lifecycle\\npolicies to archive to Amazon Glacier","C":"Storing your data in Amazon S3, and using lifecycle policies to\\narchive to S3-infrequently-access","D":"Storing your data in Amazon S3, and using lifecycle policies to\\narchive to Amazon Glacier"},"answer":"D","explanation":"Amazon Glacier is a long term secure and durable storage\\nservice that is used for data archiving and long-term back-ups.\\nThe configuration is a set of rules to define the action on the Amazon S3\\n\\n\\fbucket for a group of objects. Lifecycle configuration enables you to manage\\nthe S3 lifecycle. Following actions can be defined for Amazon S3:\\nTransition Actions: Transition actions describe the transition of one storage\\nclass to another storage class, for example, you want to change your S3\\nstorage to Glacier after 30 days.\\nExpiration Actions: This action defines the expiration timeline of objects. S3\\ndeletes the expired objects automatically."},{"id":"202","question":"Barrett just created a regional cluster Aurora MySQL in apsoutheast-2 on AWS console as part of a new venture. The Aurora\\ncluster is highly available and durable, with a number of replicas in\\nthe Aurora database. What statement is correct about the replicas of\\nthe Aurora database? (Choose 2)","options":{"A":"Replicas are configured with smaller storage volume than the\\nprimary DB instance to save cost","B":"By default, there is only one replica generated in each\\navailability zone","C":"The replicas are located in different availability zones in the\\nsame region as the primary database, which is ap-southeast-2","D":"The replicas can share both the read and write traffic","E":"Aurora automatically fails over to an Aurora Replica if the\\nprimary DB instance becomes unavailable"},"answer":"C and E","explanation":"A primary DB instance and Aurora replicas are included in an\\nAmazon Aurora Database cluster. Since replicas are distributed over multiple\\navailability zones to ensure that even if one availability region is out of\\noperation, the server still operates. In the case of primary DB failover to an\\nAurora, replica is the key advantage of Aurora."},{"id":"203","question":"Using ELB, Auto-scaling group of Java/Tomcat application\\nservers, and DynamoDB as a data store in EC2 instance, a webstartup is running its successful social news app. Web applications\\nrequire high memory therefore, m2x large is the most suitable\\ninstance. The semi-automated creation and testing of a new AMI for\\nthe application servers is required by each new deployment, which\\n\\n\\ftakes some time and is done only once a week. A new chat feature\\nwas recently introduced in nodejs and is waiting to be integrated into\\nthe architecture. The new component is shown CPU bound in the\\nfirst test because the company has some experience using Chef, and\\nhas decided to streamline the deployment process and use AWS\\nOpsWorks as an application lifecycle tool to simplify application\\nmanagement and reduce deployment cycles. What configuration is\\nneeded in AWS OpsWorks to integrate the new chat module into the\\nmost cost-effective solution?","options":{"A":"Create one AWS OpsWorks stack, create two AWS OpsWorks\\nlayers and create one custom recipe","B":"Create two AWS OpsWorks stacks, create two AWS\\nOpsWorks layers and create one custom recipe","C":"Create two AWS OpsWorks stacks, create two AWS\\nOpsWorks layers and create two custom recipe","D":"Create one AWS OpsWorks stack, create one AWS OpsWorks\\nlayer and create one custom recipe"},"answer":"A","explanation":"Only one Opswork stack with two layers can be used, one for\\nNode.js and one for the standard app.\\nThe configuration of your entire application is defined by an AWS OpsWorks\\nStack: load balancers, server software, database, etc. You control each part of\\nthe stack by building layers that define the software packages and other\\nconfiguration details such as Elastic IPs and security groups. You can also\\ndeploy your software on layers by identifying the repository and using Chef\\nRecipes optionally to automate everything Chef can do, such as creating\\ndirectories and users, setting up databases, etc. You can use the built-in\\nautomation of OpsWorks Stacks to scale your application and recover from\\ninstance failures automatically. You can check and control who can view and\\nmanage the resources your application uses, including SSH access to the\\ninstances your application uses."},{"id":"204","question":"You are running an application globally. You have multiple EC2\\ninstances running in different regions. You want to monitor the\\nperformance of each instance using CloudWatch. What will you do?","options":{"A":"Create a separate dashboard in each region","B":"This is not possible","C":"Register instances running on a different region to CloudWatch","D":"Have one single dashboard to report metrics to CloudWatch\\nfrom different regions"},"answer":"D","explanation":"AWS resources can be monitored through a single\\nCloudWatch dashboard in several regions. For example, you can create a\\ndashboard that displays the use of CPUs for an EC2 instance in the us-west-2\\nregion with your billing metrics in the us-east-1 region."},{"id":"205","question":"AWS Codepipeline is used by Bert as a tool of continuous\\nintegration and deployment for a Lambda function. In its deployment\\nstage, the deployment provider is CloudFormation and the Action\\nMode is configured as “Create or update a stack”. Initially, the\\npipeline was working fine but there has been an inappropriate update\\nin CloudFormation stack due to some mistake in the template. Now,\\nBert is asked to create a design in which updating directly on stack is\\nnot done and instead, there must be a way to preview the stack\\nchanges before the deployment is completed. How should he work on\\nthis task?","options":{"A":"Modify the Action Mode to “Create or replace a change set”\\nfor users to review the change. Then add another deployment\\nstage to execute the change set if the change set is approved","B":"Change the Action Mode to “Delete a stack” so that the\\nenvironment is clean. Then add another stage to deploy the\\nCloudFormation stack using the new template","C":"Modify the deployment provider to CodeDeploy to avoid any\\nissues on the CloudFormation stack update","D":"Change the code review process so that any issues on\\nCloudFormation stacks are avoided"},"answer":"A","explanation":"The change set is created for analyzing the changes in the\\n\\n\\fresources before execution, by using “Create or replace a change set” action\\nmode. If changes appear correct, the change set is executed."},{"id":"206","question":"An online web store adopted AWS CloudFormation to automate\\nload-testing of the details of their online products. They created two\\ntemplates of CloudFormation, one is for the details of their products\\nand the other one for load testing stack. Load-testing stack creates an\\nRDS Postgres database and two web servers running on EC2 instance\\nthat measures response time, sends HTTP requests, and records the\\nresults into the database. Test time is usually 15 – 30 minutes. The\\nAWS CloudFormation stacks are torn down immediately after the test\\ncompletion. The recorded test results of Amazon RDS database must\\nremain accessible for virtualization and analysis.\\nIf the AWS CloudFormation load-testing stack is deleted, then what could\\nbe the possible solutions that allow access to the test results. (Choose 2)","options":{"A":"Define an updated policy to prevent the deletion of the Amazon\\nRDS database after the AWS CloudFormation stack is deleted","B":"Define a DeletionPolicy of type Retain for the Amazon RDS\\nresource to assure that the RDS database is not deleted with the\\nAWS CloudFormation stack","C":"Define a DeletionPolicy of type Snapshot for the Amazon RDS\\nresource to assure that the RDS database can be restored after\\nthe AWS CloudFormation stack is deleted","D":"Define automated backups with a backup retention period of 30\\ndays for the Amazon RDS database and perform point-in-time\\nrecovery of the database after the AWS CloudFormation stack\\nis deleted","E":"Define an Amazon RDS Read-Replica in the load-testing AWS\\nCloudFormation stack and define a dependency relation\\nbetween master and replica via the Depends On attribute"},"answer":"B and C","explanation":"If the user wants to preserve any resource, then DeletionPolicy\\nshould be attached to it. DeletetionPolicy attributes allow the user to preserve\\nor in some cases backup a resource when its stack is deleted. AWS\\nCloudFormation deletes a resource by default if no DeletionPolicy attribute is\\n\\n\\fattached to it.\\nIf the user specifies the Retain for any source, this will prevent the resource\\ndeletion if its stack is deleted."},{"id":"207","question":"When considering AWS Elastic Beanstalk, the \'Swap\\nEnvironment URLs’ is helpful in which of the following\\ndeployments?","options":{"A":"Immutable Rolling Deployments","B":"Blue-Green Deployments","C":"Canary Deployments","D":"Mutable Rolling Deployments"},"answer":"B","explanation":"When you update your application versions, Elastic Beanstalk\\nimplements the in-place update, which may make your application\\nunavailable for a short period of time. This downtime can be prevented by a\\nblue- green deployment where the new version is deployed in a different\\nenvironment. CNAMEs of both environments can be swapped, and the traffic\\ncan be redirected to a new version instantly.\\nDeployments in blue/green require your environment to run independently of\\nyour production database if your application uses one. If you have an\\nAmazon RDS DB instance attached to your environment, the data will not be\\ntransferred to your second environment and will be lost if the original\\nenvironment is terminated."},{"id":"208","question":"A team uses many Lambda functions to develop micro-services.\\nThe group has consulted Aliana on how the project can be designed.\\nThe pipeline needs to be AWS-CodePipeline, which enables the\\nLambda code to be continuously generated and the applications\\ndeployed in AWS, wherever a team member has a new Git commit.\\nWhat service combinations are appropriate for that scenario in AWS\\nCodePipeline? (Choose 2)","options":{"A":"Source stage (CodeCommit), Build stage (Travis CI),\\nDeployment stage (CloudFormation)","B":"Source stage (Github), Build stage (Jenkins), Deployment stage\\n(CodeDeploy)","C":"Source stage (Bitbucket), Build stage (CodeBuild),\\nDeployment stage (Amazon ECS)","D":"Source stage (GitHub), Build stage (Jenkins), Deployment\\nstage (Elastic Beanstalk)","E":"Source stage (CodeCommit), Build stage (CodeBuild),\\nDeployment stage (CloudFormation)"},"answer":"B and E","explanation":"Different services within CodePipeline can be implemented at\\nvarious stages, which is highly flexible for users. Nevertheless, not every\\nservice is suitable in this particular case, even if it may be valid for another\\nscenario. When a new pipeline is created, then you need to configure the\\nsource, build and deploy stages:\\nSource stage include- CodeCommit, ECR, GitHub, and S3\\nBuild stage include- Jenkins or CodeBuild\\nDeploy stage includes- CodeDeploy, ElasticBeanstalk,\\nCloudFormation, S3, ECS and Service Catalog\\nOption D is close to the answer but Elastic Beanstalk is used for deployment\\nof EC2 instance not for Lambda. While all other options are invalid."},{"id":"209","question":"Which system architecture is best for a company that is working\\non automatic photograph tagging by using Artificial Neural Networks\\n(ANNs), which have C++ format and its processes on GPU? The\\nimages loaded in the S3 bucket are in millions but on average, 3\\nimages per day. You control the S3 bucket for you in a batch. You\\nhave control on one more S3 bucket, in which a customer publishes\\nJSON formatted manifest. Bootstrap time of your neural network\\nsoftware is 5 minutes, and an image takes 10 milliseconds to process\\nusing full GPU. Tags are JSON formatted, which must be published\\nto S3 bucket.","options":{"A":"Make an S3 notification configuration, which publishes to\\nAWS Lambda of the manifest bucket. Make the Lambda\\nCloudFormation stack, which contains the logic to construct an\\nAuto-scaling worker tier EC2 G2 instances with the artificial\\n\\n\\fneural network code on each instance. Handle the\\nCloudFormation Stacks creation success or failure using\\nanother Lambda function. Create an SQS queue of the images\\nin the manifest. Tear the stack down when the queue is empty","B":"Deploy your artificial neural network code to AWS Lambda as\\na bundled binary for the C++ extension. Make an S3\\nnotification configuration on the manifest, which publishes to\\nanother AWS Lambda running controller code. This controller\\ncode publishes all the images in the manifest to AWS Kinesis.\\nYour ANN code Lambda function uses the Kinesis as an Event\\nsource. The system automatically scales when the stream\\ncontains the images","C":"Create an OpsWorks stack with two layers. The first contains\\nlifecycle scripts for launching and bootstrapping an HTTP API\\non G2 instances for image processing, and the second has an\\nalways-on instance, which monitors the S3 manifest bucket for\\nnew files. When a new file is detected, requests instances to\\nboot on the new artificial neural network layer. When the\\ninstances are booted and the HTTP APIs are up, submit\\nprocessing requests to individual instances","D":"Create an Auto-scaling, Load Balanced Elastic Beanstalk\\nworker tier Application, and Environment. Deploy the artificial\\nneural network code to G2 instances in this tier. Set the desired\\ncapacity to 1. Make the code periodically check S3 for new\\nmanifests. When a new manifest is detected, push all of the\\nimages in the manifest into the SQS queue associated with the\\nElastic Beanstalk worker tier"},"answer":"A","explanation":"The S3 Events are the best way to be informed when the\\nimages are sent to the bucket. You do not need to provide infrastructure here\\nin advance, and since the S3 source provides event management, this should\\nbe used.\\nAmazon S3 can publish events (e.g., when an object is created in a bucket) to\\nAWS Lambda and use your Lambda function as a parameter by passing the\\nevent data. This integration allows you to write Amazon S3 events Lambda\\n\\n\\ffunctions. In Amazon S3, you add bucket notification settings that identify\\nthe type of event you want Amazon S3 to publish and the Lambda function\\nyou want to invoke.\\nFurther information as to why the second function of Lambda is required:\\nYou can use AWS Lambda to create a CloudFormation stack.\\nCloudFormation stack creation is an asynchronous call, so we do not have to\\nwait until the whole stack moves to FAILED/SUCCEEDED state. In the\\nCloudFormation advance section, you can get the notification of the status of\\nthe stack via SNS notification."},{"id":"210","question":"The organization Byron works for is creating an app and he must\\ndecide which AWS services to use. Customers should be able to\\nupload new pictures to an S3 bucket via this Mobile App. A Lambda\\nfunction is triggered when a new object has been uploaded. There is a\\ncouple of processing steps later on. The Lambda function, for\\nexample, calls the Recognition API and Amazon Recognition service.\\nMetadata including both size and format is extracted in another\\nLambda function. The data is ultimately stored in a DynamoDB.\\nWhat AWS software would he use to manage image analysis tasks?","options":{"A":"Use a Jenkins pipeline to add several steps, which manage the\\nvarious tasks for this application","B":"Create an AWS Batch Job to manage and coordinate the tasks\\nfor this application","C":"Use a CloudFormation template to create several SQS queues\\nand Lambda functions to manage the workflow for this\\napplication","D":"Use AWS Step Functions to manage the steps and tasks into a\\nserverless workflow"},"answer":"D","explanation":"In this scenario, a number of steps need to be organized by an\\nAWS system. As the AWS Step Functions are suitable to handle a number of\\nservices, the serverless workflow will turn the workflow into a state machine.\\nThe most useful is the AWS Step Functions. If the query states that several\\nsteps are necessary, AWS Step functions are the first functionality to be\\nconsidered."},{"id":"211","question":"An AWS organization with several OUs (Organizational Units)\\nis formed in a major company. The security administrator needs to\\ncreate several CloudWatch Events rules due to several new security\\ncriteria. For policy, it is necessary that an SNS alert is triggered when\\ncertain AWS services are unexpectedly used that were not used by an\\nOU for 6 months prior to this. How would you receive the latest\\nsystem information to identify the possible services that can be\\nincluded in the CloudWatch Event rule?","options":{"A":"In AWS Resource Groups service, add a resource group for the\\ncandidate AWS services. View the last-accessed information\\nfor the group","B":"In the IAM access advisor, view the service-last-accessed\\ninformation for each OU to help identify which services should\\nbe added in the CloudWatch Event rule","C":"Send an AWS CLI command to AWS Config service to get the\\nservice-last-accessed information for each organizational unit","D":"In AWS Organization console, click each OU and view the\\nservice-last-accessed information"},"answer":"B","explanation":"IAM access advisor is able to find the most last-accessed data\\nfor every OU."},{"id":"212","question":"Albert is using AWS CodeBuild service to handle the build task\\nin a CI/CD pipeline. In the pre-build phase of buildspec.yml, there is\\na docker login command such as “docker login –u $USER_NAME –\\np $LOGIN_PASSWORD”. And its user name and password are\\nprovided as variables in the env phase in the same buildspec.yml file.\\nThe credentials may be exposed, which is one of the security issues.\\nWhat is the best way to overcome this issue?","options":{"A":"Store the credentials in a file and put the file in an S3 bucket.\\nEncrypt the S3 bucket via SSE-S3. Modify the buildspec.yml\\nfile to use the encrypted file in the S3 bucket","B":"In the env phase of the buildspec.yml file, use the parameter-\\n\\n\\fstore to specify the user name and password. The values are\\nstored in Systems Manager parameter store","C":"Store the buildspec.yml file in AWS CodeCommit rather than\\nGitHub as IAM rules can be configured in CodeCommit to\\nensure the security","D":"Add a strong IAM rule in AWS CodeBuild to make sure that\\nonly limited users can access the buildspec.yml file"},"answer":"B","explanation":"Systems Manager Parameter store is an ideal place to store the\\nconfidential data. In the buildspec.yml file, you can use the parameter store in\\nenv phase. In form of key-value pair."},{"id":"213","question":"Harrison plans to move his on-premises server to save the cost\\nand for the disaster recovery plan. In order to configure the servers\\nproperly, he plans to use AWS SMS. From the following option,\\nwhich is not a good use case for AWS SMS?","options":{"A":"A data center running in Microsoft System Center Virtual\\nMachine Manager","B":"A Windows10 Pro server running in Microsoft Hyper-V","C":"An Ubuntu16.04 linux server running in VirtualBox","D":"A Java application running in a VMware vSphere Enterprise\\nmachine"},"answer":"C","explanation":"For AWS SMS, Oracle VirtualBox is not supported. Because\\nservers in VMware vSphere, Microsoft Hyper-V / SCVMM, and Azure\\nvirtual machines can use Server Migration Service to duplicate server VMs\\nas AWS AMIs, then EC2 instances can be generated using the AMIs."},{"id":"214","question":"A company has given you the task to configure an AWS Elastic\\nBeanstalk work tier for easy debugging, but you are facing problems\\nin finishing queue jobs, what should you configure?","options":{"A":"Enhanced Health Reporting","B":"Blue-Green Deployments","C":"Rolling Deployments","D":"Dead Letter Queue"},"answer":"D","explanation":"Elastic Beanstalk worker environment supports Amazon SQS\\nqueue service dead letter queues. In the dead letter queue, other queues can\\nsend messages that for some reasons could not be processed. Messages that\\nare unsuccessful in the processing are targeted from source queue to the\\ndead-letter queue. You can gather these types of messages in dead-letter\\nqueues to find the reason for their failure."},{"id":"215","question":"An IT company has various applications with end users around\\nthe globe. Route53 is used to route the traffic. Specific\\nimplementation techniques are applied, depending on the different\\napplication features. Like, Canary is used by some sensitive\\napplications, while Blue/Green is used by others. Many programs\\nalso use Route53 for disaster recovery plan implementation. In order\\nto carry out these strategies, what Route53 routing policy is required?","options":{"A":"Weighted for Blue/Green. Weighted for Canary. Failover for\\nDisaster recovery","B":"Weighted for Blue/Green. Latency for Canary. Failover for\\nDisaster recovery","C":"Simple for Blue/Green. Weighted for Canary. Latency for\\nDisaster recovery","D":"Latency for Blue/Green. Simple for Canary. Weighted for\\nDisaster recovery"},"answer":"A","explanation":"Weighted can be used both for Blue/Green and Canary\\ndeployments as this is an easy way of routing traffic to two stacks as\\nappropriate. Failure to the route can assist in configuring an active-passive\\nfailover that can be used as a disaster recovery technique."},{"id":"216","question":"In order to construct a pipeline for the Docker image and to\\ntransfer it to the AWS ECR, a DevOps engineer is installed in AWS\\nCodePipeline. The codepipeline configures the source stage to be\\n\\n\\fAWS CodeCommit for the source data. The service provider is AWS\\nCodeBuild in the development phase. There are three stage; predesign, build, and post construct stages for its buildspec.yml folder. It\\nis the building stage in the pipeline, which generates the picture and\\ntransfers it to the ECR repository. What command should be put in\\nthe buildspec.yml file in the pre-build stage?","options":{"A":"docker push $REPOSITORY_URI:latest","B":"docker tag $REPOSITORY_URI:latest","C":"aws ecr get-login --region $AWS_DEFAULT_REGION","D":"docker build -t $REPOSITORY_URI:latest"},"answer":"C","explanation":"In the buildspec.yml file, there is collection of build commands\\nand related configuration to run build for AWS CodeBuild. Preparation steps\\nsuch as ecr login, the ECR database URI and tags definition should be used\\nin pre-build phase."},{"id":"217","question":"Jack plans to launch instances that have an application installed\\nwith Auto-scaling. Which of the following methods will help to\\nensure that the instances are up and running for traffic from users in\\nthe shortest possible time?","options":{"A":"Use user data to launch scripts to install the software","B":"Log in to each instance and install the software","C":"Use AMIs, which already have the software installed","D":"Use a Docker container to launch the software"},"answer":"C","explanation":"As you are using the AMI, which already has the required\\nsoftware installed, it will therefore implement the fastest way to launch an\\ninstance. You can configure the public AMI as a custom AMI with your own\\ndefined configuration. The instance launched from that AMI will contain all\\nthe modifications you have made."},{"id":"218","question":"Arthur is working in a company that has recently extended its\\ndata center into a VPC on AWS. The on-premises users are required\\n\\n\\fto manage AWS resources from the AWS console. He is restricted\\nfrom re-creating IAM users. Which of the options below will fit his\\nauthentication needs?","options":{"A":"Use on-premises SAML 2 O-compliant identity provider (IDP)\\nto grant the members federated access to the AWS\\nManagement Console via the AWS single-sign-on (SSO)\\nendpoint","B":"Use on-premises SAML2.0-compliant Identity Provider (IDP)\\nto retrieve temporary security credentials to enable members to\\nsign in to the AWS Management Console","C":"Use Auth 2.0 to retrieve temporary AWS security credentials to\\nenable members to sign in to the AWS Management Console","D":"Use on-premises SAML2.0-compliant Identity Provider (IDP)\\nto retrieve temporary security credentials to enable members to\\nsign in to the AWS management console"},"answer":"A","explanation":"He can use a role to set up your SAML 2.0 IDP and AWS so\\nthat federated users can access the AWS management console. The role\\nempowers the user to perform tasks in the console."},{"id":"219","question":"Charlie is working for an organization that has an on-premises\\ninfrastructure. There is a plan to move to AWS. The arrangement is\\nto move the development environment first. There is a number of\\ncustom based applications that should be deployed for the\\ndevelopment community. Which of the following can actualize the\\napplication for the development group? (Choose 2)","options":{"A":"Create Docker\\ncomponents\\n\\ncontainers\\n\\nfor\\n\\nthe\\n\\ncustom\\n\\napplication","B":"Use OpsWorks to deploy the docker containers","C":"Use CloudFormation to deploy the docker containers","D":"Use Elastic Beanstalk to deploy the docker containers"},"answer":"A and D","explanation":"Elastic Beanstalk allows the use of Docker containers for web\\napplication deployment. You can create your own runtime environment using\\nDocker containers. You can choose your own framework, programming\\nlanguage and any software dependencies not provided by any other platform\\n(for example package managers or tools). Dockers include all configuration\\ninformation and applications that your web application needs to be running is\\nautomatically included."},{"id":"220","question":"Mark works in a company that uses AWS tools. One of the main\\nsafety measures is ensuring encryption of all information both in rest\\nand in transit. Which one is the correct implementation in line with\\nthis policy?","options":{"A":"Enabling sticky sessions on your load balancer","B":"Enabling Proxy Protocol","C":"Using Server-side encryption for data encryption in transit and\\nSSL termination on ELB for data encryption at rest","D":"Using S3 Server-side encryption for data encryption at rest and\\nSSL termination on ELB for data encryption in transit"},"answer":"D","explanation":"Enabling S3 SSE encryption will encrypt data at rest for EBS\\nvolumes and SSL termination allows encrypted traffic between the client and\\nELB. If SSL termination is not enabled, then there is a need to use layer 4 for\\nencryption that is not supported by sticky session.\\nProxy protocol functionality gives you additional insight into your clients\'\\ncommunication data by using TCP load balancing."},{"id":"221","question":"You support the development team to create a new web\\napplication with an Aurora MySQL cluster. The cluster was built\\nwith a Dev/Test model as the specifications were not clear. The DB\\ncluster had only one writer and the read replica was not available.\\nThe Aurora database has recently proven successful and you have\\nbeen directed to configure the DB cluster to make it available. What\\nshould you do to set up the current cluster Aurora?","options":{"A":"Delete the cluster and create a new Aurora cluster with\\n\\n\\fserverless features to make it highly available","B":"In RDS console, add Aurora Replicas to the DB cluster by\\nactions -> add reader","C":"Since an active Aurora cluster cannot configure read replicas,\\ndelete and recreate the Aurora cluster with a Production\\ntemplate","D":"Use AWS CLI to configure cross region replications for the\\nDB cluster to make it highly available"},"answer":"B","explanation":"In order to make Aurora highly available, you can add Aurora\\nreplicas by using the RDS console to the DB cluster. Removing the cluster\\nand recreating is not the proper solution, also cross region replication is not\\nneeded so this option is also incorrect. Serverless features may improve\\ncapacity but availability of both the serverless or provisioned cluster is the\\nsame."},{"id":"222","question":"Martin is building a CloudFormation stack using an AWS\\nCodePipeline. The stack is used in a human approval project to\\ndeploy resources that allow the implementation of a state machine to\\npause for a task. Once the client has approved the project, the process\\nbegins. CloudFormation already has resources including the Lambda\\nfunction that sends an email link for approval, the API Gateway\\nendpoint, which triggers Lambda function, and an SNS subject that\\nemails for approval. What is another important resource to build in\\nthe template?","options":{"A":"An SWF resource in the\\nAWS::SWFFunctions::Activity\\n\\nCloudFormation\\n\\ntype","B":"A state machine resource created in the type of\\nAWS::StepFunctions::Activity","C":"An SWF state machine resource configured in\\nAWS::SWFFunctions::StateMachine","D":"A Step Function resource created in the type\\nAWS::StepFunctions::StateMachine"},"answer":"D","explanation":"In this query, after the client has approved the task the\\nworkflow progresses to the next level. It can be run using a state machine\\nbecause the Step Function state machine as a key tool for obtaining the\\nmanual approval function during the process."},{"id":"223","question":"AWS CodePipeline has been designed to build and implement\\nan AWS Step Feature. The AWS Step Function implements a state\\nmachine for querying a large number of DynamoDB records. Now,\\nafter a new version has already been introduced, your lead has asked\\nyou to add an additional test stage in the present pipeline. In the new\\nphase, which service will you use to begin the test step?","options":{"A":"Use a Lambda function to trigger the Step Function in order to\\ndo the testing","B":"Configure an API endpoint in API Gateway. The API will call\\nStartExecution to start the state machine of the Step Function","C":"Add a CloudWatch Event rule as the action provider in the new\\nstage. Configure the Step Function as the target of the Event\\nrule","D":"Add a Jenkins job to invoke the Step Function to perform any\\ntesting needed"},"answer":"A","explanation":"A custom stage with an action group can be added to\\nCodePipeline. Then the user can adjust to the requirements of the action\\ngroup. There are several methods for the AWS Step Function to trigger the\\nstate machine. During the test phase, the step function of a Lambda function\\ncan be configured to invoke Step Function. To start Step Function, you can\\nuse CloudWatch events but it is not available as action type in Code Pipeline.\\nSimilarly, API Gateway is also not available. While the use of Jenkin will\\nonly create unnecessary complexity."},{"id":"224","question":"While working on the CloudFormation template, you want to\\nuse intrinsic functions to assign values to properties that will not be\\navailable until runtime. Choose the best description to use intrinsic\\nfunctions.","options":{"A":"You can use intrinsic functions only in the resource properties\\npart of a template","B":"You can use intrinsic functions in any part of a template,\\nexcept AWS TemplateFormatVersion and Description","C":"You can use intrinsic functions in any part of a template","D":"You can only use intrinsic functions in a specific part of a\\ntemplate. You can use intrinsic functions in resource\\nproperties, metadata attributes, and update policy attributes"},"answer":"D","explanation":"AWS CloudFormation offers multiple built-in functions for the\\nmanagement of your stacks. To assign values to properties that are not\\navailable until runtime, use the intrinsic functions in your templates.\\nOnly in certain parts of a template can intrinsic functions be used. Currently,\\nintrinsic functions can be used to update policy attributes, outputs, Metadata\\nand resource properties. You can also use intrinsic functions to build stack\\nresources on condition."},{"id":"225","question":"A user has launched an EC2 instance using CloudFormation. He\\nwants that Auto-scaling and ELB stack creation starts after the EC2\\ninstance is launched and configured properly. What should be the\\npossible way of configuration?","options":{"A":"The user can use the WaitCondition resource to hold the\\ncreation of the other dependent resources","B":"The user can use the HoldCondition resource to wait for the\\ncreation of the other dependent resources","C":"It is not possible that the stack creation will wait until one\\nservice is created and launched","D":"The user can use the DependentCondition resource to hold the\\ncreation of the other dependent resources"},"answer":"A","explanation":"You can use the WaitCondition to coordinate stack resource\\ncreation with external configuration actions and to track the configuration\\nprocess status."},{"id":"226","question":"Max was given a mission to build a Jenkins job to do the backup\\nfor a very complicated system. The Jenkins job will activate the\\nbackup function every week. The backup itself consists of a number\\nof steps including running preparations steps, launching DB\\nsnapshots, checking the status of DB snapshots, running post-scripts,\\netc. Every move can be made by a Lambda function. Therefore, the\\nsolution designer expects to see and monitor the operational status of\\nan AWS-owned system for each phase. To achieve this, what service\\nshould Max use?","options":{"A":"AWS EventBridge","B":"AWS CodePipeline","C":"AWS Step Function","D":"AWS SNS"},"answer":"C","explanation":"In this problem, it is necessary to see every phase of the\\nbackup in an AWS system. Step functions may fulfill the need as they have a\\nuser interface to understand the running status of a state machine for every\\nstep. For e.g., like first to submit jobs, you get job status either completed or\\nfailed after a waiting period."},{"id":"227","question":"You were hired for a start-up company as a DevOps engineer.\\nYour company uses AWS for 100% of its infrastructure. Currently,\\nthe deployment is not automated and has experienced many failures\\nwhile trying to deploy to production. The company has told you that\\nthe risk mitigation process is the most important thing now, and you\\nhave enough funds for tools and AWS resources. Depending on the\\ntype, the company stack includes a 2-tier API with data stored in\\nDynamoDB or S3. In Auto-scaling groups, the compute layer is EC2.\\ncompany uses Route53 for DNS to an ELB. A load of an ELB\\nbalance over EC2 instances. The scaling group varies properly from 4\\nto 12 EC2 servers. Which of the following approaches, given the\\nstack of this company and its priorities, best meets the needs of the\\ncompany?","options":{"A":"Model the stack in AWS OpsWorks as a single Stack, with 1\\n\\n\\fcompute layer and its associated ELB. Use Chef and App\\nDeployments to automate Rolling Deployment","B":"Model the stack in three CloudFormation templates: Data layer,\\ncompute layer, and networking layer. Write stack deployment\\nand integration testing automation following Blue-Green\\nmethodologies","C":"Model the stack in AWS Elastic Beanstalk as a single\\nApplication with multiple Environments. Use Elastic\\nBeanstalk’s Rolling Deploy option to progressively roll out\\napplication code changes when promoting across environments","D":"Model the stack in 1 CloudFormation template, to ensure\\nconsistency and dependency graph resolution. Write\\ndeployment and integration testing automation following\\nRolling Deployment methodologies"},"answer":"B","explanation":"You should use blue/green deployment and nested\\nCloudFormation stack for deployment.\\nWhen infrastructure grows, common patterns may arise, in which each of\\nyour templates declares the same components. You can create dedicated\\ntemplates and separate common components. In this way, you can mix and\\nmatch various templates, but use nested stacks to create a single stack. To\\ncreate other stacks with in a stack, one should use nested stacks.\\nAWS::CloudFormation::Stack resource is used in your template to reference\\nother templates to create nested stacks."},{"id":"228","question":"You work for a big company and the department is responsible\\nfor the new CI/CD pipeline to move from the premises to the AWS.\\nThe software uses a license that is limited to the number of vCPUs\\nand the same license was agreed to be used on the AWS platform.\\nThe technology is built by Amazon AMI and several teams including\\nQA and DEV are often using the Jenkins pipeline to launch new EC2\\ninstances. Your team manager worries about the excess of the number\\nof servers used and ask you to add a step in Jenkins to achieve the\\ncurrent license status consumption. What is the best way to do this?","options":{"A":"Create a License Configuration for this license in AWS License\\n\\n\\fManager. Call the CLI list-usage-for-license-configuration to\\nget the license consumption status","B":"Use a DynamoDB table to record the usage status of vCPU.\\nAdd a Jenkins step to read and write the table to manage the\\nlicense consumption status","C":"Create a Lambda to keep counting the number of servers and\\ncompare them with the license limit","D":"Use a shell script to count the total number of EC2 instances in\\nall teams and compare the number with the limit that the\\nlicense allows"},"answer":"A","explanation":"This concern relates to the use of the license. AWS License\\nManager, which simplifies the process of getting software licenses to AWS,\\nshould be the first service to consider. License settings can be generated as\\nvCPU with a license type: the settings can be associated with an AMI and the\\nstatus of the use is then tracked."},{"id":"229","question":"A programmer asks to help set up a new Java project for AWS\\nCodeBuild. Maven is responsible for the design. There are several\\nsteps in its buildspec.yml file such as install, pre-build and build. If a\\ncommand fails in process, CodeBuild still has the opportunity to run\\nsome shell commands, such as log compilation, in order to carry out\\ncustoms operations. How should the CodeBuild project be designed\\nto meet the needs?","options":{"A":"Trigger an SNS notification if a build fails in the CodeBuild\\nproject. Use a Lambda Function to subscribe to the SNS topic\\nand handle the required custom operations","B":"Create a CloudWatch Events rule for any CodeBuild failure\\nevent. Add a Lambda Function as the target to do custom\\noperations","C":"Put the custom operation commands in the post_build phase so\\nthat they can still run when the build fails","D":"Add a finally block after the commands block in each phase.\\nPut the custom shell commands in the finally block"},"answer":"D","explanation":"The buildspec.yml file has provided the final optional block for\\nexecuting commands even if a command fails in the command block to\\novercome this situation."},{"id":"230","question":"An IT company’s technical assistant comes to know that Elastic\\nBeanstalk service provides a managed update facility, which is minor\\nand patches version updates. The company starts hosting a production\\nenvironment in Elastic Beanstalk. The technical assistant comes to\\nyou to ask the effects of an update on the system as the system\\nrequires these updates periodically. What would you tell him about\\nmanaged update facility?","options":{"A":"Elastic Beanstalk applies managed updates with no down time","B":"Elastic Beanstalk applies managed updates with no reduction\\ncapacity","C":"Package updates can be a configurable weekly maintenance\\nwindow","D":"All of the above"},"answer":"D","explanation":"In a configurable weekly maintenance window with Managed\\nPlatform Updates, you can set up your environment to apply minor and patch\\nversion updates automatically. Elastic Beanstalk applies managed updates\\nwithout downtime or reduced capacity and immediately cancels the update if\\nyour application fails health checks of running instances when the new\\nversion executes your application."},{"id":"231","question":"A company is designing a CloudFormation template, which\\ndeploys a LAMP stack. Its users deployed the stack and\\nCREATE_COMPLETE status is showing, but the apache server is\\nstill not up and running and is experiencing issues while starting.\\nNow, the company wants that CREATE_COMPLETE status is\\nshown only when all resources are completely up and running. What\\nshould be done to fulfill this requirement? (Choose 2)","options":{"A":"Use lifecycle hooks to mark the completion of the creation and\\nconfiguration of the underlying resource","B":"Use the CFN helper scripts to signal once the resource\\nconfiguration is complete","C":"Define a stack policy, which defines that all underlying\\nresources should be up and running before showing a status of\\nCREATE_COMPLETE","D":"Use the CreationPolicy to ensure it is associated with the EC2\\nInstance resource"},"answer":"B and D","explanation":"You might specify additional measures to set up the instance,\\nlike installing software packages or Bootstrap applications, for provisioning\\nan Amazon EC2 instance in an AWS CloudFormation stack. Normally, after\\ncreating the instance successfully, CloudFormation proceeds with stack\\ncreation. However, you can use a Creation Policy so that only after your\\nconfiguration actions are done, CloudFormation continues with stack\\ncreation. You will, therefore, know that your apps are ready to go when the\\nstack is successful."},{"id":"232","question":"You are a DevOps Engineer in a multi-national company. In\\norder to start building its resources in AWS, the company wants to\\nuse CloudFormation templates. The templates for different\\ndepartments, such as networking, security, apps, etc. are required.\\nWhat is the best way to develop these templates for CloudFormation?","options":{"A":"Create separate logical templates, for example, a separate\\ntemplate for networking, security, application, etc. Then nest\\nthe relevant templates","B":"Consider using Elastic Beanstalk to create your environments\\nsince CloudFormation is not built for such customization","C":"Consider using OpsWorks to create your environments since\\nCloudFormation is not built for such customization","D":"Use a single CloudFormation template, since this would reduce\\nthe maintenance overhead on the templates itself"},"answer":"A","explanation":"When infrastructure grows, common patterns may arise, in\\nwhich each of your templates declares the same components. You can create\\n\\n\\fdedicated templates and separate common components. In this way, you can\\nmix and match various templates, but use nested stacks to create a single\\nstack. To create other stacks with in a stack, one should use nested stacks.\\nAWS::CloudFormation::Stack resource is used in the template as reference\\nfor other templates to create nested stacks."},{"id":"233","question":"Mike has an application to support thousands of users, therefore,\\nhe has created a DynamoDB table. Every user can access his or her\\nown information only at a particular table is his requirement. Most of\\nthe user’s accounts are with a third- party ID provider, including\\nFacebook, Google or Amazon Login. How would he implement this?\\n(Choose 2)","options":{"A":"By using a third-party identity provider such as Google,\\nFacebook or Amazon so users can become an AWS IAM User\\nwith access to the application","B":"By creating an IAM User for all users so that they can access\\nthe application","C":"By using web identity federation and register your application\\nwith a third-party identity provider such as Google, Amazon, or\\nFacebook","D":"By creating an IAM role, which has specific access to the\\nDynamoDB table"},"answer":"C and D","explanation":"You do not have to create a customized login code or manage\\nyour own user identities with a web identity federation. Instead, application\\nusers can use well- knownidentity provider IDP for sign-in, such as Amazon\\nLogin, Facebook, Google or other OIDC compatible IDPs, and then\\nexchange the received authentication token for AWS temporary security\\ncredentials, that map in an IAM role for the resources access in your AWS\\naccount. Using an IDP helps you maintain your AWS account secure because\\nyour application needs not to integrate and distribute long-term security\\ncredentials."},{"id":"234","question":"Kelvin is designing a CloudFormation stack to create a web\\nserver and a database server. He needs to make sure that the database\\n\\n\\fserver is created before the creation of the webserver database. How\\ncan he do that?","options":{"A":"By ensuring that the database server is defined as a child of the\\nweb server in the CloudFormation template","B":"By ensuring that the database server is defined first and before\\nthe web server in the CloudFormation template. The stack\\ncreation normally goes in order to create the resources","C":"By using the DependsOn attribute to ensure that the database\\nserver is created before the web server","D":"By ensuring that the web server is defined as a child of the\\ndatabase server in the CloudFormation template"},"answer":"C","explanation":"You may specify with the DependsOn attribute that a\\nparticular resource creation follows another. If you want to restrict the\\ncreation of any resource that it should create after the specific resource\\ncreation, then add that resource in the DependsOn attribute."},{"id":"235","question":"You are responsible for designing a number of CloudFormation\\ntemplates. You must change the stack resources sometimes on the\\nbasis of the requirement. How can you monitor the impact of\\nresource change in a CloudFormation stack before stack changes are\\nimplemented?","options":{"A":"By using CloudFormation change sets to check for the impact\\nof the changes","B":"By using CloudFormation Stack Policies to check for the\\nimpact of the changes","C":"There is no way to control this. You need to check for the\\nimpact beforehand","D":"By using CloudFormation Rolling Updates to check for the\\nimpact of the changes"},"answer":"A","explanation":"When you need to update a stack, it helps you to update stacks\\n\\n\\fin confidence if you understand how your changes affect running resources\\nbefore implementing them. Change sets allow you to preview how proposed\\nstack changes can affect your running sources e.g., whether your changes\\nwill delete or replace critical resources, and AWS CloudFormation will only\\nmake the changes to your stack when you decide to run the change set,\\nallowing you to decide whether to follow the proposed changes or find other\\nchanges by creating another change set. The AWS CloudFormation Console,\\nAWS CLI or AWS CloudFormation API allows you to create and manage\\nchange sets."},{"id":"236","question":"Choose 3 answers from the following options, which are true\\nabout the OpsWork Stack Instances.","options":{"A":"You can use instances running on your own hardware","B":"You can start and stop instances manually","C":"A stacks instance can be a combination of both Linux and\\nWindows based operating systems","D":"You can use EC2 Instances that were created outside the\\nboundary of OpsWork\\nAnswer: A, B, and D\\nExplanation: Following are the features of OpsWork:\\nYou can start, stop, or automatically scale the number of\\ninstances by AWS OpsWorks Stacks. With any stack you can\\nuse automatic time-based scaling; Linux stacks can also use\\nload-based scaling\\nYou can also register instances with a Linux stack that has been\\ncreated outside of AWS OpsWorks Stacks in addition to the use\\nof AWS OpsWorks Stacks in Amazon EC2 instances. This\\nincludes EC2 instances and instances on your own hardware.\\nThey have to run one of the Linux distributions that is\\nsupported however, you may not be able to register on-premises\\nWindows instances or Amazon EC2\\nA stack can run Linux or Windows instances. A stack may have\\nvarious Linux versions or distributions on various instances, but\\n\\n\\fLinux and Windows cannot be mixed\\n237. In order to implement the A/B test strategy for a web\\napplication, a DevOps team is using Lambda@Edge. Logic may be\\nadded in the CloudFront CDN by Lambda@Edge to select what\\ncontent to deliver without touching the application code. It is easily\\npossible to test and analyze two versions of app content.\\nThe end user sends the viewer request to CloudFront then it sends the\\norigin request to the origin server. Now the server origin sends the\\nresponse to cache of CloudFront and then sends the viewer response to\\nend user. From the following, which of the following can be configured to\\nadd the logic to choose different versions of applications for A/B testing?\\n(Choose 3)\\nA. CloudFront Cache\\nB. Viewer Response\\nC. Origin Response\\nD. Viewer Request","E":"Origin Request\\nAnswer: C, D, and E\\nExplanation: In order to choose a version of the application, you can\\nconfigure Viewer request: When CloudFront receives a viewer request, it\\nchecks to see if the object in the edge cache is the requested object.\\nOrigin Request: If the object is in the edge cache, then this function does not\\nexecute. It is executed when CloudFront forwards the request to the origin\\nserver.\\nOrigin Response: It will execute when the CloudFront receives a response\\nfrom the origin before caching it to the edge cache. When there is an error\\nfrom the origin, this function still executes. It will not execute in a case when\\nan object is in the cache or response is generated by the function, which was\\ntriggered by an origin request event.\\n238. A company creates a CloudFormation Template that passes user\\ndata to the underlying EC2 Instance. Choose the function that is\\nnormally used in the CloudFormation template to transfer data into\\n\\n\\fthe UserData section.\\nA. \\"UserData\\": {\\n\\n\\"Fn::Ref\\": {\\n\\nB. \\"UserData\\": {\\n\\"Fn::GetAtt\\": {\\nC. \\"UserData\\": {\\n\\"Fn::FindInMap\\": {\\nD. \\"UserData\\": { \\"Fn::Base64\\": {"},"answer":"D","explanation":"The intrinsic Fn::Base64 function returns the input string in\\nBase64 representation. This function is typically used to transfer encoded\\ndata via the UserData property to Amazon EC2 instances."},{"id":"239","question":"You are working in a company where CloudFormation stack\\nresources are creating some problems. Select the options from the\\nfollowing, which will help you in debugging. (Choose)","options":{"A":"Use the AWS CloudFormation console to view the status of\\nyour stack","B":"Use AWSConfig to debug all the API call’s sent by the\\nCloudFormation stack","C":"Use CloudTrail to debug all the API call’s sent by the\\nCloudFormation stack","D":"See the logs in the /var/log directory for Linux instances"},"answer":"A and D","explanation":"You can view a list of stack events in the AWS\\nCloudFormation stack while your stack is being created, updated, or deleted\\nvia console. Select the failure event from this list and then view the status\\nreason for the event. You can see the cloud-init and cfn logs for Amazon EC2\\nproblems. These logs can be found in the /var / log/ directory on the Amazon\\nEC2 instance. These logs record processes and command outputs during\\ninstance setup by AWS CloudFormation. View EC2Configure service and\\ncFN logs for Windows in the%ProgramFiles%\\\\Amazon\\\\EC2ConfigService\\nand C:\\\\cfn\\\\log."},{"id":"240","question":"Kennard has launched multiple instances in different availability\\nzones in an Auto-scaling group as he is running a high traffic\\n\\n\\fapplication. He noticed that one availability zone is not receiving any\\ntraffic. What could be the reason?","options":{"A":"Auto-scaling can be enabled for multi AZ only in North\\nVirginia region","B":"An availability zone is not added to Elastic Load Balancer","C":"Auto-scaling only works in a single region","D":"Instances need to be manually added to the availability zones"},"answer":"B","explanation":"The Elastic Load Balancing creates a load balance node in the\\navailable zone when you add an availability zone to your load balancer. Load\\nbalancer nodes accept customer traffic and forward requests in one or more\\navailability zones to healthy registered instances."},{"id":"241","question":"An organization hired a DevOps engineer who is responsible for\\nan AWS Elastic Beanstalk application. They want to move a\\ncontinuous deployment model, releasing updates to the application\\nmultiple times per day with zero downtime. How will the DevOps\\nengineer do this with immediate roll back to the previous version?","options":{"A":"By creating a second Elastic Beanstalk environment with the\\nnew application version, and configuring the old environment\\nto redirect clients using the HTTP 301 response code to the\\nnew environment","B":"By developing the application to poll for a new application\\nversion in code repository; then downloading and installing it\\nto each running Elastic Beanstalk instance","C":"By creating a second Elastic Beanstalk environment running\\nthe new application version, and swapping the environment\\nCNAMEs","D":"By enabling rolling updates in the Elastic Beanstalk\\nenvironment and setting an appropriate pause time for\\napplication startup"},"answer":"C","explanation":"Due to the fact that Elastic Beanstalk performs an in-place\\nupdate to your application versions, your application may not be available to\\nusers for a short period of time. This down time may be avoided by using a\\nblue/green deployment, in which the new version is deployed to a separate\\nenvironment, and the CNAMEs of both environments can be switched on\\ninstantly to the new version to redirect the traffic."},{"id":"242","question":"Your company makes you responsible for the development of a\\nnumber of cloud templates. During a stack update, you must be\\ncareful that no one can update production-based resources\\naccidentally on the stack. How can this be done most effectively?","options":{"A":"By using S3 bucket policies to protect the resources","B":"By using MFA to protect the resources","C":"By using a Stack based policy to protect the production based\\nresources","D":"By creating tags for the resources and then creating IAM\\npolicies to protect the resources"},"answer":"C","explanation":"All update actions are permitted on all resources when a stack\\nis created. Updating all of the resources on the stack can be used (by default)\\nby the one who has stack update permission. During an update, some\\nresources may need to be interrupted or replaced completely, leading to new\\nphysical identities or entirely new storage. You can avoid unintended updates\\nor removal of stack resources during a stack update via stack policy. A stack\\npolicy is a JSON document defining update actions on designated resources."},{"id":"243","question":"A successful web product has been built in a fintech company\\nusing Node.js with an Elastic Load Balancer instance on EC2\\nc4.xlarge. And the RDS PostgreSQL m5.4xlarge instance is used for\\nits database. In the AWS region ap-southeast-1, all the AWS services\\nare built. The manager of this company needs to learn how to build a\\ndisaster recovery system because this process had not yet been setup.\\nIn order to determine the right DR system services, which questions\\nare to be asked first by the manager? (Choose 2)","options":{"A":"Which type of Elastic Load Balancer does the product use?","B":"Whether another engine type of RDS is required?","C":"What is the budget for the disaster recovery system?","D":"Which instance type of EC2 does it need?","E":"What are the RTO and RPO for the product?"},"answer":"C and E","explanation":"First thing you need to know is the budget because the budget\\naffects the design of the DR system. To achieve the budget target, AWS has\\noffered a number of cost-effective services. The second thing that you need\\nto know is the RTO and RPO. While other options regarding instance type,\\nRDS engine type, and Load balancer type are not important for DR planning."},{"id":"244","question":"An AWS reader app has been developed by a firm. As the\\nnumber of users increases, a search feature for the application is\\nincreasingly necessary. The firm’s team needs this feature to be\\ndeveloped in an independent AWS CodePipeline pipeline as soon as\\npossible. The source stage for the new pipeline is a CodeCommit\\nrepository containing the data in the JSON format used for the search\\nservice. The build phase has been setup by AWS CodeBuild. Which\\nsearch feature is best implemented in AWS?","options":{"A":"Use AWS Lambda to manage an EC2 Elasticsearch application\\nfor the search feature. The build stage is responsible for\\nbuilding the Lambda function","B":"Use AWS CloudSearch to implement this feature. The build\\nstage uses AWS CLI to configure the data in the search domain\\nof CloudSearch","C":"Use AWS EC2 to provision and manage a cluster of servers\\nrunning SOLR applications for the search feature. The build\\nstage is responsible for baking an AMI","D":"Use AWS CloudSearch to implement this search function. The\\nbuild stage builds a CloudFormation template. Add another\\ndeploy stage to deploy the CloudFormation stack for the\\nCloudSearch search domain"},"answer":"B","explanation":"As this points to the need for a solution as soon as possible,\\nAWS CloudSearch is first to be seen as a fully managed AWS service and\\nusers are free to easily set up and manage a website or application search\\nsolution. The AWS CLI aws cloudsearchdomain upload-documents can be\\nused to upload JSON format data."},{"id":"245","question":"Paul has recently developed a new mobile app to handle largescale analytics workloads stored in Amazon Redshift. Access to the\\nAmazon Redshift tables, therefore, is needed. In practical terms and\\nin security terms, which of the following methods would best allow\\nthe tables to be accessed?","options":{"A":"Create an IAM user and generate encryption keys for that user.\\nCreate a policy for RedShift read-only access. Embed the keys\\nin the application","B":"Create a RedShift read-only access policy in IAM and embed\\nthose credentials in the application","C":"Create an HSM client certificate in Redshift and authenticate\\nusing this certificate","D":"Use roles that allow a web identity federated user to assume a\\nrole that allows access to the RedShift table by providing\\ntemporary credentials"},"answer":"D","explanation":"The ideal approach for accessing any AWS service is to use\\nroles. Options A and C are therefore incorrect. You must also use the web\\nidentity federation for any web application. Option B is the correct option.\\nYou should request for AWS services, which must be signed with an AWS\\naccess key while writing such an app. However, it is recommended that you\\ndo not use embedded or long term AWS credentials with apps even in an\\nencrypted store that a user downloads. Instead, build an app that requiresd the\\nAWS temporary security credentials dynamically whenever web identity\\nfederation is used The temporary credentials map to an AWS role and only\\nallow executing the tasks required by the mobile app."},{"id":"246","question":"After the launching of EC2 instance in an Auto-scaling group,\\n\\n\\fyou have implemented a system to dynamically automate\\nconfiguration deployment and application. Your system uses a\\nconfiguration management tool, in which the master node is not\\navailable. This tool works in a standalone configuration. Because of\\nthe application load volatility, new instances should be launched\\nwithin 3 minutes of the launching of instance operating system. The\\nfollowing times are required for the completion of deployment stages:\\nConfiguration management agent installation: 2 minutes\\nThe configuration of an instance using artefacts: 4 minutes\\nApplication framework installation: 15 minutes\\nDeployment of application code: 1 minute\\nHow do you automate deployment with this type of standalone agent\\nconfiguration?","options":{"A":"Build a custom Amazon Machine Image that includes the\\nconfiguration management agent and application framework\\npre-installed.\\nConfigure\\nyour\\nAuto-scaling\\nlaunch\\nconfiguration with an Amazon EC2 UserData script to pull\\nconfiguration artefacts and application code from an Amazon\\nS3 bucket, and then execute the agent to configure the system","B":"Configure your Auto-scaling launch configuration with an\\nAmazon EC2 UserData script to install the agent, pull\\nconfiguration artifacts and application code from an Amazon\\nS3 bucket, and then execute the agent to configure the\\ninfrastructure and application","C":"Build a custom Amazon Machine Image that includes all\\ncomponents pre-installed, including an agent, configuration\\nartefacts, application frameworks, and code. Create a startup\\nscript that executes the agent to configure the system on startup","D":"Create a web service that polls the Amazon EC2 API to check\\nfor new instances that are launched in an Auto-scaling group.\\nWhen it recognizes a new instance, execute a remote script via\\nSSH to install the agent, SCP the configuration artefacts and\\napplication code, and finally execute the agent to configure the\\nsystem"},"answer":"C","explanation":"Since new instances must be installed in 3 minutes, all\\ncomponents should be pre-baked in an AMI as a result. When you attempt to\\nuse the User Data option, it takes time to install and configure various\\ncomponents based on the time mentioned in the question."},{"id":"247","question":"An enterprise maintained a large number of applications in AWS\\nvia CloudFormation stacks. There are many development teams with\\nvarious roles such as Developer, UI, QA, etc. To create, modify and\\ndelete those stacks, different roles should have different access. In\\nhandling all these CloudFormations stacks as a product, the DevOps\\nteam needs a centralized AWS resource. And by allowing access to\\nIAM users and groups, the team can handle the product provisioning.\\nWhich AWS tool should the team use?","options":{"A":"AWS Service Catalog","B":"Trusted Advisor","C":"CloudFormation","D":"Systems Manager"},"answer":"A","explanation":"In AWS Service Catalog, there are some concepts like\\nportfolio and product. A portfolio is the collection of products with\\nconfiguration detail. The product is a service, which is specified by the AWS\\nCloudFormation template to be made available for deployment on AWS for\\nthe user demand."},{"id":"248","question":"A company has started to use AWS Service Catalog to manage\\nits CloudFormation stacks within the company. In the Service\\nCatalog, several portfolios have been created with relevant products\\nconfigured. Membership for these various products should be\\ndistributed to different users and teams. For example, only the\\nDevOps team may build, change, or uninstall the item for an essential\\npayment service. How should the user access be managed in the\\nService Catalog?","options":{"A":"In Service Catalog, use AWS CLI or console to configure ACL\\n\\n\\fpolicies to manage user access","B":"Assign AWS Cognito User Pools to manage access to different\\nproducts","C":"Assign permissions to IAM users, groups, and roles","D":"Make sure the AWS Organization is created. Manage\\nOrganization Service Control Policies in Service Catalog to\\ncontrol the access"},"answer":"C","explanation":"The Service Catalog allows users to access a portfolio that\\nallows them to search the portfolio or launch products within the portfolio.\\nWith IAM permissions you can manage access and assign IAM users, IAM\\ngroups and IAM roles to the portfolio."},{"id":"249","question":"Without redirecting or modifying browser URL, the group wants\\nto test various versions of the page for users. The group does not\\nintend to change any front-end or back-end code with additional logic\\nin order to perform this kind of A/B testing. And the CDN network\\nmust be made aware of this logic. What is the best place for the\\nswitching logic?","options":{"A":"Lambda and API Gateway","B":"Weighted Routing at Route53","C":"Global Accelerator at CloudFront","D":"Lambda@Edge"},"answer":"D","explanation":"Without any need to change or redirect the browser URL,\\nLambda@Edge can provide help to perform A/B testing. So it should be\\nchosen as additional logic to perform A/B testing."},{"id":"250","question":"Philip is creating templates using CloudFormation that has a\\nparameter, which intakes the database password. How can he make\\nsure that if anybody describes the stack, they will not get the\\npassword?","options":{"A":"By using the hidden property for the parameter value","B":"By setting the hidden attribute for the CloudFormation resource","C":"By using the NoEcho property for the parameter value","D":"By using the password attribute for the resource"},"answer":"C","explanation":"Set the NoEcho property to true for sensitive parameter values\\n(e.g., passwords). In this way, the value of your parameter is displayed as\\nasterisks (****), whenever anybody describes your stack."},{"id":"251","question":"An enterprise is using a warm standby DR strategy for its\\napplication with RTO and RPO of 30 minutes. The application\\nconsists of EC2 instance and RDS MySQL. From the following\\noptions, which is helpful to meet these RTO and RPO? (Choose 2)","options":{"A":"AWS Storage Gateway","B":"Route 53","C":"RDS MySQL Serverless Feature","D":"Auto-scaling Group","E":"S3 Glacier"},"answer":"B and D","explanation":"Route53 is an easy, fast and safe solution to help in switching\\nthe traffic to standby when needed. The auto-scaling group is appropriate to\\nmeet the required capacity by adjusting the number of instances."},{"id":"252","question":"A big company handles the source code using AWS\\nCodeCommit. There are over 100 repositories and numerous teams\\nworking on various projects. You need to provide the appropriate\\naccess to different users as a DevOps engineer. For example, the\\ndevelopment team should not access the repositories containing\\nsensitive information. How are you supposed to manage this?","options":{"A":"In IAM, for each IAM user, add an IAM policy that allows or\\ndenies actions on repositories based on repository names","B":"Configure Git tags in AWS CodeCommit repositories. Create\\npolicies in IAM that allow or deny actions on repositories\\nbased on the Git tags","C":"Tag repositories in AWS CodeCommit. Create policies in IAM\\nthat allow or deny actions on repositories based on the tags\\nassociated with repositories","D":"In AWS CodeCommit console, create CodeCommit policies to\\nIAM groups that allow or deny actions on repositories"},"answer":"C","explanation":"In AWS CodeCommit, you have a new feature to tag the\\nrepositories because by using tags, IAM policies can easily be configured for\\ncontrolled access. For example, you define the policy of denying permission\\nto all the resources to perform CodeCommit on the sensitive data."},{"id":"253","question":"John’s company has started using AWS CodeCommit to\\nmaintain the source code. Thousands of developers are present and\\nnew Git repositories are introduced regularly in CodeCommit. The\\nDevOps team was instructed to formulate an approach in the creation\\nof a new repository through the running of a script and after the script\\nis running, it can automatically add setup code, such as model and\\nREADME. What is the best approach to manage everything?","options":{"A":"Put the code into the S3 bucket. Design a CloudFormation\\ntemplate to create a repository with the resource type\\nAWS::CodeCommit::Repository. Add a property to include the\\ncode. Create a CloudFormation stack when needed","B":"CodeCommit does not support this. Use GitHub instead","C":"Design a CloudFormation template to create a repository with\\nthe resource type AWS::CodeCommit::Repository. Design a\\nLambda function to commit the initial code to the new\\nrepository","D":"Design a shell script using Git commands to create a new\\nrepository and submit the init code"},"answer":"A","explanation":"When you are creating a repository with AWS\\nCloudFormation, AWS CodeCommit supports it including the code.\\nCloudFormation stack can be handled easily. A new repository is configured\\nvia stack and in the meantime, the program code is inserted in the repository."},{"id":"254","question":"You are using AWS Elastic Beanstalk to run a social media\\nmarketing application, for which you have written a component in\\nRuby. To support different marketing campaigns, this application\\ncomponent sends messages to social media sites. Your management\\nneeds to record responses to these messages from social media to\\nanalyze the marketing campaign\'s effectiveness compared to past and\\nfuture efforts. A new application component has already been\\ndeveloped for the social media site API to read the replies. Which\\nmethod should you use to record social media responses for\\nanalytical historical data in a sustainable data storage that can be\\naccessed at any time?","options":{"A":"Deploy the new application component as an Elastic Beanstalk\\napplication, read the data from the social media sites, store it in\\nDynamoDB, and use Apache Hive with Amazon Elastic\\nMapReduce for analytics","B":"Deploy the new application component in an Auto-scaling\\ngroup of Amazon EC2 instances, read the data from the social\\nmedia sites, store it with Amazon Elastic Block Store, and use\\nAWS Data Pipeline to publish it to Amazon Kinesis for\\nanalytics","C":"Deploy the new application component in an Auto-scaling\\ngroup of Amazon EC2 instances, read the data from the social\\nmedia sites, store it in Amazon Glacier, and use AWS Data\\nPipeline to publish it to Amazon RedShift for analytics","D":"Deploy the new application component as an Amazon Elastic\\nBeanstalk application, read the data from the social media site,\\nstore it with Amazon Elastic Block store, and use Amazon\\nKinesis to stream the data to Amazon CloudWatch for analytics"},"answer":"A","explanation":"For all applications requiring a consistent single-digit\\nmillisecond latency at every scale, then Amazon DynamoDB is the best data\\nservice. Amazon DynamoDB is a fast and flexible NoSQL database service.\\nIt is a cloud database, which is fully managed and supports key value storage\\nand document models. Due to its flexible data model, reliable performance\\nand automated throughput capacity scaling, the system fits mobile, web,\\n\\n\\fgaming, ad technology, IoT and many other applications greatly."},{"id":"255","question":"A start-up IT firm has just moved its most important web-based\\nsoftware to AWS. A disaster recovery program in AWS must be built\\nas soon as possible, otherwise, the failure may have a huge impact on\\nthe credibility of the business and on the cash flow. The organization\\ndoes, however, have a budget run out and has to review its operating\\ncosts. What tools will help the organization control costs during the\\nimplementation of a disaster recovery system? (Choose 2)","options":{"A":"Use an S3 lifecycle policy to move all stored data in S3 to\\nGlacier after one day to lower down the cost","B":"Use a Trusted Advisor to monitor the EC2 instances that have a\\nlow utilization rate in the standby system. Terminate those\\ninstances to save costs","C":"Use a suitable Auto-scaling Group to control the number of\\nrunning instances","D":"Configure an EBS Lifecycle Policy to delete old EBS\\nsnapshots","E":"Create more spot instances in the hot standby system"},"answer":"C and D","explanation":"In order to reduce costs, ensuring that the process can operate\\nas usual and meet the required RPO and RTO must also be taken into\\naccount.\\nThe Auto-scaling group is helping to reduce the number of operating\\ninstances in order to save costs. EBS Lifecycle Policy promotes effective\\nhandling of EBS snapshots. Cost-saving can be done by removing old\\nimages."},{"id":"256","question":"You maintain an application that serves as a front end and uses\\nMongoDB for document management on an acceptable web server.\\nYou use the user data tab to set up the application to pre-bake AMI\\nwith the newest version of the web server. You now have an\\namendment to the underlying version of the OS and must, therefore,\\ndeploy it. How will you do that as easily as possible?","options":{"A":"By creating a CloudFormation stack with the new AMI and\\nthen deploying the application accordingly","B":"By creating a new pre-baked AMI with the new OS and using\\nthe User Data section to deploy the application","C":"By creating a CloudFormation stack with the new AMI and\\nthen deploying the application accordingly","D":"By creating an Opswork stack with the new AMI and then\\ndeploying the application accordingly"},"answer":"B","explanation":"The ideal way is to continue the same deployment process that\\nwas used previously and create a new AMI and use the user data section in\\norder to deploy the application."},{"id":"257","question":"Your development team uses Elastic Beanstalk. Deploying\\nmultiple versions of your application is your responsibility. How do\\nyou ideally ensure that you do not cross Elastic Beanstalk\'s\\napplication version limit?","options":{"A":"By using lifecycle policies in Elastic Beanstalk","B":"By creating a script to delete the older versions","C":"By creating a Lambda function to delete the older versions","D":"By using AWSConfig to delete the older versions"},"answer":"A","explanation":"Elastic Beanstalk creates an application version whenever you\\nupload your application\'s new version with the Elastic Beanstalk console or\\nEB CLI. You will eventually reach the application version limit and will not\\nbe able to create a new version of that application if you do not delete\\npreviously created versions that you are no longer using. The application\\nversion lifecycle policy can prevent you from reaching the limit. An Elastic\\nBeanstalk lifecycle policy tells Elastic Beanstalk to remove old versions of an\\napp or remove versions of the application if the total number of versions of\\nthe app exceeds the specified number."},{"id":"258","question":"You are working on an application that has three EC2 instances\\nin the Auto-scaling group behind an Elastic Load Balancer. You find\\n\\n\\fthat the Auto-scaling group was updated with a new launch\\nconfiguration that refers to an updated AMI. Your ELB health checks\\nwere successful yet, you still have received complains of error from\\nusers. What will you do to prevent this situation from happening\\nagain?","options":{"A":"Update the launch configuration instead of updating the Autoscaling group","B":"Create a new ELB and attach the Auto-scaling group to the\\nELB","C":"Manually terminate the instances with the older launch\\nconfiguration","D":"Create a new launch configuration with the updated AMI and\\nassociate it with the Auto-scaling group. Increase the size of\\nthe group to six and when instances become healthy, revert to\\nthree"},"answer":"D","explanation":"At a time, one launch configuration is associated with an Autoscaling group, and after creation, you cannot change the launch configuration.\\nYou can use an existing launch configuration as the foundation for a new\\nlaunch configuration then, upgrade the Auto-scaling group to use a new\\nlaunch configuration. After changing the launch setup for an Auto-scaling\\ngroup, any new instances can be launched with the new configuration options\\nwithout affecting the existing instances. Then, to check the launching of new\\ninstances, change Auto-scaling group size to 6 and once the instances are\\ninitiated, turn it back to"},{"id":"3","question":"259. Choose the invalid statement regarding the application\\ndeployment if you want to deploy applications to ELB.","options":{"A":"The application can be bundled in a zip file","B":"The application should not exceed 512 MB in size","C":"The application can include parent directories","D":"The application can be a war file, which can be deployed to the\\napplication server"},"answer":"C","explanation":"If a new application or application version is to be deployed\\nusing the AWS Elastic Beanstalk console, a source package is needed. The\\nfollowing requirements must be met by your source bundle:\\nIt should consist of one ZIP or WAR file (multiple WAR files\\ncan be included within your ZIP file)\\nIt should not exceed 512 MB\\nIt should not include a parent folder or top-level directory\\n(subdirectories are fine)"},{"id":"260","question":"In AWS CodeDeploy, ABC company used a single web\\napplication, which includes EC2, for AWS services. CodeDeploy\\nworks well, but your manager wants to get notifications in time when\\nthe next installation is successful or fails, in the slack channel. You\\nwere told to implement this in AWS CloudWatch events. A Lambda\\nfeature that only covers deployments would be the target of the Event\\nrule. What is the right event pattern for the rule?","options":{"A":"{\\n\\"source\\": [\\n\\"aws.codedeploy\\"\\n],\\n\\"detail-type\\": [\\n\\"CodeDeploy Deployment State-change Notification\\"\\n],\\n\\"detail\\": {\\n\\"state\\": [\\n\\"FAILURE\\",\\n\\"SUCCESS\\"\\n“READY”\\n“START”\\n]\\n}\\n}","B":"{\\n\\"source\\": [\\n\\"aws.codedeploy\\"\\n],\\n\\"detail-type\\": [\\n\\"CodeDeploy Deployment State-change Notification\\"\\n],\\n\\"detail\\": {\\n\\"state\\": [\\n\\"FAILURE\\",\\n\\"SUCCESS\\"\\n]\\n}\\n}","C":"{\\n\\"source\\": [\\n\\"aws.codedeploy\\"\\n],\\n\\"detail-type\\": [\\n\\"AWS API Call via CloudTrail”\\n],\\n\\"detail\\": {\\n\\"state\\": [\\n\\"FAILURE\\",\\n\\"SUCCESS\\"\\n]\\n}\\n}","D":"{\\n\\"source\\": [\\n\\"aws.codedeploy\\"\\n],\\n\\"detail-type\\": [\\n\\n\\f\\"CodeDeploy Deployment State-change Notification\\"\\n],\\n\\"detail\\": {\\n\\"state\\": [\\n\\"FAILURE\\",\\n\\"START\\"\\n]\\n}\\n}"},"answer":"B","explanation":"The CloudWatch Events policy is useful for tracking\\nCodeDeploy status. Because this example needs to track the change in\\ndeployment status, the “detail-type” should be \\"CodeDeploy Deployment\\nState-change Notification\\"."},{"id":"261","question":"Which of the following 3 things would you be able to\\naccomplish with the benefit of the CloudWatch logs?","options":{"A":"Send the log data to AWS Lambda for custom processing or to\\nload into other systems","B":"Record API calls for your AWS account and deliver log files\\ncontaining API calls to your Amazon S3 bucket","C":"Stream the log data to Amazon Kinesis","D":"Stream the log data into Amazon Elasticsearch in near realtime with CloudWatch logs subscriptions\\nAnswer: A, C, and D\\nExplanation: For fast and continuous data intake and aggregation, Amazon\\nKinesis can be used. The data used include IT infrastructure log data,\\napplication logs, social media, feeds for market data, and clickstream data.\\nAmazon Lambda is a web service that can be used to compute logs published\\nby CloudWatch logs without servers.\\nTo deploy, operate, and scale Elasticsearch for log analytics, full text search,\\napplication monitoring, and many more in the simplest way, Amazon\\nElasticsearch Service can be used.\\n262. In your organization, you have set up the following AWS\\n\\n\\fservices: Auto-scaling Group, Elastic Load Balancer, and EC2\\nInstances. If the utilization of CPU is less than 30%, you have to\\nterminate an instance from the Auto-scaling group. How will you do\\nthat?\\nA. By creating a CloudWatch alarm to send a notification to the\\nAuto-scaling group when the aggregated CPU utilization is less\\nthan 30% and configuring the Auto-scaling policy to remove\\none instance\\nB. By creating a CloudWatch alarm to send a notification to SQS.\\nSQS can then remove one instance from the Auto-scaling\\nGroup\\nC. By creating a CloudWatch alarm to send a notification to the\\nadmin team. The admin team can then manually terminate an\\ninstance from the Auto-scaling group\\nD. By creating a CloudWatch alarm to send a notification to the\\nELB. The ELB can then remove one instance from the Autoscaling Group"},"answer":"A","explanation":"You should define two policies, one for scaling in (terminating\\ninstances) and one for scaling out (launching instances) for monitoring each\\nevent. For example, when the network bandwidth reaches a certain level, you\\nwant to scale out and for that purpose, you have to create a policy specifying\\nthat Auto-scaling should start with a certain number of instances to help your\\ntraffic. But, if the network bandwidth level goes down when the network\\nbandwidth level goes back down, you have to define the scale in the policy."},{"id":"263","question":"What is not a supported Elastic Beanstalk service platform?","options":{"A":"PHP","B":"AngularJS","C":".Net","D":"Java"},"answer":"B","explanation":"Following are the supported platforms on Elastic Beanstalk:\\n\\n\\fGo\\nJava SE\\nJava with Tomcat\\n.NET on windows server with IIS\\nNode.js\\nPHP\\nPython\\nRuby\\nPacker Builder\\nSingle container Docker\\nMulticontainer Docker\\nPreconfigured Docker"},{"id":"264","question":"Your team will run a Java project using AWS CodeStar. AWS\\nCodeCommit includes the source code. The build stage is managed\\nwith AWS CodeBuild followed by a stack with resources like the\\nLambda function via the CloudFormation deployment stage. Several\\nteam members, including Jason (owner), Tony (viewer) and Eric\\n(contributor), have been added to the CodeStar project. The various\\nroles in the project should be allowed to different members. How are\\nCodeStar’s permissions managed?","options":{"A":"In CodeStar console, configure the team member roles by\\nassigning different read and write permissions to stages such as\\nbuild or deploy","B":"For different team members, users need to create appropriate\\nIAM policies first and then assign these policies to IAM users","C":"Different team member roles have relevant IAM policies\\nallocated automatically by AWS. CodeStar users only need to\\nmake sure the correct roles are assigned to team members","D":"Users need to create IAM service roles with suitable IAM\\npolicies. Then assign these service roles to different team\\n\\n\\fmembers in CodeStar depending on their roles"},"answer":"C","explanation":"When a project is created, AWS CodeStar will automatically\\ncreate IAM policies for your customers. These rules are used to control levels\\nof access to CodeStar team members. The related IAM policies are\\nautomatically added to the IAM users when users add team members with\\nroles."},{"id":"265","question":"In a project, a Linux-based instance stack in Opswork has been\\ndefined. Furthermore, you want to attach a database to it. Which of\\nthe following is an important step towards ensuring that the Linux\\napplication can communicate with the database?","options":{"A":"Configuring SSL so that the instance can communicate with the\\ndatabase","B":"Configuring database tags for the OpsWork application layer","C":"Adding another stack with the database layer and attaching it to\\nthe application stack","D":"Adding the appropriate driver packages to ensure the\\napplication can work with the database"},"answer":"D","explanation":"For Linux Stacks, you have to add the appropriate driver\\npackage to the associated application layer if you want to associate an\\nAmazon RDS service layer with your application. To do this, follow the\\ngiven steps:\\ni.\\n\\nClick “Layers” in the navigation pane and open the app server\'s\\n“Recipes” tab.\\n\\nii.\\n\\nClick “Edit” and insert OS Packages with the appropriate driver\\npackage. For example, if a layer contains instances of Amazon\\nLinux or mysql-client, you should specify mysql if the layer\\ncontains instances of Ubuntu.\\n\\niii.\\n\\nSave changes and redeploy the app."},{"id":"266","question":"You have been tasked to build a dashboard for security control\\n\\n\\fin AWS. The dashboard should be able to determine if EC2 instances\\nare vulnerable and exposed (CVEs). This incident should be detected,\\nfor example, when an EC2 instance does not download a particular\\npatch and is exposed to a known CVE. What is the best approach to\\ndo so?","options":{"A":"Configure AWS Macie and include CVE rule package in the\\nassessment template","B":"Enable AWS Inspector and make sure all EC2 instances have\\nthe Inspector agents installed properly. Include the CVE rule\\npackage in the assessment template","C":"In AWS Systems Manager, include CVE patches in patch\\nbaselines. Use patch manager to apply system patches to all\\nEC2 instances","D":"Enable AWS GuardDuty and include the CVE rule package in\\nthe GuardDuty template. Monitor CVE findings in the console"},"answer":"B","explanation":"One of the rule packages that AWS Inspector can configure is\\ncommon faults and exposures (CVEs). & nbsp; AWS Inspector can detect if\\nEC2 instances are exposed to CVE after configuring the Common\\nVulnerabilities and Exposures (CVE) rule packages on the evaluation\\ntemplate."},{"id":"267","question":"Your company designed an application to transfer all user logs\\nto a Kinesis channel, and those logs will live for 24 hours in the\\nstreams. A recent strategy of Splunk Enterprise is to scan, track, and\\nreview request logs. Also, on an EC2 instance, the Splunk Server has\\nbeen deployed. What is the best approach to loading streaming data\\ninto the Splunk instance on the Kinesis Stream?","options":{"A":"Use AWS SDK in an EC2 instance to get the records from\\nKinesis Stream and forward the records to the Splunk instance","B":"Use Amazon Kinesis analytics to analyze and transfer real-time\\nstreaming data in Kinesis Stream to the destination, which is\\nthe Splunk instance","C":"Use Amazon Kinesis Data Firehose as a fully managed service\\nto deliver real-time streaming data in Kinesis Stream to the\\nSplunk instance","D":"Configure the Kinesis Stream to auto deliver the received logs\\nto the Splunk destination for the server to index the logs"},"answer":"C","explanation":"Amazon Kinesis Data Firehose is an appropriate data\\nconsumer for the data supplier, in this case, the Kinesis Stream. Kinesis\\nFirehose is used to send logs to destinations such as Amazon S3, Amazon\\nRedshift, Splunk, and Amazon ES."},{"id":"268","question":"You are creating a DynamoDB application for the storage of\\nJSON data. The read and write capability of the DynamoDB table has\\nalready been set. The amount of traffic received during the\\ndeployment period by the application is not known. Your IT officer\\nasks you to ensure that DynamoDB is not throttled and is not an\\napplication bottleneck. What steps should you take for this? (Choose\\n2)","options":{"A":"Monitor the SystemErrors metric using CloudWatch","B":"Create a CloudWatch alarm, which would then send a trigger to\\nAWS Lambda to increase the Read and Write capacity of the\\nDynamoDB table","C":"Create a CloudWatch alarm, which would then send a trigger to\\nAWS Lambda to create a new DynamoDB table","D":"Monitor\\nthe\\nConsumedReadCapacityUnits\\nand\\nConsumedWriteCapacityUnits metric using CloudWatch"},"answer":"B and D","explanation":"ConsumedReadCapacityUnits\\nand\\nConsumedWriteCapacityUnits over the specified time period can monitor for\\na DynamoDB table to track the usage of your provisioned throughput."},{"id":"269","question":"You are responsible for an application, which is using EC2,\\nELB, and Auto-scaling. The ELB access logs were requested by your\\nmanager. You do not find any log in the S3 bucket. Why is that\\n\\n\\fhappening?","options":{"A":"The Auto-scaling service is not sending the required logs to\\nELB","B":"You do not have the necessary access to the logs generated by\\nELB","C":"The EC2 Instances are not sending the required logs to ELB","D":"By default, ELB access logs are disabled"},"answer":"D","explanation":"Access logging is by default disabled. It is an Elastic Load\\nBalancing feature. Once the load balancer logs are enabled, Elastic Load\\nBalancing captures the logs and keeps them in a specified Amazon S3 bucket.\\nThe access logging can be disabled at any time."},{"id":"270","question":"For several years, an organization has been using the Jenkins\\ndatabase on site as a CI/CD tool. Most of its services have recently\\nmoved to AWS. AWS CodePipeline will be used as the new tool in\\norder to replace the Jenkins database by the DevOps team. Initial\\nresearch and reporting were given to the CEO. What are\\nCodePipeline\'s strengths relative to that of the Jenkins server?\\n(Choose 2)","options":{"A":"IAM policies can be configured to control the access to\\nCodePipeline resources","B":"Similar to Jenkins, CodePipeline also provides a large number\\nof plugins in the AWS marketplace","C":"AWS CodePipeline is totally free and you only need to pay\\nrelated resources generated in the pipeline such as EC2","D":"AWS CodePipeline integrates well with other AWS services\\nsuch as CodeCommit and CodeBuild. It is easily configured for\\nusers of the AWS ecosystem","E":"AWS CodePipeline is open-sourced just as Jenkins so that\\nusers can contribute to the community"},"answer":"A and D","explanation":"You can use AWS CodePipeline with various other services\\nbecause it will integrate easily with them and also be easily configured. In\\norder to manage the resource access in the CodePipeline, IAM policies can\\nbe used for users, groups, and roles."},{"id":"271","question":"Which of the following can be used to monitor whether the\\nchanges made to your AWS resources are reliable and durable\\nlogging solutions?","options":{"A":"Create a new CloudTrail with one new S3 bucket to store the\\nlogs. Configure SNS to send log file delivery notifications to\\nyour management system. Use IAM roles and S3 bucket\\npolicies on the S3 bucket that stores your logs","B":"Create three new CloudTrail trails with three new S3 buckets to\\nstore the logs one for the AWS Management console, one for\\nAWS SDKs and one for command line tools. Use IAM roles\\nand S3 bucket policies on the S3 buckets that store your logs","C":"Create a new CloudTrail trail with an existing S3 bucket to\\nstore the logs and with the global services option selected. Use\\nS3 ACLs, and Multi Factor Authentication (MFA) Delete on\\nthe S3 bucket that stores your logs","D":"Create a new CloudTrail trail with one new S3 bucket to store\\nthe logs and with the global services option selected. Use IAM\\nroles S3 bucket policies and Multi Factor Authentication\\n(MFA) Delete on the S3 bucket that stores your logs"},"answer":"D","explanation":"AWS CloudTrail service is integrated with AWS Identity and\\nAccess Management (IAM). It is a service that logs AWS events from or on\\nbehalf of your AWS account. CloudTrail logs AWS authenticated API calls\\nas well as AWS sign-in events and collects information of this event in the\\nfiles that are supplied to Amazon S3 buckets. You must make sure all\\nservices are included. Therefore, option C is partly correct.\\nOptions A and B are incorrect because having three S3 buckets and SNS\\nnotifications just adds overhead."},{"id":"272","question":"A crew is working on the development of a new AWS EC2 web\\n\\n\\fapplication. Atlassian JIRA Software is used for many existing\\nprojects within the company. The team is expected to work with a\\nsingle project management portal where information is available such\\nas JIRA tales, on-going implementation, system endpoints, etc. for\\nthis new project. How can the team meet the expectations?","options":{"A":"Create an AWS CodeStar project. Customize the project\\ndashboard as required with a link to the JIRA server URL","B":"Create an AWS CodeStar project. Configure the connection to\\nJIRA in the CodeStar project. Customize the project dashboard\\nas required","C":"Use AWS CodePipeline to manage the source, build and\\ndeployment. Add a stage with the action provider as Jenkins. In\\nthe Jenkins server, configure a JIRA plugin to integrate with\\nAtlassian JIRA","D":"Use the CloudFormation stack to create CloudWatch\\ndashboards to manage the project including the JIRA URL"},"answer":"B","explanation":"AWS CodeStar has provided compatibility with the Atlassian\\nJIRA framework, which allows the CodeStar dashboard to easily integrate\\nissue tracking and project management tools that the JIRA provides. After\\nJIRA has been incorporated, all JIRA project tickets can be easily seen by\\nCodeStar users. In the dashboard, you can find JIRA information."},{"id":"273","question":"A large company has a massive quantity of data in AWS S3.\\nThese S3 buckets include the application’s read or write data. The\\nsafety auditor was concerned that some sensitive information could\\nbe revealed in S3. Some programs, for example, can store some text\\nfiles containing PII data for customers. In these S3 buckets, the\\nauditor asked for a solution to quickly scan potential security issues.\\nWhat is the best solution?","options":{"A":"Configure Amazon Athena in S3 and create Athena SQL\\ntables. Query security issues by using SQL commands","B":"Enable AWS GuardDuty as it can analyze all the application\\n\\n\\fdata in S3 and generate security findings","C":"Configure AWS Inspector in S3. It is able to use machine\\nlearning to search for security issues in S3 and provide\\nCloudWatch alarms to the admin users","D":"Enable Amazon Macie as it can scan security issues in S3 and\\ngenerate alerts based on the level of risks"},"answer":"D","explanation":"When you are concerned about the PII information security in\\nAmazon S3, always think about Macie. After enabling it, S3 objects can be\\nscanned by using PII priority."},{"id":"274","question":"You have an OpsWork stack on Linux instances. Your recipe\\nexecution failed. How can you diagnose the reason for failure?","options":{"A":"Bu logging into the instance and checking if the recipe was\\nproperly configured","B":"By de-registering the instance and checking the EC2 Logs","C":"By using AWS CloudTrail and checking the Opswork logs to\\ndiagnose the error","D":"By using AWS Config and checking the Opswork logs to\\ndiagnose the error"},"answer":"A","explanation":"Failure to use a recipe will lead to the instance setup failed\\nstate rather than online. While the instance of AWS OpsWorks Stacks is not\\nonline, it is often useful to login to resolve the matter in EC"},{"id":"2","question":"The EC2\\ninstance is running. You can check whether an application or a personalized\\ncookbook is installed correctly. The AWS OpsWork Stacks built-in support\\nfor SSH and RDP login is available only for the online state instances.\\n275. AWS platform is used by the company to host the bulk of its\\napplications and services. You have handled a great many AWS\\nassets as an AWS administrator to meet business needs. In many\\nAWS regions and accounts, you sometimes need to build, upgrade or\\ndelete similar resources. For example, to decide whether CloudTrail\\nis allowed on all accounts, you have to set up an AWS Config rule.\\n\\n\\fWhat is the safest way to implement these settings in multiple regions\\nor on account of single operations?","options":{"A":"Create CloudFormation StackSets using templates. Specify\\nregions and accounts depending on the requirements for the\\nStackSets","B":"Configure an AWS CodePipeline to deploy AWS resources in\\na deployment stage via CodeDeploy. Execute the pipeline in\\ndifferent regions and accounts","C":"Create Lambda functions to use the AWS SDK to create AWS\\nresources. Run the Lambda functions in different regions and\\naccounts","D":"Prepare CloudFormation templates and create CloudFormation\\nstacks using the templates in various regions and accounts"},"answer":"A","explanation":"The key question is that AWS infrastructure should be set up\\nor managed in a single operation across different regions and accounts.\\nCloudFormation StackSets must first be considered as CloudFormation\\nstacks can be easily supplied to selected target accounts in specified regions.\\nFor a single CloudFormation StackSet, regions, and accounts can be picked."},{"id":"276","question":"The Kinesis Stream was used by a financial firm to store\\nprocessed logs from a busy application in real time. The data is then\\nsent to a Kinesis Firehose distribution system that provides\\ninformation to the final destination of the S3 container. The data\\ninput format is RFC3163 Syslog. Before the data is delivered, the\\nformat must be converted to JSON in Kinesis Firehose. How will this\\nbe done?","options":{"A":"Kinesis Firehose cannot transform the data format inside of it.\\nInstead, it has to be done in Kinesis Stream","B":"Configure Kinesis Data Firehose to use third-party JSON\\ndeserializer tool Apache Hive JSON SerDe to convert the data\\nto JSON format","C":"Create a Lambda function for data transformation using a\\n\\n\\fblueprint. Kinesis Data Firehose can invoke the Lambda\\nfunction to transform incoming source data","D":"In Kinesis Data Firehose, invoke AWS Glue to create a schema\\nin order to modify the format of incoming source data"},"answer":"C","explanation":"In order to convert the data format in Kinesis Firehose, you can\\nuse a blueprint that converts the syslog data into the JSON format."},{"id":"277","question":"The DevOps group assesses AWS CodeBuild to figure out if it is\\nsuitable for developing new software. Different objects or\\nenvironments are needed to build artifacts for these applications. The\\nenvironmental image, for which the construction project is to run\\nmust be selected in AWS CodeBuild. From the given options, what\\nare good CodeBuild environment images? (Choose 2)","options":{"A":"A Centos Linux image from an AMI created by the user","B":"A custom docker image, which is hosted in an external Docker\\nregistry","C":"A Suse Linux image managed by the user","D":"A RedHat Linux image managed by AWS CodeBuild","E":"A Windows server image managed by AWS CodeBuild"},"answer":"B and E","explanation":"For CodeBuild build environments, there are two types of\\nenvironments: custom or managed images. Instead of AMI, Docker image\\nshould be used. RedHat Linux image is not supported and custom image\\nshould be a docker image. While Windows server image is supported."},{"id":"278","question":"Your organization owns several AWS accounts. The AWS\\noperation team creates several base docker images in AWS ECR.\\nAnother development team is working on a new project, in which the\\nbuild phase needs to use AWS CodeBuild to build artifacts. One\\nrequirement is that the environment image of CodeBuild must use an\\nECR docker image owned by the operation team. However, the ECR\\ndocker image is located in a different AWS account. How would you\\nresolve this and create the CodeBuild project?","options":{"A":"Select custom image and choose another registry for an\\nexternal Docker registry. In external registry URI, enter the\\nECR repository URI of the other account","B":"Select the custom image and choose the ECR image registry.\\nEnter the full ECR repository URI for the repository in the\\nother account","C":"CodeBuild does not support cross account ECR images. Copy\\nthe image to the ECR registry in the same account first","D":"Select AWS managed Ubuntu image. In the image, pull the\\nECR docker image from another account"},"answer":"B","explanation":"AWS CodeBuild supports accessing cross-account ECR\\nimages. In the console of AWS CodeBuild, you can select the ECR registry\\nand then select the account."},{"id":"279","question":"Robert’s team has created a new AWS CodeDeploy software to\\nsimplify Amazon EC2 deployments under auto-scaling. A\\ndeployment operation may have failed because of software problems.\\nIn the production environment, your manager asked you to\\nautomatically setup a rollback to the deployment group. What\\nstatement is correct when it comes to the automatic rollback setup of\\nCodeDeploy?","options":{"A":"Automatic rollback can only be configured for EC2 instances.\\nOn-premises instances do not have this feature","B":"Users can configure SNS notifications for deployment\\nactivities. Rollback can be triggered whenever SNS topics\\nreceive notifications","C":"Automatic rollback can be triggered when alarm thresholds are\\nmet","D":"There is no automatic rollback for the CodeDeploy deployment\\ngroup. However, users can trigger rollback manually"},"answer":"C","explanation":"By configuring automatic rollback in AWS CodeDeploy, it can\\n\\n\\fbe triggered in case of deployment fails or if a certain threshold is met."},{"id":"280","question":"A multi-level architecture is being operated on AWS with the\\nNginx web server instances. The application is having bugs when\\noperated by users. How can you quickly and effectively identify those\\nerrors?","options":{"A":"Send all the errors to AWS Lambda for processing","B":"Send all the errors to AWS Config for processing","C":"Install the CloudWatch Logs agent and send Nginx access log\\ndata to CloudWatch. From there, pipe the log data through to a\\nthird party logging and graphing tool","D":"Install the CloudWatch Logs agent and send Nginx access log\\ndata to CloudWatch. Then, filter the log streams for searching\\nthe relevant errors"},"answer":"D","explanation":"For searching and matching terms, phrases or values in your\\nlog events, you can use metric filters. In your log events, you can increase the\\nvalue of a CloudWatch metric when a metric filter finds a term, phrase or\\nvalues. For example, to scan and count the occurrence of the word ERROR in\\nyour log events, a metric filter can be created."},{"id":"281","question":"A new project to build a pipeline for a new Android app was\\nassigned to you in the AWS CodePipeline operation. The source\\nstage of the pipeline is GitHub and CodeBuild is used for building the\\napp. This uses a buildspec file to create objects that are stored in an\\nS3 bucket during the construction phase. To test the new version of\\nthe App in AWS Device Farm, a further step needs to be added. The\\nQA team has already developed a test project in AWS Server Farm.\\nIn AWS CodePipeline, how should you configure this new stage?","options":{"A":"Add a new test stage and add AWS Device Farm as the action\\nprovider. Configure the AWS Device Farm project ID and\\ndevice pool ARN in the stage","B":"Add a new stage and add an SNS topic as the action provider.\\n\\n\\fThe SNS topic will trigger the AWS Device Farm project to\\nexecute the test","C":"Add a new deploy stage and add AWS CloudFormation as the\\naction provider","D":"The CloudFormation stack will create and initiate the AWS\\nDevice Farm project","E":"Add a new stage and add an action provider using a Lambda\\nfunction. The Lambda function is responsible to trigger the\\nAWS Device Farm project"},"answer":"A","explanation":"You can create a continuous integration flow using the AWS\\nCodePipeline, in which your software will be built and tested every time you\\nmove a commit into your repository.\\nAdd a new test phase and include the service provider AWS Server Farm.\\nConfigure the AWS Device Farm project ID and device pool ARN in the\\nstage."},{"id":"282","question":"As a DevOps Engineers, you have to host a custom application\\nwith custom dependencies for a development team by using AWS\\nservice. From the following options, choose the perfect way to\\nperform your task.","options":{"A":"Package the application and dependencies with Docker, and\\ndeploy the Docker container with Elastic Beanstalk","B":"Package the application and dependencies with in Elastic\\nBeanstalk, and deploy with Elastic Beanstalk","C":"Package the application and dependencies with Docker, and\\ndeploy the Docker container with CloudFormation","D":"Package the application and dependencies in an S3 file, and\\ndeploy the Docker container with Elastic Beanstalk"},"answer":"A","explanation":"The deployment of a web application from Docker containers\\nis supported by Elastic Beanstalk. You can set your own runtime\\nenvironment with Docker containers. You can choose a platform,\\nprogramming language, and any application dependencies that are not\\n\\n\\fsupported by other platforms, such as package managers or tools. Docker\\ncontainers are autonomous and contain all the configuration information and\\nsoftware needed to run your web application."},{"id":"283","question":"Raffaele has heavy AWS users and possesses many AWS\\nresources such as EC2, S3, RDS, etc. in his company. Now he is\\nrequired to develop an ongoing monitoring service in AWS to track\\nservices from the safety standpoint. For example, if an EC2 instance\\nis compromised and used to carry out a Denial of Service (DoS)\\nattack using the UDP protocol, the service should be able to identify\\npotential risks. What should Raffaele do?","options":{"A":"Activate VPC Flow Logs, AWS CloudTrail event logs, and\\nDNS logs and transfer the logs to a dedicated S3 bucket.\\nConfigure Athena to query the logs to identify potential\\nsecurity problems","B":"Enable AWS Enterprise support plan and activate full features\\nof Trusted Advisor, which can quickly provide alarms for\\nsecurity related issues","C":"Enable AWS Macie to continuously scan AWS security risks in\\nresources such as EC2. It can identify potential issues and\\nprovide alarms such as if an EC2 instance is compromised","D":"Enable AWS Macie to continuously scan AWS security risks in\\nresources such as EC2. It can identify potential issues and\\nprovide alarms such as if an EC2 instance is compromised"},"answer":"D","explanation":"Amazon GuardDuty is a security service capable of constantly\\ntracking and analyzing the information assets including VPC stream logs,\\nAWS CloudTrail event logs, and DNS logs. Like a high-risk alarm found by\\nGuardDuty when an EC2 instance may be used to attack Denial of Service\\nusing the UDP Protocol on TCP port."},{"id":"284","question":"In his AWS EC2 instances, Raymond recently experienced an IT\\nsecurity incident. An offender used an EC2 penetration testing tool\\nfrom Kali Linux, found weaknesses in the EC2 configurations, and\\ngained unauthorized access to the company\'s resources. To ensure\\n\\n\\fthat all EC2 cases are properly patched, Raymond needs to develop a\\nplan. For such potential security risks, a monitoring tool is also\\nneeded. How should he work together with his team to fulfill the\\nrequirements? (Choose 2)","options":{"A":"Configure monitoring dashboard in AWS QuickSight, which\\nuses machine learning skills to discover security incidents that\\nare happening","B":"Use AWS Systems Manager Run Command to apply necessary\\npatches every 30 days to ensure all EC2 instances are always\\npatched compliant","C":"Enable AWS GuardDuty to monitor potential security\\nincidents. Create CloudWatch Event rules based on the\\nfindings and trigger SNS notifications","D":"Configure patch baselines in AWS Systems Manager and use\\nPatch Manager to apply patches in a maintenance window","E":"Configure AWS Macie to continuously monitor security issues\\nfor AWS resources. Configure SNS notifications based on\\nMacie alarms in CloudWatch Events"},"answer":"C and D","explanation":"In EC2 instances, patches can easily be applied according to\\ndefined patch baselines by the patch manager. Using AWS GuardDuty to\\ntrack the security problems listed above is the best solution. PenTest:\\nIAMUser / KaliLinux is a finding type of GuardDuty. When a computer\\noperating Kali Linux makes API calls with your AWS Account credentials,\\nGuardDuty will report this threat."},{"id":"285","question":"Rex has several AWS CloudFormation StackSets. A StackSet\\nhas been developed in several regions to set up web application\\nnetwork resources, such as IAM roles and Security Groups. A\\nparameter value of CloudFormation StackSets must be modified\\nbecause of some new features in the program. Nevertheless, only two\\nregions need this modification and the parameter for the other regions\\nshould not be modified. How can he do this?","options":{"A":"Choose Override StackSet parameters from the Actions menu.\\n\\n\\fSpecify the two regions that he wants to modify and then\\noverride the StackSet parameters for these regions","B":"Specify the two regions that he wants to change and then\\nmodify the StackSet parameters for only these two regions","C":"Parameters of StackSets cannot be modified for selected\\nregions. Deregister these two regions from the StackSets and\\nthen create a new StackSet with new parameters in these two\\nregions","D":"Parameters of StackSets can only be modified for all regions.\\nThis operation cannot be done unless the CloudFormation\\nstacks in these two regions are removed from the StackSets"},"answer":"A","explanation":"In this case, in two regions, stack instances need to have\\nproperty values different from the one specified when StackSets are created.\\nThe parameters for current stack instances can be overridden. First, select the\\nStackSets and choose to override StackSet parameter then specify the\\naccount and region. Edit the required parameter value."},{"id":"286","question":"Your application is running behind a load balancer on Amazon\\nEC2 instances. Your company has decided to use a strategy for the\\nblue/green deployment. How do you do this for every deployment?","options":{"A":"Launch more Amazon EC2 instances to ensure high\\navailability, de-register each Amazon EC2 instance from the\\nload balancer, upgrade it, and test it, and then register it again\\nwith the load balancer","B":"Set up Amazon Route53 health checks to fail over from any\\nAmazon EC2 instance that is currently being deployed to","C":"Using AWS CloudFormation, create a test stack for validating\\nthe code, and then deploy the code to each production Amazon\\nEC2 instance","D":"Create a new load balancer with new Amazon EC2 instances,\\ncarry out the deployment, and then switch DNS over to the new\\nload balancer using Amazon Route53 after testing"},"answer":"D","explanation":"To do this you must:\\nFirstly, create a new ELB to show new changes in production\\nFor the distribution of traffic to the 2 ELB based on an 80- 20%\\ntraffic scenario, use the Weighted Route Policy for Route"},{"id":"53","question":"This\\nis the normal scenario, according to the requirement, the\\npercentage can be changed\\nFinally, if all modifications have been tested, Route53 can be set\\nto 100% for the new ELB\\nOption A is incorrect, as the deployment scenario is not blue green. You are\\nnot able to control the users for a new EC2 instance.\\nOption B is incorrect because this is not a Blue Green Deployment Failure\\nscenario. You need to have two environments working side by side in Blue\\nGreen deployments.\\nOption C is wrong because the changes will run side by side with a\\nproduction stack.\\n287. The team is creating an online bid program that has been used to\\nstore information for customers in DynamoDB tables. For read, a\\nresponse time of microseconds is needed. The latest DynamoDB\\ntables do not seem to provide this efficiency however. What is the\\nbest approach to greatly improve the reading output without altering\\ncurrent program logic?","options":{"A":"Configure a DynamoDB Accelerator (DAX) cluster for the\\napplication to use which can deliver up to a 10x performance\\nimprovement","B":"Create a read replica table in another region (Global Table) to\\nimprove the read capacity","C":"Create a Global Secondary Index for the DynamoDB table so\\nthat queries can be more efficient","D":"Configure an on-demand read/write capacity mode for the\\nDynamoDB table"},"answer":"A","explanation":"DAX is a microsecond latency DynamoDB service for access\\nto eventually consistent data. DAX is a fully managed, highly available inmemory cache that boosts DynamoDB performance x"},{"id":"10","question":"However, because\\nDAX is compatible with existing DynamoDB API calls, no alteration in\\napplication code is required.\\n288. You decided to change the instance type of your production\\ninstances that run in the Auto-scaling group. To launch your\\narchitecture, you used the CloudFormation template and currently\\nused 4 instances in production. The service cannot be interrupted\\ntherefore two instances should always run during the update. Which\\nof the following options can be applicable?","options":{"A":"AutoScalingReplacingUpdate","B":"AutoScalingScheduledAction","C":"AutoScalingRollingUpdate","D":"AutoScalingIntegrationUpdate"},"answer":"C","explanation":"The AWS::AutoScaling::AutoScalingGroup resource supports\\nan UpdatePolicy attribute, which defines how an Auto-scaling group resource\\nis updated when an update to the CloudFormation stack occurs. A common\\napproach is executed rolling update for updating an Auto-scaling group by\\ndefining the AutoScalingRollingUpdate policy. This keeps the same Autoscaling group and, according to the indicated parameters, replaces old\\ninstances with new ones."},{"id":"289","question":"You configure the Continuous Integration (CI) system to create\\nAMIs in your deployment process. You want to build them in a costeffective, automated way. What method are you supposed to use?","options":{"A":"Upload all contents of the image to Amazon S3 launch the base\\ninstance, download all of the contents from Amazon S3 and\\ncreate the AMI","B":"Have the CI system launch a new spot instance bootstrap the\\ncode and apps onto the instance and create an AMI out of it","C":"Have the CI system launch a new instance, bootstrap the code\\n\\n\\fand apps onto the instance and create an AMI out of it","D":"Attach an Amazon EBS volume to your CI instance, build the\\nroot file system of your image on the volume, and use the\\nCreateImage API call to create an AMI out of this volume"},"answer":"B","explanation":"You can add Automation as a post-build step to pre-install\\napplication releases to Amazon Machine Images (AMI) if you use Jenkins\\nsoftware within a CI/CD pipeline. The Jenkins scheduling function can also\\nbe applied to call Automation and create your own OS patching cadence."},{"id":"290","question":"A website is running in a virtual private cloud, using a load\\nbalancer and an Auto-scaling group. Your Head of Security has asked\\nyou to set up a system of monitoring that will rapidly detect and\\nnotify your team when there is a sudden increase in traffic. How\\nwould you configure that?","options":{"A":"Set up an Amazon CloudWatch alarm for the Elastic Load\\nBalancing NetworkIn metric and then use Amazon SNS to alert\\nyour team","B":"Set up a cron job to actively monitor the AWS CloudTrail logs\\nfor increased traffic and use Amazon SNS to alert your team","C":"Use an Amazon EMR job to run every thirty minutes analyze\\nthe CloudWatch logs from your application Amazon EC2\\ninstances in a batch manner to detect a sharp increase in traffic\\nand then use the Amazon SNS SMS notification to alert your\\nteam","D":"Use an Amazon EMR job to run every thirty minutes, analyze\\nthe Elastic Load Balancing access logs in a batch manner to\\ndetect a sharp increase in traffic and then use the Amazon\\nSimple Email Service to alert your team","E":"Set up a cron job to actively monitor the AWS CloudTrail logs\\nfor increased traffic and use Amazon SNS to alert your team"},"answer":"A","explanation":"NetworkIn Metric: The number of bytes received by each\\ninstance on all network interfaces. The metric shows the amount of input\\n\\n\\ftraffic of the network on a particular instance. The number of bytes received\\nduring the period is the number of bytes reported. This number can be\\ndivided by 300 bytes per second if you are using basic monitoring. Divide it\\nby 60, if you have detailed monitoring."},{"id":"291","question":"Amazon SQS and Auto-scaling are used by the program to\\nhandle background work. The Auto-scaling policy is based on the\\namount and maximum instance count of 100 messaging in the queue.\\nThe category has never increased beyond 50 since the application\\nwas launched. The Auto-scaling group is now 100, the queue size is\\ngrowing and there are very few jobs that are completed. The number\\nof messages sent to the queue is regular. What should you do to\\nidentify why the queue size is unusually high and reduce it?","options":{"A":"Analyze the application logs to identify possible reasons for\\nmessage processing failure and resolve the cause for failures","B":"Temporarily increase the AutoScaling group’s desired value to\\n200. When the queue size has been reduced, reduce it to 50","C":"Analyze CloudTrail logs for Amazon SQS to ensure that the\\ninstances Amazon EC2 role have permission to receive\\nmessages from the queue","D":"Create additional Auto Scaling groups enabling the processing\\nof the queue to be performed in parallel"},"answer":"A","explanation":"The best option here is to examine the application logs and fix\\nthe failure. In the application, you may have a functionality problem that is\\ncausing messages to queue up and increase the number of fleet instances\\nwithin the Auto-scaling group."},{"id":"292","question":"In several Amazon EC2 instances, you have an I/O and a\\nnetwork-intensive application that cannot handle a large ongoing\\nincrease in traffic. Two volumes of Amazon EBS PIOPS are used in\\nthe Amazon EC2 instances, each with the identical instance.\\nChoose the right approach in order to reduce the load on instances with the\\nleast interference with the application.","options":{"A":"Stop each instance and change each instance to a larger\\nAmazon EC2 instance type that has enhanced networking\\nenabled and is Amazon EBS-optimized. Ensure that RAID\\nstriping is also set up on each instance","B":"Add an instance-store volume for each running Amazon EC2\\ninstance and implement RAID striping to improve I/O\\nperformance","C":"Create an AMI from each instance, and set up Auto Scaling\\ngroups with a larger instance type that has enhanced\\nnetworking enabled and is Amazon EBS-optimized","D":"Create an AMI from an instance, and set up an Auto Scaling\\ngroup with an instance type that has enhanced networking\\nenabled and is Amazon EBS-optimized","E":"Add an Amazon EBS volume for each running Amazon EC2\\ninstance and implement RAID striping to improve I/O\\nperformance"},"answer":"D","explanation":"An Amazon Machine Image (AMI) gives the necessary\\ninformation for launching an instance that is a virtual cloud-based server.\\nWhen you launch an instance, you specify an AMI, and you can start as many\\ninstances as you need from an AMI. You can also launch instances from as\\nmany AMIs as you want."},{"id":"293","question":"You developed a new feature using AWS services. You want to\\ntest it from inside a staging VPC. How would you do this with the\\nfastest turnaround time?","options":{"A":"Use an Amazon EC2 instance that frequently polls the version\\ncontrol system to detect the new feature, use AWS\\nCloudFormation and Amazon EC2 user data to run any testing\\nharnesses to verify application functionality and then use\\nAmazon SNS to notify the development team of the results","B":"Use an Elastic Beanstalk application that polls the version\\ncontrol system to detect the new feature, use AWS\\nCloudFormation and Amazon EC2 user data to run any testing\\n\\n\\fharnesses to verify application functionality and then use\\nAmazon Kinesis to notify the development team of the results","C":"Launch an Amazon Elastic Compute Cloud (EC2) instance in\\nthe staging VPC in response to a development request, and use\\nconfiguration management to set up the application. Run any\\ntesting harnesses to verify application functionality and then\\nuse Amazon Simple Notification Service (SNS) to notify the\\ndevelopment team of the results","D":"Use AWS CloudFormation to launch an Amazon EC2 instance\\nusing Amazon EC2 user data to run any testing harnesses to\\nverify application functionality and then use Amazon Kinesis\\nto notify the development team of the results"},"answer":"C","explanation":"It would take more time to install Amazon Kinesis and would\\nnot be ideal to notify the concerned team as shortly as possible.\\nSince the test must be performed at the staging VPC, it is best to launch EC2\\nin the staging VPC.\\nThe best answer to this question would be the management of AWS\\nconfiguration together with SNS.\\nAWS Config provides a detailed inventory of current AWS resources and\\nrecords configuration modifications on a continuous basis such as the tags\\nvalue in the instance, security group entry/exit rules, and network ACL rules\\nin VPCs (see the AWS Config website for the list of supported AWS\\nresources). The AWS Config allows customers to determine how a resource\\nwas configured at any time, to view resource dependencies and to send\\nnotifications when the resource settings change. The AWS Config Rules are\\na new package that enables customers to assess whether their AWS resources\\nmeet the configuration requirements. In order to assess compliance of AWS\\nresources, customers can either use predefined AWS-managed rules or define\\nthemselves.\\nThe application should be tested in a staging VPC that is not described in\\noption A, therefore, option C is correct."},{"id":"294","question":"Your company assigns you the management of application that\\nuses Amazon SDK and Amazon EC2 roles to store and retrieve\\nAmazon S3 data, access multiple tables of DynamoDB and exchange\\n\\n\\fmessages using Amazon SQS queues. Your Compliance VicePresident is concerned that your security practices is not outstanding.\\nHe asked you to check that the application AWS access key is not\\nolder than six months, and to provide control evidence that these keys\\nare rotated at least once every six months. Which option suits the best\\nto provide the required information to VP?","options":{"A":"Update your application to log changes to its AWS access key\\ncredential file and use a periodic Amazon EMR job to create a\\ncompliance report for your VP","B":"Create a script to query the IAM list-access keys API to get\\nyour application access key creation date and create a batch\\nprocess to periodically create a compliance report for your VP","C":"Create a new set of instructions for your configuration\\nmanagement tool that will periodically create and rotate the\\napplication’s existing access keys and provide a compliance\\nreport to your VP","D":"Provide your VP with a link to IAM AWS documentation to\\naddress the VP’s key rotation concerns"},"answer":"B","explanation":"To know when access keys have been developed, use the \\"iam:\\nListAccessKeys\\". This knowledge can be used to identify which keys are\\nolder than six months. Execute a batch process to generate a conformity\\nreport as requested by the Department of Compliance VP."},{"id":"295","question":"You have an application that has mandate requirements for\\nsecurity and compliance and the protected health information that\\nbelongs to your application should be encrypted both at rest and\\ntransit. The data flows through the load balancer and is stored on\\nAmazon EBS volumes using three-architecture for processing. The\\noutputs are stored in S3 using AWS SDK service. Choose the two\\noptions which allow fulfilling the security requirements.","options":{"A":"Use SSL termination with a SAN SSL certificate on the load\\nbalancer. Amazon EC2 with all Amazon EBS volumes using\\nAmazon EBS encryption, and Amazon S3 with server-side\\n\\n\\fencryption with customer-managed keys","B":"Use TCP load balancing on the load balancer. SSL termination\\non the Amazon EC2 instances. OS-level disk encryption on the\\nAmazon EBS volumes and Amazon S3 with server-side\\nencryption","C":"Use SSL termination on the load balancer, Amazon EBS\\nencryption on Amazon EC2 instances and Amazon S3 with\\nserver-side encryption","D":"Use TCP load balancing on the load balancer. SSL termination\\non the Amazon EC2 instances and Amazon S3 with server-side\\nencryption","E":"Use SSL termination on the load balancer an SSL listener on\\nthe Amazon EC2 instances, Amazon EBS encryption on EBS\\nvolumes containing PHI and Amazon S3 with server-side\\nencryption"},"answer":"B and E","explanation":"HTTPS/SSL Listeners\\nThe following security features can be used to create a load balancer:\\nSSL Server Certificates\\nYou have to deploy X.509 certificates (SSL server certificates) on your load\\nbalancer if you use HTTPS or SSL for your front-end connections. Before\\nsending requests to the backend instance (known as SSL termination), the\\nload balancer decodes requests from clients.\\nYou can use TCP for front and back-end connections and deploy certificates\\non registered instances processing requests if you do not want a load balancer\\nto handle SSL termination (known as SSL offloading).\\nCreate a classic load balancer with an HTTPS Listener\\nA load balancer receives requests from customers and distributes the load\\nbalancer requests throughout the EC2 instances registered with the load\\nbalancer.\\nYou can create a load balancer that listens on both the HTTP (80) and\\nHTTPS (443) ports. If you specify the HTTPS listener to send requests to\\nport 80 instances, the load balancer ends the requests, and no load balancer\\ncommunications to the instances are encrypted. If the HTTPS listener sends\\n\\n\\frequests to port 443 instances, the load balancer communication is encrypted\\nto the instances.\\nOptions A & C is not correct because the transit between ELB and EC2\\ninstances is missing in encryption.\\nOption D is incorrect because the data related to the EC2 instances lack\\nencryption at rest."},{"id":"296","question":"Ace has an organization that has an AWS application with three\\ntiers: frontend, backend and database. Different AWS services\\nincluding EC2, ELB, Auto-scaling, Route53, RDS, and others are\\nbeing used. The RTO (Recovery Time Objective) is set at 1 hour for\\nthe entire application. What can help you achieve this goal?","options":{"A":"Create a Jenkins pipeline to automatically create AMIs for EC2\\ninstances. Execute the pipeline every hour","B":"Create regular EBS snapshots every hour using EBS lifecycle\\nmanager","C":"Create a warm standby in another region. Use Route53 failover\\nrouting policy to route to the standby if the active application\\nhas an outage","D":"In RDS, configure each database to create regular automated\\nsnapshots every hour. Copy the snapshots to another region"},"answer":"C","explanation":"The question asks for the strategy that can help the most to\\nachieve the goal. The program includes multiple elements and all of them\\nshould be considered. The RTO (Recovery Time Objective) is set as 1 hour,\\nwhich means the application will recover within 1 hour after the failure. The\\nRPO (Recovery Point Objective) does not mention in the issue. It is a\\nstandard approach to trying to accomplish a quick RTO. It can be the\\nrecovery time because the rest of the service is running in Warm standby."},{"id":"297","question":"During a deployment cycle, you recently found a major error in\\nyour web application. It took four hours by the team during this\\nunsuccessful deployment to return to a previously functional state,\\nwhich left customers with poor user experience. Your team discussed\\nthe need to roll back failed deployments quicker and more robust.\\n\\n\\fYour web application is running on Amazon EC2 and is using Elastic\\nLoad Balancing to balance your load. How do you solve the\\nproblem?","options":{"A":"Using Elastic Beanstalk, redeploy your web application and use\\nthe Elastic Beanstalk API to trigger a FailedDeployment API\\ncall to initiate a rollback to the previous version","B":"Create deployable versioned bundles of your application. Store\\nthe bundle on Amazon S3. Use an AWS OpsWorks stack to\\nredeploy your web application and use AWS OpsWorks\\napplication versioning to initiate a rollback during failures","C":"Use an AWS OpsWorks stack to re-deploy your web\\napplication and use AWS OpsWorks DeploymentCommand to\\ninitiate a rollback during failures","D":"Create deployable versioned bundles of your application. Store\\nthe bundle on Amazon S3. Re-deploy your web application on\\nElastic Beanstalk and enable the Elastic Beanstalk auto–\\nrollback feature tied to CloudWatch metrics that define failure"},"answer":"C","explanation":"AWS DeploymentCommand contains a rollback option in it.\\nApps can be used by the following commands:\\ndeploy: Deploy App.\\nRuby on Rails apps has an optional migrate args parameter. To migrate the\\ndatabase, set Args to {\\"migrate\\":[\\"true\\"]}.\\nThe default setting is {\\"migrate\\":[\\"false\\"]}.\\nThe app will roll back to the previous version with the \\"rollback\\" feature.\\nAWS OpsWorks stores the previous versions, up to five versions, when we\\nupdate an app.\\nWe can roll an app back in four versions with this command."},{"id":"298","question":"Addis has a Chef Version 11.10 running on AWS OpsWorks\\nStack. He has its own cookbook hosted on Amazon S3, and this is\\nspecified in the stack as a custom cookbook. A cookbook located in\\nan external Git repository is required, which is an open source\\ncookbook. How could he use both of the custom books?","options":{"A":"In the AWS OpsWorks stack settings, enable Berkshelf. Create\\na new cookbook with a Berksfile that specifies the other two\\ncookbooks. Configure the stack to use this new cookbook","B":"In the OpsWorks stack settings, add the open source project’s\\ncookbook details in addition to your cookbook","C":"In your cookbook, create an S3 symlink object that points to\\nthe open source project’s cookbook","D":"Contact the open source project’s maintainers and request that\\nthey pull your cookbook into theirs. Update the stack to use\\ntheir cookbook"},"answer":"A","explanation":"You need a way to install and manage dependencies to use an\\nexternal cookbook in an instance. A cookbook that supports a dependency\\nmanager named Berkshelf is the preferred approach. In addition to work with\\nthe test kitchens and Vagrants, Berkshelf is working on Amazon EC2\\ninstances such as AWS OpsWorks Stacks instances."},{"id":"299","question":"A group of developers within your company wishes to move its\\ncurrent application in Elastic Beanstalk and use Elastic Load\\nBalancing and Amazon SQS. Currently, you have a custom\\napplication server. How can this requirement be achieved?","options":{"A":"Configure an AWS OpsWorks stack that installs the third-party\\napplication server and creates a load balancer and an Amazon\\nSQS queue and then deploys it to Elastic Beanstalk","B":"Create a custom Elastic Beanstalk platform that contains the\\nthird-party application server and runs a script that creates a\\nload balancer and an Amazon SQS queue","C":"Configure an Elastic Beanstalk platform using AWS OpsWorks\\ndeploy it to Elastic Beanstalk and run a script that creates a\\nload balancer and an Amazon SQS queue","D":"Use a Docker container that has the third-party application\\nserver installed on it and create an application source bundle\\nthat produces the load balancer and an Amazon SQS queue"},"answer":"D","explanation":"Elastic Beanstalk allows the use of Docker server for web\\napplications. You can set up your own runtime environment with Docker\\ncontainers. You can choose your own platform, programming language, and\\napplications that are not supported by any other platforms (such as package\\nmanagers or tools). Docker containers are autonomous and hold all\\nconfiguration and software details that your web application needs to work\\nwith."},{"id":"300","question":"Your web application is running on three M3 instances in three\\nAZs. A group of three to thirty instances are scaled using the Autoscaling group. When you review the CloudWatch metrics, you see\\nthat there are sometimes 15 instances in your Auto Scaling group.\\nThe web application reads and writes to a DynamoDB-configured\\nbackend with 800 Write and read capacity units. The company’s ID is\\nyour DynamoDB main key. In your web application, you receive 25\\nTB of data. You have one customer who complains delay in load time\\nwhen his employees arrive at the office at 9:00 am and load the\\nwebsite consisting of content drawn out by DynamoDB. Other\\ncustomers use the web application routinely. Select the response to\\nensure high availability and reduce access times for the customer.","options":{"A":"Double the number of Read Capacity Units in your DynamoDB\\ninstance because the instance is probably being throttled when\\nthe customer accesses the website and your web application","B":"Add a caching layer in front of your web application by\\nchoosing ElastiCache Memcached instances in one of the AZs","C":"Implement an Amazon SQS queue between your DynamoDB\\ndatabase layer and the web application layer to minimize the\\nlarge burst in traffic the customer generates when everyone\\narrives at the office at 9:00 am and begins accessing the\\nwebsite","D":"Change your Auto Scaling group configuration to use Amazon\\nC3 instance types, because the web application layer is\\nprobably running out of compute capacity","E":"Use data pipelines to migrate your DynamoDB table to a new\\nDynamoDB table with a primary key that is evenly distributed\\n\\n\\facross your dataset. Update your web application to request\\ndata from the new table"},"answer":"E","explanation":"The table\'s provisioned throughput optimal depends on the\\nfollowing factors:\\nThe primary key selection\\nThe workload patterns on individual items\\nEach item in a table is uniquely defined by the primary key. The primary key\\ncan be simple (partition key) or composite (partition key and sort key).\\nDynamoDB divides the items of a table into multiple partitions when saving\\ndata and mainly distributes the data based on the key partition value.\\nConsequently, you keep the workload evenly across key partition values to\\nachieve the full amount of query throughput provided for a table. Distributing\\nrequests across partition key values distributes the requests across partitions.\\nWe can create a new index when we import data from S3 with the\\ndatapipeline into a new dynamodb table.\\nFollowing are the steps:\\ni.\\n\\nLogin to the console and select DynamoDB.\\n\\nii.\\n\\nSelect the table you have to copy.\\n\\niii.\\n\\nSelect Export / Import. For copying DynamoDB table to S3 or\\nfrom S3 to DynamoDB table export / import uses dataPipeline\\nand EMR.\\n\\niv.\\n\\nIf you do not have two IAM roles for export / import, you must\\ncreate them.\\n\\nv.\\n\\nClick the export table option.\\n\\nvi.\\n\\nYou will need to specify the following:\\nS3 bucket to copy the table data and another bucket to\\nstore log files for operation. The same bucket can be\\nused\\nThe percentage of the throughput capacity for the\\ntable to be used to read data from the table (to copy to\\n\\n\\fS3) that was provided. The default value is 25%. The\\nincreased percentage will accelerate backup\\nThe IAM roles: The values will be default\\nvii.\\n\\nChoose the option \\"create data pipeline\\", and the backup will\\nbe scheduled depending on the table size, the backup may take\\ntime.\\n\\nviii.\\n\\nAfter exporting, check logs to verify that no bugs are there.\\n\\nix.\\n\\nNote the hash and range key information of the table.\\n\\nx.\\n\\nDelete the table.\\n\\nxi.\\n\\nCreate a table with the right index. Set the provisioned\\nthroughput.\\n\\nxii.\\n\\nUsing import option, import into a table from S"},{"id":"3","question":"xiii.\\n\\nAfter completion, do check that it is error free.\\n\\n301. You want to allow automated testing of your CloudFormation\\ntemplate as part of your deployment pipeline. What assessments need\\nto be carried out to ensure that feedback is quicker and costs and risk\\nare minimized? (Choose 3)","options":{"A":"In the testing environment, while creating the stack, specify an\\nAmazon SNS topic for subscription. Your testing environment\\nruns tests when it receives notification when the stack is\\ncreated or updated","B":"In the testing environment, create and update the stack with the\\ntemplate. If the template fails rollback will return the stack and\\nits resources to exactly the same state","C":"Validate the AWS CloudFormation template against the\\nofficial XSD scheme definition published by Amazon Web\\nServices","D":"Use the AWS CloudFormation Validate Template to validate\\nthe syntax of the template","E":"Validate the template’s is syntax using a general JSON parser","F":"Use the AWS CloudFormation Validate Template to validate\\n\\n\\fthe properties of resources defined in the template\\nAnswer: A, B, and D\\nExplanation: The command AWS CloudFormation validate-template is\\ndesigned to test the template syntax only. It does not guarantee that the\\nproperty values for that resource that you listed are correct. Nor does it\\ndetermine the number of services that will be available when the stack is\\ncreated. You can attempt to create the stack to test the operational validity.\\nAWS CloudFormation stack does not include a sandbox or trial area, so you\\nwill be paid for the resources you build during your study.\\n302. An application is written in Go Programming language, which\\nhas to be deployed to AWS. The application code is stored on a Git\\nrepository. Choose two methods for this deployment.\\nA. Write a Dockerfile that installs the Go base image and fetches\\nyour application using Git. Create an AWS CloudFormation\\ntemplate that creates and associates an AWS::EC2::Instance\\nresource type with an AWS::EC2::Container resource type\\nB. Write a Dockerfile that installs the Go base image and uses Git\\nto fetch your application. Create a new AWS OpsWorks stack\\nthat contains a Docker layer that uses the Dockerrun.aws.json\\nfile to deploy your container and then use the Dockerfile to\\nautomate the deployment\\nC. Create a new AWS Elastic Beanstalk application and configure\\na Go environment to host your application. Using Git, check\\nout the latest version of the code, once the local repository for\\nElastic Beanstalk is configured using \\"eb create\\" command to\\ncreate an environment and then use \\"eb deploy\\" command to\\ndeploy the application\\nD. Write a Dockerfile that installs the Go base image and fetches\\nyour application using Git. Create a new AWS Elastic\\nBeanstalk application and use this Dockerfile to automate the\\ndeployment"},"answer":"C and D","explanation":"Option B is incorrect because OpsWorks works with Chef\\nrecipes and not with Docker containers.\\n\\n\\fOption A is incorrect because there is no AWS::EC2::Container resource for\\nCloudFormation.\\nThe deployment of a web application from Docker containers is supported by\\nElastic Beanstalk. You can set your own runtime environment with Docker\\ncontainers. You can set your own runtime environment with Docker\\ncontainers. You can choose your own platform, the language of\\nprogramming, and any application dependencies that are not supported by\\nother platforms, such as package managers or tools. Docker containers are\\nindependent and contain all configuration information and software required\\nto execute your web application."},{"id":"303","question":"Adrian has multi-level architecture that uses a load balancer and\\nis supported by a web tier within an Amazon EC2 Auto-scaling\\ngroup. This architecture serves public facing web traffic. During a\\ntraffic peak, Adrian notices that the amount of incoming traffic and\\nAuto-scaling policy that he had setup is adding new instances\\ndisproportionately. How should he stop the Auto-scaling group in\\nreaction to incoming traffic from scaling incorrectly?","options":{"A":"Using a custom CloudWatch metric, insert the elapsed time\\nsince the instance launch to the time the instance responds to an\\nElastic Load Balancing health check and periodically adjust the\\nPause Time of the UpdatePolicy and reduce the Scaling\\nAdjustment property by 50%","B":"Using CloudWatch and the instance BootTime metric, increase\\nthe PauseTime and CoolDown property on the Auto Scaling\\ngroup to be over the value of the metric","C":"Using a custom CloudWatch metric, insert the elapsed time\\nsince the instance launch to the time the instance responds to an\\nElastic Load Balancing health check, and periodically adjust\\nthe Pause Time and the CoolDown property on the\\nAutoScaling group to be over the value of the metric","D":"Using a third-party configuration management tool and the\\nAWS SDK, suspend all ScheduledActions of the Auto-scaling\\ngroup until after the traffic peak and then resume all\\nscheduledActions"},"answer":"C","explanation":"The question focuses on adding traffic-related instances with\\nan Auto-scaling group.\\nTo control how rolling updates are performed when changes are made to the\\nlaunch configuration of the auto scaling group, you can add a UpdatePolicy\\nattribute to your Auto-scaling group. It is mainly used in combination with\\nAutoScalingGroup resource CloudFormation Templates.\\nIn AutoScalingGroup\'s UpdatePolicy attribute, PauseTime is used.\\nIf you do not have the correct settings configured, a rolling update on an\\nAuto-scaling group can lead to unexpected behaviors.\\nPauseTime refers to the amount of time that AWS CloudFormation takes to\\nstart Software applications after making a change to a set of instances. For\\nexample, to scale up the instances in an Auto-scaling group, you might need\\nto specify PauseTime."},{"id":"304","question":"You have created a web application based on the Auto-scaling\\ngroup of web servers using ELB. You create a new AMI for every\\napplication version deployment. You are releasing a new version of\\nthe application in which you want a controlled migration of users\\nwith the A/B deployment technique, while the fleet is constant 12\\nhours long to ensure that the new version is functioning. What\\nmethod should you opt for to enable this technique while easily\\nrolling back when required?","options":{"A":"Create an Auto-scaling launch configuration with the new\\nAMI. Configure Auto-scaling to vary the proportion of\\ninstances launched from the two launch configurations","B":"Create a load balancer. Create an Auto-scaling launch\\nconfiguration with the new AMI to use the new launch\\nconfiguration and to register instances with the new load\\nbalancer. Use Amazon Route53 weighted Round Robin to vary\\nthe proportion of requests sent to the load balancers","C":"Launch new instances using the new AMI and attach them to\\nthe Auto-scaling group. Configure Elastic Load Balancing to\\nvary the proportion of requests sent to instances running the\\ntwo application versions","D":"Create an Auto-scaling launch configuration with the new\\n\\n\\fAMI. Configure the Auto-scaling group with the new launch\\nconfiguration. Use the Auto-scaling rolling updates feature to\\nmigrate to the new version","E":"Create an Auto-scaling launch configuration with the new\\nAMI. Create an Auto-scaling group configured to use the new\\nlaunch configuration and to register instances with the same\\nload balancer. Vary the desired capacity of each group to\\nmigrate"},"answer":"B","explanation":"Because you want to control the use of the new application, the\\nbest way is to use the weighted method of Route"},{"id":"53","question":"Weighted routing can\\nassign a single domain (example.com) or subdomain name\\n(acme.example.com) to multiple resources and choose the amount of traffic\\nthat is routed to each resource. This can be helpful for a range of purposes\\nsuch as load balancing and testing of new software versions.\\n305. Your Company uses a third-party configuration management\\ntool for web application development environment in order to create\\na Docker container on a local developer’s machine. What are you\\nsupposed to do for making sure that your application is not effected\\nby the web application, supporting network storage and security\\ninfrastructure after the deployment for staging and production\\nenvironments in AWS occurs?","options":{"A":"Define an AWS CloudFormation template to place your\\ninfrastructure into version control and use the same template to\\ndeploy the Docker container into Elastic Beanstalk for staging\\nand production","B":"Because the application is inside a Docker container, there are\\nno infrastructure differences to be taken into account when\\nmoving from the local development environments to AWS for\\nstaging and production","C":"Write a script using the AWS SDK or CLI to deploy the\\napplication code from version control to the local development\\nenvironments staging and production using AWS OpsWorks","D":"Define an AWS CloudFormation template for each stage of the\\napplication deployment lifecycle–development, staging, and\\n\\n\\fproduction – and have tagged in each template to define the\\nenvironment."},"answer":"A","explanation":"The deployment of a web application from Docker containers\\nis supported by Elastic Beanstalk. You can set your own runtime\\nenvironment with Docker containers. You can choose a platform,\\nprogramming language, and any application dependencies that are not\\nsupported by other platforms, such as package managers or tools. Docker\\ncontainers are autonomous and contain all the configuration information and\\nsoftware needed to run your web application.\\nIf you create your infrastructure by using Docker with Elastic Beanstalk, it\\nhandles capacity supply details, load-balancing, scaling and application\\nhealth monitoring automatically."},{"id":"306","question":"Your company has set up automated resources on AWS and you\\nare assigned to take care of these resources. Your task is to integrate\\nsome of the company’s chef recipes to be used for the existing\\nOpsWorks stack in AWS. The problem is, when you visited the\\nrecipes section, you could not find the option to add any recipes.\\nWhat could be reason for this and how will you fix it?","options":{"A":"The stack layers were created without the custom cookbooks\\noption. Change the layer setting accordingly","B":"The stack layers were created without the custom cookbooks\\noption. Just change the stack settings accordingly","C":"Once you create a stack, you cannot assign custom recipes, this\\nneeds to be done when the stack is created","D":"Once you create layers in the stack, you cannot assign custom\\nrecipes, this needs to be done when the layers are created"},"answer":"B","explanation":"This is mentioned in the AWS documentation: To have a stack\\ninstall and use custom cookbooks, you must configure the stack to enable\\ncustom cookbooks. If it is not already configured, you must then provide the\\nrepository URL and any related information such as password.\\nhttps://docs.aws.amazon.com/opsworks/latest/userguide/workingcookbook-\\n\\n\\finstallingcustom-enable.html"},{"id":"307","question":"You are a DevOps engineer for a company that has a number of\\nCloudFormation templates in AWS. The IT security department\\nwants to know all the users who use CloudFormation stacks in the\\ncompany’s AWS account. Which of the following can be done to\\ntake care of this security concern?","options":{"A":"Connecting SQS and CloudFormation so that a message is\\npublished for each resource created in the CloudFormation\\nstack","B":"Enabling CloudWatch logs for each CloudFormation stack to\\ntrack the resource creation events","C":"Enabling CloudTrail logs so that the API calls can be recorded","D":"Enabling CloudWatch events for each CloudFormation stack to\\ntrack resource creation events"},"answer":"C","explanation":"This is given as a best practice on the AWS documentation:\\nAWS CloudTrail tracks anyone making AWS CloudFormation API calls in\\nyour AWS account. API calls are logged whenever anyone uses the AWS\\nCloudFormation API, the AWS CloudFormation console, a back-end\\nconsole, or AWS CloudFormation AWS CLI commands. Enable logging and\\nspecify an Amazon S3 bucket to store the logs. That way, if you ever need to,\\nyou can audit who made what AWS CloudFormation call in your account.\\nhttps://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/bestpractices.html"},{"id":"308","question":"Your development team is planning for continuous release\\ncycles for their application. They want to use the AWS service\\navailable to be able to deploy a web application and rollback to its\\nprevious version quickly if required. Which of the following can be\\nused to meet this requirement? (Choose 2)","options":{"A":"Use the CloudFormation service. Create separate templates for\\neach application revision and deploy them accordingly","B":"Use the OpsWorks service to deploy the web instances. Deploy\\nthe app to the OpsWorks web layer. Rollback using the Deploy\\napp in OpsWorks","C":"Use the Elastic BeanStalk service. Create separate\\nenvironments for each application revision. Revert back to an\\nenvironment in case the new environment does not work","D":"Use the Elastic beanstalk service. Use Application versions and\\nupload the revisions of your application. Deploy the revisions\\naccordingly and rollback to prior versions accordingly"},"answer":"B and D","explanation":"The AWS documentation states:\\nIn Elastic BeanStalk, and application version refers to a specific, labeled\\niteration of deployable code for a web application. An application version\\npoints to an Amazon Simple Storage Service (Amazon S3) object that\\ncontains the deployable code such as a Java WAR file. An application\\nversion is part of an application. Applications can have many versions and\\neach application version is unique. In a running environment, you can deploy\\nany application version you already uploaded to the application or you can\\nupload and immediately deploy a new application version. You might upload\\nmultiple application versions to test differences between one version of your\\nweb\\napplication\\nto\\nanother.\\nhttps://docs.aws.amazon.com/elasticbeanstalk/latest/dg/concepts.html\\nAWS OpsWorks Stacks app represents code that you want to run on an\\napplication server. The code itself resides in a repository such as an Amazon\\nS3 archive; the app contains the information required to deploy the code to\\nthe appropriate application server instances.\\nhttps://docs.aws.amazon.com/opsworks/latest/userguide/workingapps.html"},{"id":"309","question":"On AWS, to be able to scale out to react the increase in demand\\nquickly, you need to build out a layer in a software stack. You are\\nrunning the code on EC2 instances in an Auto-scaling group behind\\nan ELB. Which application deployment should you use?","options":{"A":"Create a new Auto Scaling Launch Configuration with\\nUserData scripts configured to pull the latest code at all times","B":"Create a Dockerfile when preparing to deploy a new version to\\nproduction and publish it to S3. Use UserData in the Auto\\nScaling Launch configuration to pull down the Dockerfile from\\nS3 and run it when new instances launch","C":"Bake an AMI when deploying new versions of code, and use\\nthat AMI for the Auto Scaling Launch Configuration","D":"SSH into new instances that come online, and deploy new code\\nonto the system by pulling it from an S3 bucket, which is\\npopulated by code that you refresh from source control on new\\npushes"},"answer":"C","explanation":"Since the time required to spin up an instance is required to be\\nfast, it is better to create an AMI rather than use User Data. When you use\\nUser Data, the script will be run during boot up, and hence this will be\\nslower. An Amazon Machine Image (AMI) provides the information required\\nto launch an instance, which is a virtual server in the cloud. You specify an\\nAMI when you launch an instance, and you can launch as many instances\\nfrom the AMI as you need. You can also launch instances from as many\\ndifferent AMIs as you need.\\nhttps://docs.aws.amazon.com/AWSEC2/latest/UserGuide/AMIs.html"},{"id":"310","question":"To coordinate the creation of stack resources in\\nCloudFormation, which of the following can be used? (Choose 2)","options":{"A":"CreationPolicy attribute","B":"HoldPolicy attribute","C":"AWS::CloudFormation::WaitCondition","D":"AWS::CloudFormation::HoldCondition"},"answer":"A and C","explanation":"The AWS documentation mentions that using the\\nAWS::CloudFormation::WaitCondition resource and CreationPolicy\\nattribute, you can do the following:\\nCoordinate stack resource creation with other configuration actions\\nthat are external to the stack creation\\n\\n\\fTrack the status of a configuration process\\nhttps://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/usingcfn-waitcondition.html"},{"id":"311","question":"How much time does the OpsWorks service waits for a response\\nbefore deeming an underlying instance as a failed instance?","options":{"A":"60 minutes","B":"20 minutes","C":"5 minutes","D":"1 minute"},"answer":"C","explanation":"As per the AWS documentation, every instance has an AWS\\nOpsWorks Stacks agent that communicates regularly with the service. AWS\\nOpsWorks Stacks uses that communication to monitor instance health. If an\\nagent does not communicate with the service for more than approximately\\nfive minutes, AWS OpsWorks Stacks considers the instance to have failed.\\nhttps://docs.aws.amazon.com/opsworks/latest/userguide/workinginstancesautohealing.html"},{"id":"312","question":"You are designing an OpsWorks stack in AWS. Your\\ncompany’s on-premises Chef configuration has some custom recipes\\nthat are required to run whenever an instance is launched in\\nOpsWorks. Which steps should be carried out to fulfill this\\nrequirement? (Choose 2)","options":{"A":"Ensure the recipe is placed as part of the Setup Lifecycle event\\nas part of the Stack setting","B":"Ensure the recipe is placed as part of the Setup Lifecycle event\\nas part of the Layer setting","C":"Ensure the custom cookbooks option is set in OpsWorks layer","D":"Ensure the custom cookbooks option is set in OpsWorks stack"},"answer":"B and D","explanation":"Each layer has a set of built-in recipes assigned to each\\n\\n\\flifecycle event, although some layers lack Undeploy recipes. When a\\nlifecycle event occurs on an instance, AWS OpsWorks Stacks runs the\\nappropriate set of recipes for the associated layer. For more information on\\nautomating recipes, follow the below URL:\\nhttps://docs.aws.amazon.com/opsworks/latest/userguide/workingcookbookassigningcustom.html"},{"id":"313","question":"From the following options, which one is a reliable and durable\\nlogging solution to track changes made to your AWS account?","options":{"A":"Create three new CloudTrail trails with three new S3 buckets to\\nstore the logs one for the AWS Management console, one for\\nAWS SDKs and one for command line tools. Use IAM roles\\nand S3 bucket policies on the S3 buckets that store your logs","B":"Create a new CloudTrail trail with an existing S3 bucket to\\nstore the logs and with the global services option selected. Use\\nS3 ACLs and Multi Factor Authentication (MFA) Delete on the\\nS3 bucket that stores your logs","C":"Create a new CloudTrail with one new S3 bucket to store the\\nlogs. Configure SNS to send log file delivery notifications to\\nyour management system. Use IAM roles and S3 bucket\\npolicies on the S3 bucket that stores your logs","D":"Create a new CloudTrail trail with one new S3 bucket to store\\nthe logs and with the global services option selected. Use IAM\\nroles S3 bucket policies and Multi Factor Authentication\\n(MFA) Delete on the S3 bucket that stores your logs"},"answer":"D","explanation":"AWS Identity and Access Management (IAM) is integrated\\nwith AWS CloudTrail, a service that logs AWS events made by or on behalf\\nof your AWS account. CloudTrail logs authenticated AWS API calls and also\\nAWS sign-in events, and collects this event information in files that are\\ndelivered to Amazon S3 buckets. You need to ensure that all services are\\nincluded. Hence option B is partially correct.\\nhttps://docs.aws.amazon.com/IAM/latest/UserGuide/cloudtrailintegration.html"},{"id":"314","question":"To create the current application in a specified environment,\\nwhich of the following commands for the Elastic BeanStalk CLI can\\nbe used?","options":{"A":"en app","B":"en env","C":"eb start","D":"eb create"},"answer":"D","explanation":"EB is a Command Line Interface (CLI) tool for Elastic\\nBeanStalk that you can use to deploy your applications quickly and more\\neasily. EB CLI introduces the commands eb create, eb deploy, eb open, eb\\nconsole, eb scale, eb setenv, eb config, eb terminate, eb clone, eb list, eb use,\\neb printenv, and eb ssh. In EB CLI 3.1 or later, you can also use the eb swap\\ncommand. In EB CLI 3.2 only, you can use the eb abort, eb platform, and eb\\nupgrade commands. In addition to these new commands, EB CLI 3\\ncommands differ from EB CLI 2.6 commands in several cases:\\nFor more information on EB CLI, follow the below URL:\\nhttps://docs.aws.amazon.com/elasticbeanstalk/latest/dg/eb-cli.html#eb-cli2differences"},{"id":"315","question":"Your company is planning to automate its integration, build, and\\ndeployment processes by using available AWS services. It is a part of\\nplan to use AWS CodeBuild to build the artifacts. When using AWS\\nCodeBuild, which of the following files is used to specify a collection\\nof build commands that can be used by the service during the build\\nprocess?","options":{"A":"appspec.json","B":"buildspec.xml","C":"buildspec.yml","D":"appspec.yml"},"answer":"C","explanation":"It is mentioned in the AWS documentation: AWS CodeBuild\\ncurrently supports building from the following source code repository\\nproviders. The source code must contain a build specification (build spec)\\nfile, or the build spec must be declared as part of a build project definition. A\\nbuildspec is a collection of build commands and related settings, in YAML\\nformat, that AWS CodeBuild uses to run a build.\\nhttps://docs.aws.amazon.com/codebuild/latest/userguide/planning.html"},{"id":"316","question":"As a DevOps engineer, it is your task to create a continuous\\nintegrated and continuous delivery model for your organization’s\\napplication. Which of the following services are best suited for this\\npurpose? (Choose 2)","options":{"A":"AWS SQS","B":"AWS IAM","C":"AWS CodeDeploy","D":"AWS Code PipeLine"},"answer":"C and D","explanation":"AWS CodeDeploy is a deployment service that automates\\napplication deployments to Amazon EC2 instances or on-premises instances\\nin your own facility. You can deploy a nearly unlimited variety of application\\ncontent, such as code, web and configuration files, executables, packages,\\nscripts, multimedia files, and so on. AWS CodeDeploy can deploy\\napplication content stored in Amazon S3 buckets, GitHub repositories, or\\nBitbucket repositories.\\nAWS CodePipeline is a continuous delivery service you can use to model,\\nvisualize, and automate the steps required to release your software. You can\\nquickly model and configure the different stages of a software release\\nprocess. AWS CodePipeline automates the steps required to release your\\nsoftware changes continuously.\\nYou can learn more about CodeDeploy and Code PipeLine using the given\\nbelow URLs:\\nhttps://docs.aws.amazon.com/codedeploy/latest/userguide/welcome.html\\nhttps://docs.aws.amazon.com/codepipeline/latest/userguide/welcome.html"},{"id":"317","question":"To make sure that the EC2 instances can work with AWS\\n\\n\\fCodeDeploy, which of the following pre-requisites must be ensured?\\n(Choose 2)","options":{"A":"The CodeDeoploy agent is installed on the EC2 instance","B":"The EC2 instance is placed in the default VPC","C":"The EC2 instance is configured with Enhanced Networking","D":"An IAM role is attached to the instance so that it can work with\\nthe CodeDeploy service"},"answer":"A and D","explanation":"These pre-requisites are clearly mentioned in the AWS\\ndocumentation. You can learn more about instance for CodeDeploy by\\nvisiting the given URL:\\nhttps://docs.aws.amazon.com/codedeploy/latest/userguide/instances.html"},{"id":"318","question":"You have production instances running in an Auto-scaling group\\nand it is decided that you need to change the instance type of these\\ninstances. You currently have four instances and you cannot afford\\ninterruption in service and you want the surety that two instances are\\nrunning during the update. Which of the below options can be used\\nfor this?","options":{"A":"AutoScalingIntegrationUpdate","B":"AutoScalingReplacingUpdate","C":"AutoScalingRescheduledAction","D":"AutoScalingRollingUpdate"},"answer":"D","explanation":"The AWS::AutoScaling::AutoScalingGroup resource supports\\nan UpdatePolicy attribute. This is used to define how an Auto-scaling group\\nresource is updated when an update to the CloudFormation stack occurs. A\\ncommon approach to updating an Auto Scaling group is to perform a rolling\\nupdate, which is done by specifying the AutoScalingRollingUpdate policy.\\nThis retains the same Auto-scaling group and replaces old instances with new\\nones, according to the parameters specified. For more information on Autoscaling updates, please refer to the given URL:\\n\\n\\fhttps://aws.amazon.com/premiumsupport/knowledge-center/auto-scalinggroup-rolling-updates/"},{"id":"319","question":"As a DevOps engineer, you are instructed to deploy Docker\\ncontainers using AWS OpsWorks. How will you do this? (Choose 2)","options":{"A":"Use CloudFormation to deploy Docker containers since this is\\nnot possible in OpsWorks. Then attach the CloudFormation\\nresources as a layer in OpsWorks","B":"Use custom cookbooks for your OpsWorks stack and provide\\nthe Git repository which has the chef recipes for the Docker\\ncontainers","C":"In the App for OpsWorks deployment, specify the Git URL for\\nthe recipes which will deploy the applications in the Docker\\nenvironment","D":"Use Elastic beanstalk to deploy Docker containers since this is\\nnot possible in OpsWorks. Then attach the elastic BeanStalk\\nenvironment as a layer in OpsWorks"},"answer":"B and C","explanation":"AWS OpsWorks lets you deploy and manage application of all\\nshapes and sizes. OpsWorks layers let you create blueprints for EC2\\ninstances to install and configure any software that you want. You can refer\\nto the given URL for more information about Docker and OpsWorks.\\nhttps://aws.amazon.com/blogs/devops/running-docker-on-aws-opsworks/"},{"id":"320","question":"For automatic collection of software inventory and applying OS\\npatches on EC2 instances, which of the following AWS tools can be\\nhelpful?","options":{"A":"EC2 AMIs","B":"AWS Code PipeLine","C":"EC2 Systems Manager","D":"AWS CodeDeploy"},"answer":"C","explanation":"The Amazon EC2 Systems Manager helps you automatically\\ncollect software inventory, apply OS patches, create system images, and\\nconfigure Windows and Linux operating systems. These capabilities enable\\nautomated configuration and ongoing management of systems at scale, and\\nhelp maintain software compliance for instances running in Amazon EC2 or\\non-premises. One feature within Systems Manager is Automation, which can\\nbe used to patch, update agents, or bake applications into an Amazon\\nMachine Image (AMI). With Automation, you can avoid the time and effort\\nassociated with manual image updates, and instead build AMIs through a\\nstreamlined, repeatable, and auditable process.\\nhttps://aws.amazon.com/blogs/aws/streamline-ami-maintenance-andpatching-using-amazon-ec2-systems-manager-automation/"},{"id":"321","question":"Your application uses the EC2/On-premises compute platform.\\nWhich of the following files needs to be included along with source\\ncode binaries while deploying your application using the AWS\\nCodeDeploy service?","options":{"A":"appspec.yml","B":"appspec.json","C":"appconfig.yml","D":"appconfig.json"},"answer":"A","explanation":"The AWS documentation mentions:\\nThe application specification file (AppSpec file) is a YAML-formatted file\\nused by AWS CodeDeploy to determine:\\nWhat it should install onto your instances from your application\\nrevision in Amazon S3 or GitHub\\nWhich lifecycle event hooks to run in response to deployment\\nlifecycle events\\nAn AppSpec file must be named appspec.yml and it must be placed in the\\nroot of an application\'s source code\'s directory structure. Otherwise,\\ndeployments will fail.\\nhttps://docs.aws.amazon.com/codedeploy/latest/userguide/reference-appspecfile.html"},{"id":"322","question":"Your company has a number of CloudFormation stack defined\\nin AWS. Some of these stacks have been targeted for deletion as part\\nof the routine housekeeping activity. Upon trying, few of those stacks\\nare failing to delete. Which of the following could be the reason of\\nthis failure? (Choose 2)","options":{"A":"The stack has an EC2 security group, which has EC2 instance\\nattached to it","B":"The stacks were created with wrong template version. Since the\\nstandard template version is now higher, it is preventing the\\ndeletion if the stacks. You need to contact AWS support","C":"The stack has an S3 bucket defined, which has objects present\\nin it","D":"The stack consists of an EC2 resource, which was created with\\na custom AMI"},"answer":"A and C","explanation":"It is mentioned in the AWS documentation:\\nSome resources must be empty before they can be deleted. For example, you\\nmust delete all objects in an Amazon S3 bucket or remove all instances in an\\nAmazon EC2 security group before you can delete the bucket or security\\ngroup.\\nhttps://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/troubleshooting.htm"},{"id":"323","question":"Which of the following are the offerings of AWS to secure data\\nat rest and in transit? (Choose 3)","options":{"A":"Using IOPS volumes when working with EBS volumes on EC2\\ninstances","B":"Using server side encryption for S3","C":"Encrypting all EBS volumes attached to EC2 instances","D":"Using SSL/HTTPS when using the ELB\\nAnswer: B, C, and D\\nExplanation: According to the AWS documentation, data protection refers\\nto protecting data while in-transit (as it travels to and from Amazon S3) and\\n\\n\\fat rest (while it is stored on disks in Amazon S3 data centers). You can\\nprotect data in transit by using SSL or by using client-side encryption. You\\nhave the following options of protecting data at rest in Amazon S3.\\nUse Server-Side Encryption – You request Amazon S3 to encrypt your\\nobject before saving it on disks in its data centers and decrypt it when\\nyou download the objects\\nUse Client-Side Encryption – You can encrypt data client-side and\\nupload the encrypted data to Amazon S3. In this case, you manage the\\nencryption process, the encryption keys, and related tools\\nAmazon EBS encryption offers you a simple encryption solution for your\\nEBS volumes without the need for you to build, maintain, and secure your\\nown key management infrastructure. When you create an encrypted EBS\\nvolume and attach it to a supported instance type, the following types of data\\nare encrypted:\\nData at rest inside the volume\\nAll data moving between the volume and the instance\\nAll snapshots created from the volume\\nYou can create a load balancer that uses the SSL/TLS protocol for encrypted\\nconnections (also known as SSL offload). This feature enables traffic\\nencryption between your load balancer and the clients that initiate HTTPS\\nsessions, and for connections between your load balancer and your EC2\\ninstances.\\nhttps://d0.awsstatic.com/whitepapers/aws-securing-data-at-rest-withencryption.pdf\\n324. You need to create nested stacks in AWS CloudFormation.\\nWhich of the following resource should be used?\\nA. AWS::CloudFormation::StackNest\\nB. AWS::CloudFormation::NestedStack\\nC. AWS::CloudFormation::Nested\\nD. AWS::CloudFormation::Stack"},"answer":"D","explanation":"It is mentioned in the AWS documentation, a nested stack is a\\nstack that you create within another stack by using the\\n\\n\\fAWS::CloudFormation::Stack resource. With nested stacks, you deploy and\\nmanage all resources from a single stack. You can use outputs from one stack\\nin the nested stack group as inputs to another stack in the group. Refer to the\\nbelow given URL for more information:\\nhttps://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/usingcfn-stack-exports.html"},{"id":"325","question":"Your development team uses .Net to code your web application.\\nFor continuous integration and deployment, the team wants it to be\\ndeployed to AWS. The code of application is hosted on a Git\\nrepository. Choose from the following combination of steps that\\nneeds to be taken to fulfil the requirement. (Choose 2)","options":{"A":"Use a chef recipe to deploy the code and attach it to the Elastic\\nbeanstalk environment","B":"Create a source bundle for the .Net code and upload it as an\\napplication revision","C":"Use the Code Pipeline service to provision an IIS environment\\nto host the application","D":"Use the Elastic beanstalk service to provision an IIS platform\\nweb environment to host the application"},"answer":"B and D","explanation":"When you provision an environment using the Elastic\\nBeanStalk service, you can choose the IIS platform to host the .Net based\\napplication. You can also upload the application as a zip file and specify it as\\nan application revision.\\nhttps://docs.aws.amazon.com/elasticbeanstalk/latest/dg/create_deploy_NET.html"},{"id":"326","question":"You have an application hosted on AWS and you are using\\nJenkins as your continuous integration system. The builds are then\\ndeployed on newly launched EC2 instances. You want to minimize\\nthe overall cost of the continuous integration and deployment\\npipeline. Which of the below options will help in meeting the\\nrequirements? (Choose 2)","options":{"A":"Ensure the instances are created beforehand for faster\\n\\n\\fturnaround time for the application builds to be placed","B":"Ensure that all build tests are conducted using Jenkins before\\ndeploying the build to newly launched EC2 instances","C":"Ensure that all build tests are conducted on newly launched\\nEC2 instances","D":"Ensure the instances are launched only when the build tests are\\ncompleted"},"answer":"B and D","explanation":"To ensure low cost, one can carry out the build tests on the\\nJenkins server itself. Once the build tests are completed, the build can then be\\ntransferred onto newly launched EC2 Instances. Refer to the given URL for\\nmore information about AWS and Jenkins.\\nhttps://aws.amazon.com/getting-started/projects/setup-jenkins-build-server/"},{"id":"327","question":"Your IT supervisor is worried about the cost incurred company’s\\nAWS resources that are hosted on AWS. The supervisor wants to\\nmonitor the cost usage. Which of the following options can be helpful\\nin monitoring the cost of AWS resources while also looking at the\\npossibility of cost optimization? (Choose 3)","options":{"A":"Consider using the Trusted Advisor","B":"Send all logs to CloudWatch logs and inspect the logs for\\nbilling details","C":"Create Budgets in billing section so that budgets are set before\\nhand","D":"Use the Cost Explorer to see the costs of AWS resources\\nAnswer: A, C, and D\\nExplanation: Visit the AWS Trusted Advisor console regularly. Trusted\\nAdvisor works like a customized cloud expert, analyzing your AWS\\nenvironment and providing best practice recommendations to help you save\\nmoney, improve system performance and reliability, and close security gaps.\\nConsider using budgets if you have a defined spending plan for a project or\\nservice and you want to track how close your usage and costs are to\\nexceeding your budgeted amount. Budgets use data from Cost Explorer to\\n\\n\\fprovide you with a quick way to see your usage-to-date and current estimated\\ncharges from AWS. You can also set up notifications that warn you if you\\nexceed or are about to exceed your budgeted amount.\\nFor a quick, high-level analysis use Cost Explorer, which is a free tool that\\nyou can use to view graphs of your AWS spend data. It includes a variety of\\nfilters and preconfigured views, as well as forecasting capabilities. Cost\\nExplorer displays data from the last 13 months, the current month, and the\\nforecasted costs for the next three months, and it updates this data daily.\\nhttps://aws.amazon.com/solutions/cost-optimization-monitor/\\n328. Your application is deployed on an EC2 instance and it needs to\\nwrite data to DynamoDB table. Your security policy dictates that it is\\nnot allowed to store any security keys on the EC2 instance. Keeping\\nthe security policy and the requirement in mind, which of the\\nfollowing steps would you take? (Choose 2)\\nA. Add an IAM user to a running EC2 instance\\nB. Create an IAM user that allows write access to the DynamoDB\\ntable\\nC. Create an IAM role that allows write access to the DynamoDB\\ntable\\nD. Add an IAM role to a running EC2 instance"},"answer":"C and D","explanation":"The AWS documentation for IAM roles states, we designed\\nIAM roles so that your applications can securely make API requests from\\nyour instances, without requiring you to manage the security credentials that\\nthe applications use. Instead of creating and distributing your AWS\\ncredentials, you can delegate permissions to make API requests using IAM\\nroles.\\nhttp://docs.aws.amazon.com/AWSEC2/latest/UserGuide/iam-roles-foramazon-ec2.html"},{"id":"329","question":"There are multiple applications and services running in your\\ncompany’s production AWS account at any given time. It is required\\nto understand where the cost is coming from. Without spending too\\nmuch development time, how can you provide a good understanding\\n\\n\\fof the most cost incurring application per month?","options":{"A":"Use the AWS Price API and constantly running resource\\ninventory scripts to calculate total price based on multiplication\\nof consumed resources over time","B":"Use AWS Cost Allocation Tagging for all resources that\\nsupport it. Use the Cost Explorer to analyze costs throughout\\nthe month","C":"Use custom CloudWatch Metrics in your system, and put a\\nmetric data point whenever cost is incurred","D":"Create an automation script that periodically creates AWS\\nSupport tickets requesting detailed intra-month information\\nabout your bill"},"answer":"B","explanation":"A tag is a label that you or AWS assigns to an AWS resource.\\nEach tag consists of a key and a value. A key can have more than one value.\\nYou can use tags to organize your resources, and cost allocation tags to track\\nyour AWS costs on a detailed level. After you activate cost allocation tags,\\nAWS uses the cost allocation tags to organize your resource costs on your\\ncost allocation report, to make it easier for you to categorize and track your\\nAWS costs. AWS provides two types of cost allocation tags, an AWS\\ngenerated tag and user defined tags. AWS defines, creates, and applies the\\nAWS-generated tag for you, and you define, create, and apply user-defined\\ntags. You must activate both types of tags separately before they can appear\\nin Cost Explorer or on a cost allocation report.\\nhttps://docs.aws.amazon.com/awsaccountbilling/latest/aboutv2/cost-alloctags.html"},{"id":"330","question":"You are building an internal, for non-production use application\\nbased on Go programming language. This application used MySQL\\nas database. Your developers are not very much familiar with the\\nAWS environment, but you want them to be able to deploy the code\\nwith a single command line push and you want to set this up as\\nsimply as possible. Which tool is ideal for this?","options":{"A":"AWS Elastic BeanStalk","B":"AWS EC2 + ELB","C":"AWS OpsWorks","D":"AWS CloudFormation"},"answer":"A","explanation":"With Elastic BeanStalk, you can quickly deploy and manage\\napplications in the AWS Cloud without worrying about the infrastructure that\\nruns those applications. AWS Elastic BeanStalk reduces management\\ncomplexity without restricting choice or control. You simply upload your\\napplication, and Elastic BeanStalk automatically handles the details of\\ncapacity provisioning, load balancing, scaling, and application health\\nmonitoring. Elastic Beanstalk supports applications developed in Java, PHP,\\n.NET, Node.js, Python, and Ruby, as well as different container types for\\neach language.\\nhttps://docs.aws.amazon.com/elasticbeanstalk/latest/dg/Welcome.html"},{"id":"331","question":"To process messages from an SQS queue, you have a set of EC2\\ninstances in an Auto-scaling group. The messages contain the\\nlocation in S3 from where videos need to be processed by the EC2\\ninstances. When a scale happens, it is observed that while processing\\na video, the EC2 instance gets terminated. How can you implement a\\nsolution to avoid this situation?","options":{"A":"By increasing the maximum and minimum size for the Autoscaling group, and changing the scaling policies so they scale\\nless dynamically","B":"By suspending the AZRebalance termination policy","C":"By changing the CoolDown property for the Auto-scaling\\ngroup","D":"By using lifecycle hooks to ensure the processing is complete\\nbefore the termination occurs"},"answer":"D","explanation":"This is a case where lifecycle policies can be used. The\\nlifecycle policy can be used to put the instance in a state of\\nTerminating:Wait, complete the processing and then send a signal to\\n\\n\\fcomplete the termination. Auto-scaling lifecycle hooks enable you to perform\\ncustom actions by pausing instances as Auto-scaling launches or terminates\\nthem. For example, while your newly launched instance is paused, you could\\ninstall or configure software on it.\\nhttps://docs.aws.amazon.com/autoscaling/ec2/userguide/lifecycle-hooks.html"},{"id":"332","question":"You have built an application that consists of a web and\\napplication server. You are going to deploy it using Elastic\\nBeanStalk. Before the application version is deployed on the web\\nserver, it is required to run some python scripts. Which of the\\nfollowing can be used for this?","options":{"A":"Container commands","B":"Custom resources","C":"Multiple Elastic BeanStalk environments","D":"Docker containers"},"answer":"A","explanation":"The AWS documentation mentions the following; You can use\\nthe container_commands key to execute commands that affect your\\napplication source code. Container commands run after the application and\\nweb server have been set up and the application version archive has been\\nextracted, but before the application version is deployed. Non-container\\ncommands and other customization operations are performed prior to the\\napplication source code being extracted.\\nhttps://docs.aws.amazon.com/elasticbeanstalk/latest/dg/customize-containersec2.html"},{"id":"333","question":"Your company’s application is hosted on a set of EC2 instances\\nthat are sitting behind an ELB. To host the newer version of the\\napplication, it is required to create an OpsWorks stack. You have\\nplanned to get the stack in place, carry out a level of testing, and then\\ndeploy the app at a later stage. The OpsWorks stack and layers have\\nbeen setup. For testing, current ELB is being utilized but now you are\\nnoticing that your current application has stopped responding to\\nrequests. Why do you think it happened?","options":{"A":"This is because the OpsWorks web layer is utilizing the current\\ninstances after the ELB was attached as an additional layer","B":"The ELB would have deregistered the older instances","C":"You have configured the OpsWorks stack to deploy new\\ninstances in the same domain the older instances","D":"This is because the OpsWorks stack is utilizing the current\\ninstances after the ELB was attached as a layer"},"answer":"B","explanation":"If you choose to use an existing Elastic Load Balancing load\\nbalancer, you should first confirm that it is not being used for other purposes\\nand has no attached instances. After the load balancer is attached to the layer,\\nOpsWorks removes any existing instances and configures the load balancer\\nto handle only the layer\'s instances. Although it is technically possible to use\\nthe Elastic Load Balancing console or API to modify a load balancer\'s\\nconfiguration after attaching it to a layer, you should not do so; the changes\\nwill not be permanent.\\nhttps://docs.aws.amazon.com/opsworks/latest/userguide/layers-elb.html"},{"id":"334","question":"You have a set of EC2 instances that was launched by an auto\\nscaling group. These instances are sitting behind an ELB. The\\nassurance of storing web server logs in a durable storage layer is\\nrequired. This surety is required so that the logs can later be analyzed\\nby the staff. Which of the following steps should be takes to fulfil this\\nrequirement? (Choose 2)","options":{"A":"Use AWS Data Pipeline to move log data from the Amazon S3\\nbucket to Amazon SQS in order to process and run reports","B":"On the web servers, create a scheduled task that executes a\\nscript to rotate and transmit the logs to Amazon Glacier","C":"Use AWS Data Pipeline to move log data from the Amazon S3\\nbucket to Amazon RedShift in order to process and run reports","D":"On the web servers, create a scheduled task that executes a\\nscript to rotate and transmit the logs to an Amazon S3 bucket"},"answer":"C and D","explanation":"Amazon S3 is the perfect option for durable storage. The AWS\\nDocumentation mentions the following on S3 Storage. Amazon Simple\\nStorage Service (Amazon S3) makes it simple and practical to collect, store,\\nand analyze data - regardless of format – all at massive scale. S3 is object\\nstorage built to store and retrieve any amount of data from anywhere – web\\nsites and mobile apps, corporate applications, and data from IoT sensors or\\ndevices.\\nhttps://aws.amazon.com/s3/\\nAmazon Redshift is a fast, fully managed data warehouse that makes it\\nsimple and cost-effective to analyze all your data using standard SQL and\\nyour existing Business Intelligence (BI) tools. It allows you to run complex\\nanalytic queries against petabytes of structured data, using sophisticated\\nquery optimization, columnar storage on high-performance local disks, and\\nmassively parallel query execution. Most results come back in seconds.\\nhttps://aws.amazon.com/redshift/"},{"id":"335","question":"You are leading a team that is responsible for an Elastic\\nBeanStalk application. The business requires you to move to a\\ncontinuous deployment model, releasing updates to the application\\nmultiple times per day with zero downtime. What will you do to\\nensure this with the ability to rollback immediately in a case of an\\nemergency?","options":{"A":"Create a second Elastic Beanstalk environment with the new\\napplication version, and configure the old environment to\\nredirect clients, using the HTTP 301 response code, to the new\\nenvironment","B":"Develop the application to poll for a new application version in\\nyour code repository; download and install to each running\\nElastic Beanstalk instance","C":"Create a second Elastic Beanstalk environment running the\\nnew application version, and swap the environment CNAMEs","D":"Enable rolling updates in the Elastic Beanstalk environment,\\nsetting an appropriate pause time for application startup"},"answer":"C","explanation":"Since the requirement calls for zero downtime and for the\\nability roll back quickly, we need to implement Blue green deployments\\nusing the Elastic BeanStalk service. For this, we can use the SWAP URL\\nfeature is available with Elastic BeanStalk.\\nThe AWS whitepaper on Blue green deployments mentions the following:\\nYou can avoid this downtime by deploying the new version to a separate\\nenvironment. The existing environment’s configuration is copied and used to\\nlaunch the green environment with the new version of the application. The\\nnew—green—environment will have its own URL. When it is time to\\npromote the green environment to serve production traffic, you can use\\nElastic Beanstalk\'s Swap Environment URLs feature.\\nhttps://d0.awsstatic.com/whitepapers/AWS_Blue_Green_Deployments.pdf"},{"id":"336","question":"Your company’s application consisting of web servers and AWS\\nRDS is hosted on AWS. It is a read-heavy application and you are\\nnoticing a decrease in response time due to the load on RDS instance.\\nTo scale the database tier, which of the following measures would\\nyou take? (Choose 2)","options":{"A":"Use ElastiCache in front of your Amazon RDS DB to cache\\ncommon queries","B":"Use SQS to cache the database queries","C":"Use Auto-scaling to scale out and scale in the database tier","D":"Create Amazon DB Read Replica\'s. Configure the application\\nlayer to query the read replicas for query needs"},"answer":"A and D","explanation":"According to the AWS documentation; Amazon ElastiCache is\\na web service that makes it easy to deploy, operate, and scale an in-memory\\ndata store or cache in the cloud. The service improves the performance of\\nweb applications by allowing you to retrieve information from fast, managed,\\nin-memory data stores, instead of relying entirely on slower disk-based\\ndatabases.\\nAmazon RDS Read Replicas provide enhanced performance and durability\\nfor database (DB) instances. This replication feature makes it easy to\\nelastically scale out beyond the capacity constraints of a single DB Instance\\n\\n\\ffor read-heavy database workloads. You can create one or more replicas of a\\ngiven source DB Instance and serve high-volume application read traffic\\nfrom multiple copies of your data, thereby increasing aggregate read\\nthroughput. Read replicas can also be promoted when needed to become\\nstandalone DB instances.\\nhttps://aws.amazon.com/elasticache/\\nhttps://aws.amazon.com/rds/details/read-replicas/"},{"id":"337","question":"You are assigned with a task to automate the creation of EBS\\nsnapshots, which of the following is the best way to do this?","options":{"A":"Using Cloudwatch Events to trigger the snapshots of EBS\\nVolumes","B":"Using the AWS CodeDeploy service to create a snapshot of the\\nAWS Volumes","C":"Using the AWSConfig service to create a snapshot of the AWS\\nVolumes","D":"Creating a powershell script that uses the AWS CLI to get the\\nvolumes and then running the script as a cron job"},"answer":"A","explanation":"The best is to use the inbuilt service from CloudWatch, as\\nCloudWatch Events to automate the creation of EBS Snapshots. With Option\\nD, you would be restricted to running the PowerShell script on Windows\\nmachines and maintaining the script itself. And then, you have the overhead\\nof having a separate instance just to run that script.\\nThe AWS Documentation mentions; Amazon CloudWatch Events delivers a\\nnear real-time stream of system events that describe changes in Amazon Web\\nServices (AWS) resources. Using simple rules that you can quickly set up,\\nyou can match events and route them to one or more target functions or\\nstreams. CloudWatch Events becomes aware of operational changes as they\\noccur. CloudWatch Events responds to these operational changes and takes\\ncorrective action as necessary, by sending messages to respond to the\\nenvironment, activating functions, making changes, and capturing state\\ninformation.\\nhttps://docs.aws.amazon.com/AmazonCloudWatch/latest/events/WhatIsCloudWatchEvent"},{"id":"338","question":"One of your vendors who is also a user of AWS, requires access\\nto your AWS account in a way that the vendor should be able to read\\nprotected messages in a private S3 bucket as its leisure. Which of the\\nfollowing is the best way to accomplish this?","options":{"A":"Generate a signed S3 PUT URL and a signed S3 PUT URL,\\nboth with wildcard values and 2 year durations. Pass the URLs\\nto the vendor","B":"Create a cross-account IAM Role with permission to access the\\nbucket, and grant permission to use the Role to the vendor\\nAWS account","C":"Create an EC2 Instance Profile on your account. Grant the\\nassociated IAM role full access to the bucket. Start an EC2\\ninstance with this Profile and give SSH access to the instance\\nto the vendor","D":"Create an IAM User with API Access Keys. Grant the User\\npermissions to access the bucket. Give the vendor the AWS\\nAccess Key ID and AWS Secret Access Key for the User"},"answer":"B","explanation":"You can use AWS Identity and Access Management (IAM)\\nroles and AWS Security Token Service (STS) to set up cross-account access\\nbetween AWS accounts. When you assume an IAM role in another AWS\\naccount to obtain cross-account access to services and resources in that\\naccount, AWS CloudTrail logs the cross-account activity.\\nhttps://aws.amazon.com/blogs/security/tag/cross-account-access/"},{"id":"339","question":"Multiple development teams work in your organization on a\\nvariety of programming languages. The applications being developed\\nby these teams have a lot of dependencies. The company has planned\\nto move these development environments onto AWS. Which of the\\nfollowing is the best solution to make this move?","options":{"A":"Launch separate EC2 instances to host each application type\\nfor the developer community","B":"Use the OpsWorks service, crate a stack and create separate\\n\\n\\flayers for each application environment for the developer\\ncommunity","C":"Use the Elastic BeanStalk service and use Docker containers to\\nhost each application environment for the developer\\ncommunity","D":"Use the CloudFormation service to create Docker containers\\nfor each type of application"},"answer":"C","explanation":"The AWS documentation mentions the following; Elastic\\nBeanstalk supports the deployment of web applications from Docker\\ncontainers. With Docker containers, you can define your own runtime\\nenvironment. You can choose your own platform, programming language,\\nand any application dependencies (such as package managers or tools), that\\naren\'t supported by other platforms. Docker containers are self-contained and\\ninclude all the configuration information and software your web application\\nrequires to run.\\nhttps://docs.aws.amazon.com/elasticbeanstalk/latest/dg/create_deploy_docker.html"},{"id":"340","question":"Your company’s application is hosted on a set of EC2 servers\\nbehind an ELB. These EC2 servers are launched by an Auto-scaling\\ngroup. From the following options, select the example of Blue/Green\\ndeployments in AWS.","options":{"A":"Use the OpsWorks service to deploy your resources. Use 2\\nOpsWorks layers to deploy 2 versions of your application.\\nWhen the time comes for the switch, change to the alternate\\nlayer in the OpsWorks stack","B":"Re-deploy your application behind a load balancer that uses\\nAuto-scaling groups, create a new identical Auto-scaling\\ngroup, and associate it to the load balancer. During\\ndeployment, set the desired number of instances on the old\\nAuto-scaling group to zero, and when all instances have\\nterminated, delete the old Auto-scaling group","C":"Use the Elastic beanstalk service to deploy your resources. Use\\n2 Elastic beanstalk environments. Use Rolling deployments to\\n\\n\\fswitch between the environments","D":"Use a CloudFormation stack to deploy your resources. Use 2\\nCloudFormation stacks. Whenever you want to switch over,\\ndeploy and use the resources in the second CloudFormation\\nstack"},"answer":"B","explanation":"This deployment technique is discussed in an AWS\\nwhitepaper; As you scale up the green Auto-scaling group, you can take blue\\nAuto-scaling group instances out of service by either terminating them or\\nputting them in Standby state. Standby is a good option because if you need\\nto roll back to the blue environment, you only have to put your blue server\\ninstances back in service and they are ready to go. As soon as the green\\ngroup is scaled up without issues, you can decommission the blue group by\\nadjusting the group size to zero. If you need to roll back, detach the load\\nbalancer from the green group or reduce the group size of the green group to\\nzero.\\nhttps://d0.awsstatic.com/whitepapers/AWS_Blue_Green_Deployments.pdf"},{"id":"341","question":"What are the wait states that occur during the scale in and scale\\nout process when you have added lifecycle hooks to an auto scaling\\ngroup? (Choose 2)","options":{"A":"Terminating:Wait","B":"Pending:Wait","C":"Launching:Wait","D":"Exitting:Wait"},"answer":"A and B","explanation":"As per AWS documentation; After you add lifecycle hooks to\\nyour Auto-scaling group, they work as follows:\\nAuto-scaling responds to scale out events by launching instances and\\nscale in events by terminating instances\\nAuto-scaling puts the instance into a wait state (Pending:Wait\\norTerminating:Wait). The instance is paused until either you tell Autoscaling to continue or the timeout period ends\\n\\n\\fhttps://docs.aws.amazon.com/autoscaling/ec2/userguide/lifecycle-hooks.html"},{"id":"342","question":"As a DevOps engineer, you are using OpsWorks stack to rollout\\na collection of web servers for your company. When the instances are\\nlaunched, a configuration file is needed to be set up prior to the\\nlaunching of the web application hosted on these instances. To fulfil\\nthis requirement, which of the following steps would you take?\\n(Choose 2)","options":{"A":"Configure a recipe that sets the configuration file and add it to\\nthe Deploy LifeCycle Event of the specific web layer","B":"Configure a recipe that sets the configuration file and add it to\\nthe Configure LifeCycle Event of the specific web layer","C":"Ensure that the OpsWorks stack is changed to use custom\\ncookbooks","D":"Ensure that the OpsWorks stack is changed to use the AWS\\nspecific cookbooks"},"answer":"B and C","explanation":"This is mentioned in the AWS documentation:\\nConfigure\\nThis event occurs on all of the stack\'s instances when one of the following\\noccurs:\\nAn instance enters or leaves the online state\\nYou associate an Elastic IP address with an instance or disassociate\\none from an instance\\nYou attach an Elastic Load Balancing load balancer to a layer, or\\ndetach one from a layer\\nFor example, suppose that your stack has instances A, B, and C, and you start\\na new instance, D. After D has finished running its setup recipes, AWS\\nOpsWorks Stacks triggers the Configure event on A, B, C, and D. If you\\nsubsequently stop A, AWS OpsWorks Stacks triggers the Configure event on\\nB, C, and D. AWS OpsWorks Stacks responds to the Configure event by\\nrunning each layer\'s Configure recipes, which update the instances\'\\nconfiguration to reflect the current set of online instances. The Configure\\nevent is therefore a good time to regenerate configuration files. For example,\\n\\n\\fthe HAProxy Configure recipes reconfigure the load balancer to\\naccommodate any changes in the set of online application server instances.\\nYou can also manually trigger the Configure event by using the Configure\\nstack command.\\nhttps://docs.aws.amazon.com/opsworks/latest/userguide/workingcookbookevents.html"},{"id":"343","question":"For testing an application, the development team has requested\\nyou to create a CloudFormation template. They have requested that\\nwhen the stack is deleted, the database should be preserved. How can\\nyou do this with CloudFormation?","options":{"A":"In the AWS CloudFormation template, set the\\nAWS::RDS::DBInstance’s DBlnstanceClass property to be\\nread-only","B":"In the AWS CloudFormation template, set the WaitPolicy of\\nthe AWS::RDS::DBInstance’s WaitPolicy property to “Retain”","C":"In the AWS CloudFormation template, set the DeletionPolicy\\nof theAWS::RDS::DBInstance’s DeletionPolicy property to\\n“Retain”","D":"Ensure that the RDS is created with Read Replica\'s so that the\\nRead Replica remains after the stack is torn down"},"answer":"C","explanation":"With the DeletionPolicy attribute you can preserve or (in some\\ncases) backup a resource when its stack is deleted. You specify a\\nDeletionPolicy attribute for each resource that you want to control. If a\\nresource has no DeletionPolicy attribute, AWS CloudFormation deletes the\\nresource by default. Note that this capability also applies to update operations\\nthat lead to resources being removed.\\nhttps://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/awsattribute-deletionpolicy.html"},{"id":"344","question":"Your company has migrated to AWS. It had an Active Directory\\nsetup in the on-premises setup and wants to use the same AD for\\nauthentication. Which AWS service can help you in continuing the\\nuse of the credentials of on-premises AD for authenticating AWS\\n\\n\\fresources such as AWS WorkSpaces?","options":{"A":"The ClassicLink feature on AWS","B":"The Active Directory connector service on AWS","C":"The AWS Simple AD service","D":"The Active Directory service on AWS"},"answer":"B","explanation":"As per AWS documentation, AD Connector is a directory\\ngateway with which you can redirect directory requests to your on-premises\\nMicrosoft Active Directory without caching any information in the cloud.\\nAD Connector comes in two sizes, small and large. A small AD Connector is\\ndesigned for smaller organizations of up to 500 users. A large AD Connector\\ncan support larger organizations of up to 5,000 users.\\nhttps://docs.aws.amazon.com/directoryservice/latest/adminguide/directory_ad_connector.html"},{"id":"345","question":"Your log analysis mechanism takes ten days to generate a report\\nof top ten visitors of your web application. It is now required to\\nimplement a system that can report this information in real time,\\nensuring that the report is always up to date, and can handle the\\nincrease in number of requests to your application. How can this\\nrequirement be fulfilled in the most cost effective way?","options":{"A":"Create a multi-AZ Amazon RDS MySQL cluster, post the\\nlogging data to MySQL, and run a map reduce job to retrieve\\nthe required information on user counts","B":"Configure an Auto-scaling group to increase the size of your\\nAmazon EMR cluster","C":"Post your log data to an Amazon Kinesis data stream, and\\nsubscribe your log-processing application so that is configured\\nto process your logging data","D":"Publish your log data to an Amazon S3 bucket. Use AWS\\nCloudFormation to create an Auto-scaling group to scale your\\npost-processing application that is configured to pull down\\nyour log files stored in Amazon S3","E":"Publish your data to CloudWatch Logs, and configure your\\napplication to auto-scale to handle the load on demand"},"answer":"C","explanation":"The AWS documentation mentions the following; Amazon\\nKinesis makes it easy to collect, process, and analyze real-time, streaming\\ndata so you can get timely insights and react quickly to new information.\\nAmazon Kinesis offers key capabilities to cost effectively process streaming\\ndata at any scale, along with the flexibility to choose the tools that best suit\\nthe requirements of your application. With Amazon Kinesis, you can ingest\\nreal-time data such as application logs, website clickstreams, IoT telemetry\\ndata, and more into your databases, data lakes and data warehouses, or build\\nyour own real-time applications using this data. Amazon Kinesis enables you\\nto process and analyze data as it arrives and respond in real-time instead of\\nhaving to wait until all your data is collected before the processing can begin.\\nhttps://aws.amazon.com/kinesis/"},{"id":"346","question":"You have multiple applications hosted on AWS and it is\\nrequired to store the logs from these applications in a durable storage.\\nAfter three months, these logs can be moved to archval storage.\\nWhich of the following steps would you carry out to meet this\\nrequirement? (Choose 2)","options":{"A":"Use Lifecycle policies to move the data onto Amazon Simple\\nStorage service after a period of 3 months","B":"Use Lifecycle policies to move the data onto Amazon Glacier\\nafter a period of 3 months","C":"Store the log files as they emitted from the application on to\\nAmazon Simple Storage service","D":"Store the log files as they emitted from the application on to\\nAmazon Glacier"},"answer":"B and C","explanation":"Lifecycle configuration enables you to specify the lifecycle\\nmanagement of objects in a bucket. The configuration is a set of one or more\\nrules, where each rule defines an action for Amazon S3 to apply to a group of\\nobjects. These actions can be classified as follows:\\n\\n\\f-\\n\\nTransition Actions – In which you define when objects transition to\\nanother storage class. For example, you may choose to transition\\nobjects to the STANDARD_IA (IA, for infrequent access) storage class\\n30 days after creation, or archive objects to the GLACIER storage class\\none year after creation\\nExpiration Actions – In which you specify when the objects expire.\\nThen Amazon S3 deletes the expired objects on your behalf\\nhttps://docs.aws.amazon.com/AmazonS3/latest/dev/object-lifecyclemgmt.html\\nThe AWS documentation also mentions the following; Amazon Simple\\nStorage Service (Amazon S3) makes it simple and practical to collect, store,\\nand analyze data - regardless of format – all at massive scale. S3 is object\\nstorage built to store and retrieve any amount of data from anywhere – web\\nsites and mobile apps, corporate applications, and data from IoT sensors or\\ndevices.\\nhttps://aws.amazon.com/s3/"},{"id":"347","question":"One of your applications is running in us-west-2 region and it\\nrequires 6 EC2 instances running all the time. Which of the following\\ndeployment type will provide you 100% fault tolerance if any AZ out\\nof the three AZs in us-west-2 becomes unavailable? (Choose 2)","options":{"A":"us-west-2a with 3 instances, us-west-2b with 3 instances, uswest-2c with 3 instances","B":"us-west-2a with 6 instances, us-west-2b with 6 instances, uswest-2c with 0 instances","C":"us-west-2a with 4 instances, us-west-2b with 2 instances, uswest-2c with 2 instances","D":"us-west-2a with 3 instances, us-west-2b with 3 instances, uswest-2c with 0 instances","E":"us-west-2a with 2 instances, us-west-2b with 2 instances, uswest-2c with 2 instances"},"answer":"A and B","explanation":"Since you need 6 instances running all the time, only A and B\\ncan fulfill this requirement.\\n\\n\\fAWS documentation says the following about Availability Zones; When you\\nlaunch an instance, you can select an Availability Zone or let us choose one\\nfor you. If you distribute your instances across multiple Availability Zones\\nand one instance fails, you can design your application so that an instance in\\nanother Availability Zone can handle requests.\\nhttps://docs.aws.amazon.com/AWSEC2/latest/UserGuide/using-regionsavailability-zones.html"},{"id":"348","question":"One of your instances is reporting an unhealthy system status\\ncheck. It is something you are not responsible to monitor and repair\\non your own. In your AWS environment, how might you automate\\nthe repair of the system status check failure?","options":{"A":"Implement a third-party monitoring tool","B":"Write a script that periodically shuts down and starts instances\\nbased on certain stats","C":"Create CloudWatch alarms for StatuscheckFailed_System\\nmetrics and select EC2 action. Recover the instance","D":"Write a script that queries the EC2 API for each instance status\\ncheck"},"answer":"C","explanation":"Using Amazon CloudWatch alarm actions, you can create\\nalarms that automatically stop, terminate, reboot, or recover your EC2\\ninstances. You can use the stop or terminate actions to help you save money\\nwhen you no longer need an instance to be running. You can use the reboot\\nand recover actions to automatically reboot those instances or recover them\\nonto new hardware if a system impairment occurs.\\nhttps://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/UsingAlarmActions."},{"id":"349","question":"Your company’s production web servers are running on three\\nreserved EC2 instances with EBS as root volumes. These instances\\nhave a consistent CPU load of 80%. An ELB distributes traffic to\\nthese instances. They also have production and development MultiAZ RDS MySQL databases. This is a mission critical system. What\\nwould you recommend for cost reduction without affecting the\\navailability?","options":{"A":"Consider removing the ELB","B":"Consider using spot instances","C":"Consider not using Multi-AZ RDS deployment for the\\ndevelopment database","D":"Consider using on-demand instances"},"answer":"C","explanation":"Multi-AZ databases is better for production environments\\nrather than for development environments, so you can reduce costs by not\\nusing this for development environments.\\nAmazon RDS Multi-AZ deployments provide enhanced availability and\\ndurability for Database (DB) Instances, making them a natural fit for\\nproduction database workloads. When you provision a Multi-AZ DB\\nInstance, Amazon RDS automatically creates a primary DB Instance and\\nsynchronously replicates the data to a standby instance in a different\\nAvailability Zone (AZ). Each AZ runs on its own physically distinct,\\nindependent infrastructure, and is engineered to be highly reliable. In case of\\nan infrastructure failure, Amazon RDS performs an automatic failover to the\\nstandby (or to a read replica in the case of Amazon Aurora) so that you can\\nresume database operations as soon as the failover is complete. Since the\\nendpoint for your DB Instance remains the same after a failover, your\\napplication can resume database operation without the need for manual\\nadministrative intervention.\\nhttps://aws.amazon.com/rds/details/multi-az/"},{"id":"350","question":"You have production and development instances running in your\\nVPC. To ensure better security, you want to restrict access to\\nproduction instances for the people working on development\\ninstances. Which of the following would be the best way to\\naccomplish this using policies?","options":{"A":"Define the tags on the test and production servers and add a\\ncondition to the IAM policy that allows access to specific tags","B":"Launch the test and production instances in different\\nAvailability Zones and use Multi-Factor Authentication","C":"Create an IAM policy with a condition, which allows access to\\n\\n\\fonly instances that are used for production or development","D":"Launch the test and production instances in separate VPCs and\\nuse VPC peering"},"answer":"A","explanation":"You can easily add tags that define which instances are\\nproduction and which are development instances and then ensure these tags\\nare used when controlling access via an IAM policy.\\nhttps://docs.aws.amazon.com/AWSEC2/latest/UserGuide/Using_Tags.html"},{"id":"351","question":"Your development team is working on an application that access\\nresources in AWS. The users of this application will be logging in via\\nGoogle and Facebook. Which of the following AWS mechanisms\\nwould you imply to authenticate users with Google or Facebook?","options":{"A":"Use AWS policies","B":"Use Web Identity Federation","C":"This is not possible","D":"Modify bucket policy on website bucket to be able to access\\nCSS bucket"},"answer":"B","explanation":"You can directly configure individual identity providers to\\naccess AWS resources using web identity federation. AWS currently\\nsupports authenticating users using web identity federation through several\\nidentity providers:\\nLogin with Amazon\\nFacebook login\\nGoogle Sign-in\\nhttps://docs.aws.amazon.com/sdk-for-javascript/v2/developer-guide/loadingbrowser-credentials-federated-id.html"},{"id":"352","question":"As a DevOps engineer, you are given a task to log each time an\\ninstance is scaled in or scaled out from an auto scaling group. Which\\nof the following steps would you carry out to fulfil the requirement\\nknowing that each option forms part of the solution. (Choose 2)","options":{"A":"Create a CloudWatch event that will trigger an SQS queue","B":"Create an SQS queue that will write the event to CloudWatch\\nlogs","C":"Create a CloudWatch event that will trigger a Lambda function","D":"Write a Lambda function that will write the event to\\nCloudWatch logs"},"answer":"C and D","explanation":"You can run an AWS Lambda function that logs an event\\nwhenever an Auto-scaling group launches or terminates an Amazon EC2\\ninstance and whether the launch or terminate event was successful.\\nhttps://docs.aws.amazon.com/AmazonCloudWatch/latest/events/LogASGroupState.html"},{"id":"353","question":"An Elastic BeanStalk environment was being used by your\\ndevelopment team. After a week, the environment was discarded and\\na new one was created. Upon trying to access the data on the older\\nenvironment, it was not available. Why do you think it happened?","options":{"A":"This is because before the environment termination, Elastic\\nbeanstalk copies the data to DynamoDB, and hence the data is\\nnot present in the EBS volumes","B":"This is because the underlying EC2 Instances are created with\\nno persistent local storage","C":"This is because the underlying EC2 Instances are created with\\nIOPS volumes and cannot be accessed once the environment\\nhas been terminated","D":"This is because the underlying EC2 Instances are created with\\nencrypted storage and cannot be accessed once the\\nenvironment has been terminated"},"answer":"B","explanation":"According to the AWS documentation; Elastic Beanstalk\\napplications run on Amazon EC2 instances that have no persistent local\\nstorage. When the Amazon EC2 instances terminate, the local file system is\\nnot saved, and new Amazon EC2 instances start with a default file system.\\n\\n\\fYou should design your application to store data in a persistent data source.\\nhttps://docs.aws.amazon.com/elasticbeanstalk/latest/dg/concepts.concepts.design.html"},{"id":"354","question":"Your company has a set of EC2 instances hosted in AWS that\\nare launched by an Auto-scaling group. The load on the servers is\\ncausing loss of requests; even though, auto scaling is launching more\\ninstances to manage the load but some requests are still getting lost.\\nWhich of the following can give you the most cost effective solution\\nto avoid loss of recently submitted requests?","options":{"A":"Pre-warm your ELB","B":"Use larger instances for your application","C":"Use an SQS queue to decouple the application components","D":"Keep one extra EC2 instance always powered on in case a\\nspike occurs"},"answer":"C","explanation":"Amazon Simple Queue Service (SQS) is a fully-managed\\nmessage queuing service for reliably communicating among distributed\\nsoftware components and microservices - at any scale. Building applications\\nfrom individual components that each perform a discrete function improves\\nscalability and reliability, and is best practice design for modern applications.\\nhttps://aws.amazon.com/sqs/"},{"id":"355","question":"Your company is concerned with EBS volume backup on EC2\\nand wants a proper backup solution with guaranteed durability of\\nbackup data. Which of the following solutions would you implement\\nand why?","options":{"A":"Write a cronjob that uses the AWS CLI to take a snapshot of\\nproduction EBS volumes. The data is durable because EBS\\nsnapshots are stored on the Amazon S3 standard storage class","B":"Use a lifecycle policy to back up EBS volumes stored on\\nAmazon S3 for durability","C":"Write a cronjob on the server that compresses the data that\\n\\n\\fneeds to be backed up using gzip compression, then use AWS\\nCLI to copy the data into an S3 bucket for durability","D":"Configure Amazon Storage Gateway with EBS volumes as the\\ndata source and store the backups on premise through the\\nstorage gateway"},"answer":"A","explanation":"You can take snapshots of EBS volumes and to automate the\\nprocess, you can use the CLI. The snapshots are automatically stored on S3\\nfor durability.\\nhttps://docs.aws.amazon.com/AWSEC2/latest/UserGuide/EBSSnapshots.html"},{"id":"356","question":"As a DevOps engineer, one of your responsibilities is to create\\nCloudFormation templates for your company. It is required to have\\nan S3 bucket to store logs of all the development resources. How will\\nyou achieve this?","options":{"A":"By using the metadata section in the CloudFormation template\\nto decide on whether to create the S3 bucket or not","B":"By creating an S3 bucket from before and then just providing\\naccess based on the tag value mentioned in the CloudFormation\\ntemplate","C":"By creating a parameter in the CloudFormation template and\\nthen using the Condition clause in the template to create an S3\\nbucket if the parameter has a value of development","D":"By creating separate CloudFormation templates for\\nDevelopment and production"},"answer":"C","explanation":"You might use conditions when you want to reuse a template\\nthat can create resources in different contexts, such as a test environment\\nversus a production environment. In your template, you can add an\\nEnvironmentType input parameter, which accepts either prod or test as\\ninputs. For the production environment, you might include Amazon EC2\\ninstances with certain capabilities; however, for the test environment, you\\nwant to use reduced capabilities to save money. With conditions, you can\\ndefine which resources are created and how they are configured for each\\n\\n\\fenvironment type.\\nhttps://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/conditionssection-structure.html"},{"id":"357","question":"You are writing a CloudFormation template to create 2 EC2\\ninstances behind an ELB. You want the DNS of the load balancer to\\nbe returned upon creation of the stack. Which section of the template\\nwould do this for you?","options":{"A":"Mappings","B":"Outputs","C":"Parameters","D":"Resources"},"answer":"B","explanation":"The below example shows a simple CloudFormation template.\\nIt creates an EC2 instance based on the AMI - ami-d6f32ab"},{"id":"5","question":"When the\\ninstance is created, it will output the AZ in which it is created.\\n{\\n“Resources”: {\\n“MyEC2Instance”: {\\n\\"Type\\": \\"AWS::EC2::Instance\\",\\n\\"Properties\\": {\\n\\"ImageId\\": \\"ami-d6f32ab5\\"\\n}\\n}\\n}\\n“Outputs”: {\\n“Availability”: {\\n“Description”: “The Instance ID”.\\n“Value”:\\n{ \\"Fn::GetAtt\\": [ \\"MyEC2Instance\\", \\"AvailabilityZone\\"]}\\n}\\n}\\n}\\n\\n\\fFor more information on CloudFormation, please visit the given URL:\\nhttps://aws.amazon.com/cloudformation/\\n358. Your AWS infrastructure consists of EC2 instances behind an\\nELB and the instances are launched and terminated by an Autoscaling group. You also have a AWS RDS MySQL database. Which\\nof the following can be used to take you one step further towards a\\nself-healing architecture?","options":{"A":"Create one more Auto-scaling group in another region for fault\\ntolerance","B":"Create one more ELB in another region for fault tolerance","C":"Enable Multi-AZ feature for the AWS RDS database","D":"Enable Read Replica\'s for the AWS RDS database"},"answer":"C","explanation":"As per AWS documentation, Amazon RDS Multi-AZ\\ndeployments provide enhanced availability and durability for Database (DB)\\nInstances, making them a natural fit for production database workloads.\\nWhen you provision a Multi-AZ DB Instance, Amazon RDS automatically\\ncreates a primary DB Instance and synchronously replicates the data to a\\nstandby instance in a different Availability Zone (AZ). Each AZ runs on its\\nown physically distinct, independent infrastructure, and is engineered to be\\nhighly reliable. In case of an infrastructure failure, Amazon RDS performs an\\nautomatic failover to the standby (or to a read replica in the case of Amazon\\nAurora), so that you can resume database operations as soon as the failover is\\ncomplete. Since the endpoint for your DB Instance remains the same after a\\nfailover, your application can resume database operation without the need for\\nmanual administrative intervention.\\nhttps://aws.amazon.com/rds/details/multi-az/"},{"id":"359","question":"You are storing sensitive data on AWS, which of the following\\nsteps should you take? (Choose 3)","options":{"A":"Enable S3 Encryption","B":"Enable EBS Encryption","C":"With AWS, you do not need to worry about encryption","D":"Encrypt the file system on an EBS volume using Linux tools\\nAnswer: A, B, and D\\nExplanation: Data protection refers to protecting data while in-transit (as it\\ntravels to and from Amazon S3) and at rest (while it is stored on disks in\\nAmazon S3 data centers). You can protect data in transit by using SSL or by\\nusing client-side encryption.\\nhttps://docs.aws.amazon.com/AmazonS3/latest/dev/UsingEncryption.html\\nAmazon EBS encryption offers you a simple encryption solution for your\\nEBS volumes without the need for you to build, maintain, and secure your\\nown key management infrastructure. When you create an encrypted EBS\\nvolume and attach it to a supported instance type, the following types of data\\nare encrypted:\\nData at rest inside the volume\\nAll data moving between the volume and the instance\\nAll snapshots created from the volume\\nhttps://docs.aws.amazon.com/AWSEC2/latest/UserGuide/EBSEncryption.html\\n360. The company that you work for is a startup and current funding\\nhas left it short on cash. It is not possible for your company to buy\\nthousands of dollar of storage hardware for its application that\\nreceives huge amount of data. It has opted to use AWS. Which\\nservices would you implement to store unlimited amount of data\\nwithout any effort to scale when demand unexpectedly increases?\\nA. Amazon EC2, because EBS volumes can scale to hold any\\namount of data and, when used with Auto-scaling, can be\\ndesigned for fault tolerance and high availability\\nB. Amazon S3, because it provides unlimited amounts of storage\\ndata, scales automatically, is highly available, and durable\\nC. Amazon Glacier, to keep costs low for storage and scale\\ninfinitely\\nD. Amazon Import/Export, because Amazon assists in migrating\\nlarge amounts of data to Amazon S3"},"answer":"B","explanation":"The best option is to use S3 because you can host a large\\namount of data in S3 and is the best storage option provided by AWS.\\nhttps://docs.aws.amazon.com/AmazonS3/latest/dev/Welcome.html"},{"id":"361","question":"When reviewing your auto-scaling events, you have noticed that\\nthe application is scaling up and down multiple times in the same\\nhour, what design choice could you make to preserve elasticity with\\noptimal cost? (Choose 2)","options":{"A":"Modify the Auto-scaling group cool down timers","B":"Modify the CloudWatch alarm period that triggers your Autoscaling scale down policy","C":"Modify the Auto-scaling group termination policy to terminate\\nthe newest instance first","D":"Modify the Auto-scaling policy to use scheduled scaling\\nactions"},"answer":"A and B","explanation":"The Auto-scaling cooldown period is a configurable setting for\\nyour Auto-scaling group that helps to ensure that Auto-scaling does not\\nlaunch or terminate additional instances before the previous scaling activity\\ntakes effect. After the Auto-scaling group dynamically scales using a simple\\nscaling policy, Auto-scaling waits for the cooldown period to complete\\nbefore resuming scaling activities. When you manually scale your Autoscaling group, the default is not to wait for the cooldown period, but you can\\noverride the default and honor the cooldown period. Note that, if an instance\\nbecomes unhealthy, Auto-scaling does not wait for the cooldown period to\\ncomplete before replacing the unhealthy instance.\\nhttps://docs.aws.amazon.com/autoscaling/ec2/userguide/Cooldown.html\\nYou can also modify the CloudWatch triggers to ensure the thresholds are\\nappropriate for the scale down policy.\\nhttps://docs.aws.amazon.com/autoscaling/ec2/userguide/as-scale-based-ondemand.html"},{"id":"362","question":"Your startup company is going to launch the website in the\\n\\n\\fcoming week. Chances are that the traffic will be very high in the\\nbeginning couple of weeks. If a load failure occurs, how can you set\\nup DNS failover to a static website?","options":{"A":"By adding more servers in case the application fails","B":"By using Route 53 with failover option to failover to a static S3\\nwebsite bucket or CloudFront distribution","C":"By enabling failover to an on-premises data center to the app\\nhosted there","D":"By duplicating the exact application architecture in another\\nregion and configuring DNS weight-based routing"},"answer":"B","explanation":"Amazon Route53 health checks monitor the health and\\nperformance of your web applications, web servers, and other resources.\\nIf you have multiple resources that perform the same function, you can\\nconfigure DNS failover so that Amazon Route53 will route your traffic from\\nan unhealthy resource to a healthy resource. For example, if you have two\\nweb servers and one web server becomes unhealthy, Amazon Route53 can\\nroute traffic to the other web server. So you can route traffic to a website\\nhosted on S3 or to a CloudFront distribution.\\nhttps://docs.aws.amazon.com/Route53/latest/DeveloperGuide/dnsfailover.html"},{"id":"363","question":"For your organization need, you created a CloudFormation\\ntemplate to launch EC2 instances for both production and\\ndevelopment environments in the same region. Each of the instance\\nwill have an Elastic IP and a security group associated with it. The\\ndevelopment CloudFormation stack was created successfully but the\\nproduction stack creation failed. What could be the reason?","options":{"A":"You did not choose the Production version of the AMI you are\\nusing when creating the production stack","B":"You hit the soft limit for security groups when creating the\\ndevelopment environment","C":"You hit the soft limit of 5 EIPs per region when creating the\\n\\n\\fdevelopment environment","D":"You have chosen the wrong tags when creating the instances in\\nboth environments"},"answer":"C","explanation":"The most viable reason could be that you reached the limit for\\nthe number of Elastic IPs in the region. You can find more information about\\nthe EC2 service limits on the given URL:\\nhttps://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ec2-resourcelimits.html"},{"id":"364","question":"For DR purposes, you have been assigned a task to build a\\nduplicate environment in another region. A portion of your\\nenvironment consists of EC2 instances with preconfigured software.\\nWhich of the below options will help you in configuring instances in\\nanother region?","options":{"A":"Make the EC2 instance shareable among other regions through\\nIAM permissions","B":"Create an AMI of the EC2 instance","C":"Create an AMI of the EC2 instance and copy the AMI to the\\ndesired region","D":"None of the above"},"answer":"C","explanation":"You can copy an Amazon Machine Image (AMI) within or\\nacross an AWS region using the AWS Management Console, the AWS\\ncommand line tools or SDKs, or the Amazon EC2 API, all of which support\\nthe CopyImage action. You can copy both Amazon EBS-backed AMIs and\\ninstance store-backed AMIs. You can copy AMIs with encrypted snapshots\\nand encrypted AMIs.\\nhttps://docs.aws.amazon.com/AWSEC2/latest/UserGuide/CopyingAMIs.html"},{"id":"365","question":"Your web application is running on six EC2 instances in an\\nAuto-scaling group and it consumes about 45% of the resources on\\neach instance. The number of requests to this application is consistent\\nand does not experiences spikes. This is a mission-critical application\\n\\n\\fand you want high availability at all times. You want to evenly\\ndistribute the load between all instances. You also want to use the\\nsame AMI for all instances. Which of the following architectural\\nchoices should you make?","options":{"A":"Deploy 2 EC2 instances in three regions and use Amazon\\nElastic Load Balancer","B":"Deploy 3 EC2 instances in one availability zone and 3 in\\nanother availability zone and use Amazon Elastic Load\\nBalancer","C":"Deploy 3 EC2 instances in one region and 3 in another region\\nand use Amazon Elastic Load Balancer","D":"Deploy 6 EC2 instances in one availability zone and use\\nAmazon Elastic Load Balancer"},"answer":"B","explanation":"For Option A and C, the ELB is designed to only run in one\\nregion in AWS and not across multiple regions. So these options are wrong.\\nOption D is automatically incorrect because remember that the question asks\\nfor high availability. For option A, if the AZ goes down then the entire\\napplication fails.\\nSo for high availability and maintaining the number of instances, option B is\\ncorrect. For more information on regions and AZs, visit the following URL\\nhttps://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/Concepts.RegionsAndAvaila"},{"id":"366","question":"You have a set of web servers to host a web application on\\nAWS. This app is used by a section of users and you want to monitor\\nthe number errors occurred when using the web application. Which of\\nthe following can be used to do this? (Choose 3)","options":{"A":"Increase a metric filter in Cloudwatch whenever the pattern is\\nmatched","B":"Search for the keyword “ERROR” in Cloudwatch logs","C":"Search for the keyword “ERROR” in the log files on the server","D":"Send the logs from the instances onto Cloudwatch logs\\n\\n\\fAnswer: A, B, and D\\nExplanation: The AWS documentation mentions the following; You use\\nmetric filters to search for and match terms, phrases, or values in your log\\nevents. When a metric filter finds one of the terms, phrases, or values in your\\nlog events, you can increase the value of a CloudWatch metric. For example,\\nyou can create a metric filter to search for and count the occurrence of the\\nword.\\nhttps://docs.aws.amazon.com/AmazonCloudWatch/latest/logs/FilterAndPatternSyntax.htm\\n367. Your organization’s web application is hosted on an Elastic\\nBeanStalk environment and it is instructed that whenever application\\nchanges occur, and newer versions need to be deployed; the fastest\\ndeployment approach should be used. Which of the following\\ndeployment mechanisms fulfills this requirement?\\nA. Rolling with batch\\nB. Immutable\\nC. Rolling\\nD. All at once"},"answer":"D","explanation":"The requirement is to deploy the new version as fast as\\npossible, according to AWS documentation, All at Once deployment\\nmechanism is the fastest approach of deployment.\\nhttps://docs.aws.amazon.com/elasticbeanstalk/latest/dg/usingfeatures.deploy-existing-version.html"},{"id":"368","question":"You are in charge of designing a CloudFormation template that\\ndeploys a LAMP stack. After deploying a stack, you notice that the\\nApache server is not up and running and is experiencing issues while\\nstarting up. However, the status of the stack is showing as\\nCRETAE_COMPLETE. You want all resources defined in the stack\\nto be up and running when the stack shows CREAT_COMPLETE\\nstatus. How can you achieve this? (Choose 2)","options":{"A":"By using the CFN helper scripts to signal once the resource\\nconfiguration is complete","B":"By using the CreationPolicy to ensure it is associated with the\\nEC2 Instance resource","C":"By using lifecycle hooks to mark the completion of the creation\\nand configuration of the underlying resource","D":"By defining a stack policy, which defines that all underlying\\nresources should be up and running before showing a status of\\nCREATE_COMPLETE"},"answer":"A and B","explanation":"When you provision an Amazon EC2 instance in an AWS\\nCloudFormation stack, you might specify additional actions to configure the\\ninstance, such as install software packages or bootstrap applications.\\nNormally, CloudFormation proceeds with stack creation after the instance\\nhas been successfully created. However, you can use a CreationPolicy so that\\nCloudFormation proceeds with stack creation only after your configuration\\nactions are done. That way you will know your applications are ready to go\\nafter stack creation succeeds.\\nhttps://aws.amazon.com/blogs/devops/use-a-creationpolicy-to-wait-for-oninstance-configurations/"},{"id":"369","question":"Your company’s application is hosted on AWS on EC2\\ninstances. The IT security department has given a requirement to\\nprocess and analyze the logs of these instances in real time. Which of\\nthe following can be used for this?","options":{"A":"Another EC2 Instance with a larger instance type to process the\\nlogs","B":"Amazon S3 to store the logs and then Amazon Kinesis to\\nprocess and analyze the logs in real time","C":"Amazon Glacier to store the logs and then Amazon Kinesis to\\nprocess and analyze the logs in real time","D":"Cloudwatch logs to process and analyze the logs in real time"},"answer":"B","explanation":"The AWS documentation says:\\nReal-time Metrics and Reporting\\nYou can use data collected into Kinesis Streams for simple data analysis and\\n\\n\\freporting in real time. For example, your data-processing application can\\nwork on metrics and reporting for system and application logs as the data is\\nstreaming in, rather than waiting to receive batches of data.\\nReal-time Data Analytics\\nThis combines the power of parallel processing with the value of real-time\\ndata. For example, process website clickstreams in real time, and then\\nanalyze site usability engagement using multiple different Kinesis Streams\\napplications running in parallel.\\nAmazon Glacier is meant for Archival purposes and should not be used for\\nstoring the logs for real time processing.\\nhttps://docs.aws.amazon.com/streams/latest/dev/introduction.html"},{"id":"370","question":"Which of the following deployment types are available in AWS\\nCodeDeploy? (Choose 2)","options":{"A":"Blue/Green Deployments","B":"Immutable Deployments","C":"Rolling Deployments","D":"In-place Deployments"},"answer":"A and D","explanation":"The AWS documentation mentions the following:\\nDeployment Type: The method used to make the latest application revision\\navailable on instances in a deployment group.\\n-\\n\\n-\\n\\nIn-place Deployment: The application on each instance in the\\ndeployment group is stopped, the latest application revision is installed,\\nand the new version of the application is started and validated. You can\\nchoose to use a load balancer so each instance is deregistered during its\\ndeployment and then restored to service after the deployment is\\ncomplete\\nBlue/Green Deployment: The instances in a deployment group (the\\noriginal environment) are replaced by a different set of instances (the\\nreplacement environment) using these steps:\\nInstances are provisioned for the replacement environment\\nThe latest application revision is installed on the replacement\\n\\n\\finstances\\nAn optional wait time occurs for activities such as\\napplication testing and system verification\\nInstances in the replacement environment are registered with\\nan Elastic Load Balancing load balancer, causing traffic to be\\nrerouted to them. Instances in the original environment are\\nderegistered and can be terminated or kept running for other\\nuses\\nhttps://docs.aws.amazon.com/codedeploy/latest/userguide/primarycomponents.html"},{"id":"371","question":"Your company has a number of applications that need to be\\nmigrated to AWS. Initially you thought to move these to Elastic\\nBeanStalk service, but you noticed that the underlying platform\\nservice is not an option in the Elastic BeanStalk environment. Which\\nof the following options can be used to move your applications to\\nElastic BeanStalk?","options":{"A":"Create a Docker container for the custom application and then\\ndeploy it to Elastic BeanStalk","B":"Use custom CloudFormation templates to deploy the\\napplication into Elastic BeanStalk","C":"Use custom Chef recipes to deploy your application in Elastic\\nBeanStalk","D":"Use the OpsWorks service to create a stack. In the stack, create\\na separate custom layer. Deploy the application to this layer\\nand then attach the layer to Elastic BeanStalk"},"answer":"A","explanation":"Elastic Beanstalk supports the deployment of web applications\\nfrom Docker containers. With Docker containers, you can define your own\\nruntime environment. You can choose your own platform, programming\\nlanguage, and any application dependencies (such as package managers or\\ntools), that are not supported by other platforms. Docker containers are selfcontained and include all the configuration information and software your\\nweb application requires to run.\\n\\n\\fhttps://docs.aws.amazon.com/elasticbeanstalk/latest/dg/create_deploy_docker.html"},{"id":"372","question":"Your web application is hosted on EC2 instances in an Autoscaling group that are sitting behind an ELB. You have created AMIs\\nof each application version for deployment and you want to deploy\\nthe newer version of your application using Blue/Green deployment\\ntechnique. You have chosen the Blue/Green deployment model\\nbecause you want to migrate users in a controlled manner while the\\nsize of the fleet remains constant for at least six hours to ensure that\\nthere are no issues in the new version. What would you do to enable\\nthis technique while being able to roll back easily? (Choose 2)","options":{"A":"Configure Elastic Load Balancing to vary the proportion of\\nrequests sent to instances running the two application versions","B":"Use Amazon Route53 weighted Round Robin to vary the\\nproportion of requests sent to the load balancers","C":"Create an Auto-scaling launch configuration with the new AMI\\nto use the new launch configuration and to register instances\\nwith the existing load balancer","D":"Create an Auto-scaling launch configuration with the new AMI\\nto use the new launch configuration and to register instances\\nwith the new load balancer"},"answer":"B and D","explanation":"The AWS documentation describes Blue/Green deployment as\\nthe following; You can shift traffic all at once or you can do a weighted\\ndistribution. With Amazon Route 53, you can define a percentage of traffic to\\ngo to the green environment and gradually update the weights until the green\\nenvironment carries the full production traffic. A weighted distribution\\nprovides the ability to perform canary analysis where a small percentage of\\nproduction traffic is introduced to a new environment. You can test the new\\ncode and monitor for errors, limiting the blast radius if any issues are\\nencountered. It also allows the green environment to scale out to support the\\nfull production load if you are using Elastic Load Balancing.\\nhttps://d0.awsstatic.com/whitepapers/AWS_Blue_Green_Deployments.pdf"},{"id":"373","question":"You are a DevOps Engineer in an organization whose\\n\\n\\fapplication is hosted on a single EC2 instance in AWS. You are\\nreceiving complaints from end users about slow response time of\\nyour application. What would you do to resolve this issue?","options":{"A":"By using CloudFormation to deploy the app again with an\\nAmazon RDS with the Multi-AZ feature","B":"By using Amazon RDS with the Multi-AZ feature","C":"By using Auto-scaling launch configurations to launch multiple\\ninstances and placing them behing an ELB","D":"By using Auto-scaling groups to launch multiple instances and\\nplacing them behind an ELB"},"answer":"A","explanation":"When you use Auto-scaling, you can automatically increase\\nthe size of your Auto-scaling group when demand goes up and decrease it\\nwhen demand goes down. As Auto-scaling adds and removes EC2 instances,\\nyou must ensure that the traffic for your application is distributed across all\\nof your EC2 instances. The Elastic Load Balancing service automatically\\nroutes incoming web traffic across such a dynamically changing number of\\nEC2 instances. Your load balancer acts as a single point of contact for all\\nincoming traffic to the instances in your Auto-scaling group.\\nhttps://docs.aws.amazon.com/autoscaling/ec2/userguide/autoscaling-loadbalancer.html"},{"id":"374","question":"You are designing a CloudFormation template to launch EC2\\ninstances as web servers and following user data needs to be passed\\nto the instances:\\n#!/bin/bash\\nsudo apt-get update\\nsudo apt-get install -y nginx\\nIn which portion of the template should you pass this data?","options":{"A":"In the Metadata section of the EC2 Instance in the Output\\nsection","B":"In the Metadata section of the EC2 Instance in the resources\\nsection","C":"In the properties section of the EC2 Instance in the Output\\n\\n\\fsection","D":"In the properties section of the EC2 Instance in the resources\\nsection"},"answer":"D","explanation":"The following is an example of how user data can be passed to\\ninstances using a CloudFormation template:\\n{\\n“Resources”: {\\n“WebServerInstance”: {\\n“Type”: “AWS::EC2::Instance”.\\n“Properties”: {\\n“InstanceType”: “t2.micro”.\\n“ImageId”: “ami-6f198a0c”.\\n“UserData”: {\\n“Fn::Base64”:.\\n“Fn::Join”: {\\n“\\\\n”.\\n[\\n“#!/bin/bash”.\\n“sudo apt-get update”.\\n“sudo apt-get install –y nginx]]}}\\n\\n}\\n}\\n}\\n}\\nhttps://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/deploying.applicatio"},{"id":"375","question":"While using the Elastic Load Balancer, which of the following\\nare ways to secure data in-transit? (Choose 2)","options":{"A":"Using an HTTPS front end listener for your ELB","B":"Using an HTTP front end listener for your ELB","C":"Using an SSL front end listener for your ELB","D":"Using a TCP front end listener for your ELB"},"answer":"A and C","explanation":"As per AWS documentation; You can create a load balancer\\nthat uses the SSL/TLS protocol for encrypted connections (also known as\\nSSL offload). This feature enables traffic encryption between your load\\nbalancer and the clients that initiate HTTPS sessions, and for connections\\nbetween your load balancer and your EC2 instances.\\nIf you use HTTPS or SSL for your front-end connections, you must deploy\\nan X.509 certificate (SSL server certificate) on your load balancer. The load\\nbalancer decrypts requests from clients before sending them to the back-end\\ninstances (known as SSL Termination).\\nIf you do not want the load balancer to handle the SSL termination (known\\nas SSL offloading) you can use TCP for both the front-end and back-end\\nconnections, and deploy certificates on the registered instances handling\\nrequests.\\nhttps://docs.aws.amazon.com/elasticloadbalancing/latest/classic/elb-httpsload-balancers.html\\nhttps://docs.aws.amazon.com/elasticloadbalancing/latest/classic/elb-listenerconfig.html\\nhttps://docs.aws.amazon.com/elasticloadbalancing/latest/classic/using-elblistenerconfig-quickref.html"},{"id":"376","question":"To detect the application health while performing a Blue/Green\\ndeployment, which of the following services can be used?","options":{"A":"AWS CloudTrail","B":"AWS CloudWatch","C":"AWS CodeStar","D":"AWS CodeCommit"},"answer":"B","explanation":"Amazon CloudWatch is a monitoring service for AWS Cloud\\nresources and the applications you run on AWS. CloudWatch can collect and\\ntrack metrics, collect and monitor log files, and set alarms. It provides\\nsystem-wide visibility into resource utilization, application performance, and\\noperational health, which are key to early detection of application health in\\nBlue/Green deployments.\\nhttps://d0.awsstatic.com/whitepapers/AWS_Blue_Green_Deployments.pdf"},{"id":"377","question":"Your company’s web application is hosted on a set of EC2\\ninstances sitting behind an ELB. An Amazon RDS instance is also\\nused by this app. You are required to make this infrastructure selfhealing and cost effective. Which of the following would fulfil this\\nrequirement? (Choose 2)","options":{"A":"Use CloudWatch metrics to check the utilization of the\\ndatabases servers. Use Auto-scaling group to scale the database\\ninstances accordingly based on the CloudWatch metrics","B":"Utilize the Read Replica feature for the Amazon RDS layer","C":"Use CloudWatch metrics to check the utilization of the web\\nlayer. Use Auto-scaling group to scale the web instances\\naccordingly based on the CloudWatch metrics","D":"Utilize the Multi-AZ feature for the Amazon RDS layer"},"answer":"C and D","explanation":"Self-healing architecture for the scenario in question can be\\nachieved by using Auto-scale.\\nAmazon RDS Multi-AZ deployments provide enhanced availability and\\ndurability for Database (DB) Instances, making them a natural fit for\\nproduction database workloads. When you provision a Multi-AZ DB\\nInstance, Amazon RDS automatically creates a primary DB Instance and\\nsynchronously replicates the data to a standby instance in a different\\nAvailability Zone (AZ). Each AZ runs on its own physically distinct,\\nindependent infrastructure, and is engineered to be highly reliable. In case of\\nan infrastructure failure, Amazon RDS performs an automatic failover to the\\nstandby (or to a read replica in the case of Amazon Aurora), so that you can\\nresume database operations as soon as the failover is complete. Since the\\nendpoint for your DB Instance remains the same after a failover, your\\napplication can resume database operation without the need for manual\\nadministrative intervention.\\nhttps://aws.amazon.com/rds/details/multi-az/"},{"id":"378","question":"A group of users in your organization uses a set of instances that\\nis hosting nginx server and a web application. The instances start\\nfacing technical issues after a recent version upgrade and require\\n\\n\\fimmediate restart. You did not get time to inspect what caused the\\nissue on the servers. Which of the following options, if implemented\\nprior to the incident would have assisted in you in finding out the\\nissue?","options":{"A":"Streaming all the data to Amazon Kinesis and then analyzing\\nthe data in real time","B":"Enabling detailed monitoring and checking the Cloudwatch\\nmetrics to see the cause of the issue","C":"Installing Cloudwatch logs agent on the instance and sending\\nall the logs to Cloudwatch logs","D":"Creating a snapshot of the EBS volume before restart, attaching\\nit to another instance as a volume and then diagnosing the issue"},"answer":"C","explanation":"You can publish log data from Amazon EC2 instances running\\nLinux or Windows Server, and logged events from AWS CloudTrail.\\nCloudWatch Logs can consume logs from resources in any region, but you\\ncan only view the log data in the CloudWatch console in the regions where\\nCloudWatch Logs is supported.\\nOption A is incorrect as here we are dealing with an issue concerning the\\nunderlying application that handles the data and so this solution will not help.\\nOption B is invalid as detailed monitoring will only help us to get more\\ninformation about the performance metrics of the instances, volumes etc. and\\nwill not be able to provide full information regarding technical issues.\\nOption D is incorrect as if we had created a snapshot prior to the update, it\\nmight have been useful but not after the incident.\\nhttps://docs.aws.amazon.com/AmazonCloudWatch/latest/logs/StartTheCWLAgent.html"},{"id":"379","question":"You are required to deploy a multi-container Docker\\nenvironment to Elastic BeanStalk. Which of the following files can\\nbe used to deploy a set of Docker containers in BeanStalk?","options":{"A":"Dockerrun","B":"Dockerrun.aws.json","C":"DocekrMultiFile","D":"DockerFile"},"answer":"B","explanation":"The AWS documentation states; A Dockerrun.aws.json file is\\nan Elastic BeanStalk–specific JSON file that describes how to deploy a set of\\nDocker containers as an Elastic BeanStalk application. You can use a\\nDockerrun.aws.json file for a multi-container Docker environment.\\nDockerrun.aws.json describes the containers to deploy to each container\\ninstance in the environment as well as the data volumes to create on the host\\ninstance for the containers to mount.\\nhttps://docs.aws.amazon.com/elasticbeanstalk/latest/dg/create_deploy_docker_v2config.ht"},{"id":"380","question":"Which of the following can be integrated with Jenkins\\ncontinuous integration tool?","options":{"A":"Amazon EC2","B":"Amazon ECS","C":"Amazon Elastic BeanStalk","D":"All of the above"},"answer":"D","explanation":"The following AWS services can be integrated with Jenkins:\\nAmazon EC2\\nAmazon ECR\\nAmazon SNS\\nAmazon ECS\\nAmazon S3\\nAWS CloudFormation\\nAWS CodeDeploy\\nAWS Code PipeLine\\nAWS CodeCommit\\nAWS Device Farm\\nAWS Elastic BeanStalk"},{"id":"381","question":"Your company’s application is hosted on AWS and uses\\nDynamoDB. According to the IT security policy, it is required to\\nrecord all the source IP addresses that make calls to DynamoDB\\ntable. Which of the following service can be used to fulfil this\\nrequirement?","options":{"A":"AWS CloudWatch","B":"AWS Trusted Advisor","C":"AWS CloudTrail","D":"AWS Inspector"},"answer":"C","explanation":"DynamoDB is integrated with CloudTrail, a service that\\ncaptures low-level API requests made by or on behalf of DynamoDB in your\\nAWS account and delivers the log files to an Amazon S3 bucket that you\\nspecify. CloudTrail captures calls made from the DynamoDB console or\\nfrom the DynamoDB low-level API. Using the information collected by\\nCloudTrail, you can determine what request was made to DynamoDB, the\\nsource IP address from which the request was made, who made the request,\\nwhen it was made, and so on.\\nhttps://docs.aws.amazon.com/amazondynamodb/latest/developerguide/loggingusing-cloudtrail.html"},{"id":"382","question":"Your organization’s infrastructure is built in a way that the\\ninstances access data objects that are stored in an S3 bucket. The IT\\nsecurity department is concerned about the security of the\\narchitecture and you are required to implement the following:\\nEnsure that the EC2 instance securely access data objects in the S3\\nbucket\\nEnsure that the integrity of the objects stored in S3 is maintained\\nWhich of the following would help you in fulfilling these requirements?\\n(Choose 2)","options":{"A":"Use S3 Cross Region replication to replicate the objects so that\\nthe integrity of data is maintained","B":"Create an IAM Role and ensure the EC2 Instance uses the IAM\\nRole to access the data in the bucket","C":"Use an S3 bucket policy that ensures that MFA Delete is set on\\nthe objects in the bucket","D":"Create an IAM user and ensure the EC2 Instances uses the\\nIAM user credentials to access the data in the bucket"},"answer":"B and C","explanation":"IAM roles are designed so that your applications can securely\\nmake API requests from your instances, without requiring you to manage the\\nsecurity credentials that the applications use. Instead of creating and\\ndistributing your AWS credentials, you can delegate permission to make API\\nrequests using IAM roles.\\nMFS Delete can be used to add another layer of security to S3 Objects to\\nprevent accidental deletion of objects.\\nhttps://docs.aws.amazon.com/AWSEC2/latest/UserGuide/iam-roles-foramazon-ec2.html\\nhttps://aws.amazon.com/blogs/security/securing-access-to-aws-using-mfapart-3/"},{"id":"383","question":"Which of the below mentioned source repositories can be used\\nby AWS CodeDeploy to deploy code? (Choose 3)","options":{"A":"S3 buckets","B":"GitHub Repositories","C":"Subversion Repositories","D":"Bitbucket Repositories\\nAnswer: A, C, and D\\nExplanation: You can deploy a nearly unlimited variety of application\\ncontent, such as code, web and configuration files, executables, packages,\\nscripts, multimedia files, and so on. AWS CodeDeploy can deploy\\napplication content stored in Amazon S3 buckets, GitHub repositories, or\\nBitbucket repositories. You do not need to make changes to your existing\\ncode before you can use AWS CodeDeploy.\\nhttps://docs.aws.amazon.com/codedeploy/latest/userguide/welcome.html\\n384. You want to get a snapshot of the current configuration of the\\nresources in your company’s AWS account. Which of the following\\ncan be used for this?\\nA. AWS IAM\\nB. AWS Trusted Advisor\\nC. AWS Config\\nD. AWS Trusted Advisor"},"answer":"C","explanation":"With AWS Config, you can do the following:\\nEvaluate your AWS resource configurations for desired settings\\nGet a snapshot of the current configurations of the supported resources\\nthat are associated with your AWS account\\nRetrieve configurations of one or more resources that exist in your\\naccount\\nRetrieve historical configurations of one or more resources\\nReceive a notification whenever a resource is created, modified, or\\ndeleted\\nView relationships between resources. For example, you might want\\nto find all resources that use a particular security group\\nhttps://docs.aws.amazon.com/config/latest/developerguide/WhatIsConfig.html\\n\\n\\fAbout Our Products\\nOther products from IPSpecialist LTD regarding Cloud technology\\nare:\\nAWS Certified Cloud Practitioner Technology Workbook\\nAWS Certified Solutions Architect - Associate TechnologyWorkbook\\nAWS Certified Developer - Associate Technology Workbook\\nAWS Certified SysOps Administrator - Associate Technology Workbook\\nAWS Certified Solutions Architect - Professional Technology Workbook\\nAWS Certified DevOps Engineer - Professional Technology Workbook\\nAWS Certified Advanced Networking - Specialty Technology Workbook\\nAWS Certified Big Data - Specialty Technology Workbook\\nAWS Certified Security - Specialty Technology Workbook\\nGoogle Certified Associate Cloud Engineer Technology Workbook\\nGoogle Certified Professional Cloud Architect Technology Workbook\\n\\n\\fNote from the Author:\\nReviews are gold to authors! If you have enjoyed this book and it has helped you along your\\ncertification, would you consider rating and reviewing it?\\n\\nLink to Product Page:"}]}')}}]);